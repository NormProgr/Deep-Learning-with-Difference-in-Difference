{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main model classic double robust DiD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- I caluzclate the average Variance not asymptotic Variance. i dont know what that is tbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from doubleml import DoubleMLData, DoubleMLDID\n",
    "from doubleml.datasets import make_did_SZ2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the new paper I found and this repository to replicate the results of sant'anna and Zhao (2021) paper. https://github.com/tommaso-manfe/DR-DIPW-Updated-Beyond-Regression-DiD/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "ml_g = LinearRegression()  # as in the paper, estimators not needed\n",
    "ml_m = LogisticRegression()  # as in the paper, estimators not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 1000\n",
    "n_rep = 1000\n",
    "ATTE = 0.0\n",
    "\n",
    "ATTE_estimates = np.full((n_rep), np.nan)\n",
    "coverage = np.full((n_rep), np.nan)\n",
    "ci_length = np.full((n_rep), np.nan)\n",
    "asymptotic_variance = np.full(n_rep, np.nan)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "for i_rep in range(n_rep):\n",
    "    if (i_rep % int(n_rep / 10)) == 0:\n",
    "        print(f\"Iteration: {i_rep}/{n_rep}\")\n",
    "    dml_data = make_did_SZ2020(n_obs=n_obs, dgp_type=4, cross_sectional_data=False)\n",
    "\n",
    "    dml_did = DoubleMLDID(dml_data, ml_g=ml_g, ml_m=ml_m, n_folds=5)\n",
    "    dml_did.fit()\n",
    "\n",
    "    ATTE_estimates[i_rep] = dml_did.coef.squeeze()\n",
    "    confint = dml_did.confint(level=0.95)\n",
    "    coverage[i_rep] = (confint[\"2.5 %\"].iloc[0] <= ATTE) & (\n",
    "        confint[\"97.5 %\"].iloc[0] >= ATTE\n",
    "    )\n",
    "    ci_length[i_rep] = confint[\"97.5 %\"].iloc[0] - confint[\"2.5 %\"].iloc[0]\n",
    "\n",
    "    summary_df = dml_did.summary\n",
    "    std_err = summary_df.loc[\"d\", \"std err\"]\n",
    "    asymptotic_variance[i_rep] = std_err**2\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - ATTE)\n",
    "med_bias = np.median(ATTE_estimates - ATTE)\n",
    "rmse = np.sqrt(np.mean((ATTE_estimates - ATTE) ** 2))\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(coverage)\n",
    "avg_ci_length = np.mean(ci_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWFE Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import logistic\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Define parameters\n",
    "n = 1000  # Sample size\n",
    "Xsi_ps = 0.75  # pscore index\n",
    "_lambda = 0.5  # Proportion in each period\n",
    "nboot = 199  # Number of bootstrapped draws\n",
    "\n",
    "# Define means and standard deviations\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "# Loop for 1000 runs\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Propensity score\n",
    "    pi = logistic.cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "    index_unobs_het = d * (index_lin)\n",
    "    index_att = 0\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Generate unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Generate outcomes at time 0 and time 1\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "    y01 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    y11 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = (\n",
    "        d * post * y11\n",
    "        + (1 - d) * post * y01\n",
    "        + (1 - d) * (1 - post) * y00\n",
    "        + (d) * (1 - post) * y10\n",
    "    )\n",
    "\n",
    "    # Generate id\n",
    "    id_ = np.arange(1, n + 1)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"y\": y,\n",
    "            \"post\": post.astype(int),\n",
    "            \"d\": d.astype(int),\n",
    "            \"x1\": z1,\n",
    "            \"x2\": z2,\n",
    "            \"x3\": z3,\n",
    "            \"x4\": z4,\n",
    "        },\n",
    "    )\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "\n",
    "    dta_long = dta_long.sort_values(by=\"id\")\n",
    "\n",
    "    # Perform TWFE estimation\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "\n",
    "    # Calculate asymptotic variance\n",
    "    asymptotic_variance.append(twfe_i.cov_params()[\"post:d\"])\n",
    "\n",
    "    # Bootstrap for confidence intervals\n",
    "    # (Note: This part is not included in the provided code, you'll need to implement it separately)\n",
    "\n",
    "    # Append TWFE estimate to the list\n",
    "    ATTE_estimates.append(twfe)\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "asymptotic_variance = asymptotic_variance.reshape(-1, 8)\n",
    "asymptotic_variance = asymptotic_variance[:, 0]\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(\n",
    "    np.mean((ATTE_estimates - 0) ** 2),\n",
    ")  # Assuming ATTE is 0 as mentioned in the code\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(\n",
    "    ((ATTE_estimates - 1.96 * np.sqrt(asymptotic_variance)) <= 0)\n",
    "    & ((ATTE_estimates + 1.96 * np.sqrt(asymptotic_variance)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(asymptotic_variance),\n",
    ")  # Length of 95% confidence interval\n",
    "average_ATT_estimate = np.mean(ATTE_estimates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ATT estimate across all runs: {average_ATT_estimate}\")\n",
    "\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import logistic\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "# Proportion in each period\n",
    "_lambda = 0.5\n",
    "# Number of bootstrapped draws\n",
    "nboot = 199\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Generate treatment groups\n",
    "    # Propensity score\n",
    "    pi = logistic.cdf(Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Create heterogenenous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * (index_lin)\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    # First let's generate potential outcomes: y_1_potential\n",
    "    y01 = (\n",
    "        index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    )  # This is the baseline\n",
    "    y11 = (\n",
    "        index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "    )  # This is the baseline\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = (\n",
    "        d * post * y11\n",
    "        + (1 - d) * post * y01\n",
    "        + (1 - d) * (1 - post) * y00\n",
    "        + (d) * (1 - post) * y10\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.arange(1, n + 1)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"y\": y,\n",
    "            \"post\": post.astype(int),\n",
    "            \"d\": d.astype(int),\n",
    "            \"x1\": z1,\n",
    "            \"x2\": z2,\n",
    "            \"x3\": z3,\n",
    "            \"x4\": z4,\n",
    "        },\n",
    "    )\n",
    "    dta_long = dta_long.sort_values(by=\"id\")\n",
    "\n",
    "    # Create interaction term 'post:d'\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "\n",
    "    asymptotic_variance.append(\n",
    "        twfe_i.HC0_se[\"post:d\"],\n",
    "    )  # Append the standard error for the interaction term\n",
    "\n",
    "\n",
    "# Convert lists to arrays for calculations\n",
    "twfe = np.array(twfe)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(twfe - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(twfe - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(np.mean((twfe - 0) ** 2))  # Assuming ATTE is 0 as mentioned in the code\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(\n",
    "    ((twfe - 1.96 * np.sqrt(asymptotic_variance)) <= 0)\n",
    "    & ((twfe + 1.96 * np.sqrt(asymptotic_variance)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(asymptotic_variance),\n",
    ")  # Length of 95% confidence interval\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "# Proportion in each period\n",
    "_lambda = 0.5\n",
    "nboot = 199  # Number of bootstrapped draws\n",
    "\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Propensity score\n",
    "    pi = 1 / (1 + np.exp(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4)))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "    # Create heterogenenous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * index_lin\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    y01 = index_lin + v + np.random.normal(size=n) + index_trend  # This is the baseline\n",
    "    y11 = (\n",
    "        index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "    )  # This is the baseline\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = (\n",
    "        d * post * y11\n",
    "        + (1 - d) * post * y01\n",
    "        + (1 - d) * (1 - post) * y00\n",
    "        + (d) * (1 - post) * y10\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.arange(1, n + 1)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"y\": y,\n",
    "            \"post\": post.astype(int),\n",
    "            \"d\": d.astype(int),\n",
    "            \"x1\": z1,\n",
    "            \"x2\": z2,\n",
    "            \"x3\": z3,\n",
    "            \"x4\": z4,\n",
    "        },\n",
    "    )\n",
    "    dta_long = dta_long.sort_values(by=\"id\")\n",
    "    # Create interaction term 'post:d'\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "    # Standard TWFE\n",
    "\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "    asymptotic_variance.append(twfe_i.cov_params()[\"post:d\"])\n",
    "    # Convert lists to arrays for calculations\n",
    "    ATTE_estimates.append(twfe)\n",
    "\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "asymptotic_variance = asymptotic_variance.reshape(-1, 8)\n",
    "asymptotic_variance = asymptotic_variance[:, 0]\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(\n",
    "    np.mean((ATTE_estimates - 0) ** 2),\n",
    ")  # Assuming ATTE is 0 as mentioned in the code\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(\n",
    "    ((ATTE_estimates - 1.96 * np.sqrt(asymptotic_variance)) <= 0)\n",
    "    & ((ATTE_estimates + 1.96 * np.sqrt(asymptotic_variance)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(asymptotic_variance),\n",
    ")  # Length of 95% confidence interval\n",
    "average_ATT_estimate = np.mean(ATTE_estimates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ATT estimate across all runs: {average_ATT_estimate}\")\n",
    "\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "# Sample size\n",
    "n = 1000\n",
    "\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "\n",
    "# Proportion in each period\n",
    "lambda_ = 0.5\n",
    "\n",
    "# Number of bootstrapped draws\n",
    "nboot = 199\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Gen covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Gen treatment groups\n",
    "    # Propensity score\n",
    "    pi = 1 / (1 + np.exp(-Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4)))\n",
    "    d = np.random.rand(n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "    # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * index_lin\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1, n)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    # First let's generate potential outcomes: y_1_potential\n",
    "    y01 = index_lin + v + np.random.normal(size=n) + index_trend\n",
    "    y11 = index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.rand(n) <= ti\n",
    "\n",
    "    y = (\n",
    "        d * post * y11\n",
    "        + (1 - d) * post * y01\n",
    "        + (1 - d) * (1 - post) * y00\n",
    "        + d * (1 - post) * y10\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.arange(1, n + 1)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"y\": y,\n",
    "            \"post\": post.astype(int),\n",
    "            \"d\": d.astype(int),\n",
    "            \"x1\": z1,\n",
    "            \"x2\": z2,\n",
    "            \"x3\": z3,\n",
    "            \"x4\": z4,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    dta_long = dta_long.sort_values(\"id\")\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "    # Standard TWFE\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "    asymptotic_variance.append(twfe_i.cov_params()[\"post:d\"])\n",
    "    ATTE_estimates.append(twfe)\n",
    "# Convert lists to arrays for calculations\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "asymptotic_variance = asymptotic_variance.reshape(-1, 8)\n",
    "asymptotic_variance = asymptotic_variance[:, 0]\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(\n",
    "    np.mean((ATTE_estimates - 0) ** 2),\n",
    ")  # Assuming ATTE is 0 as mentioned in the code\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(\n",
    "    ((ATTE_estimates - 1.96 * np.sqrt(asymptotic_variance)) <= 0)\n",
    "    & ((ATTE_estimates + 1.96 * np.sqrt(asymptotic_variance)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(asymptotic_variance),\n",
    ")  # Length of 95% confidence interval\n",
    "average_ATT_estimate = np.mean(ATTE_estimates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ATT estimate across all runs: {average_ATT_estimate}\")\n",
    "\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning with a few runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "be careful overfitting could be a problem here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from doubleml import DoubleMLData, DoubleMLDID\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from scikeras.wrappers import KerasClassifier  # pip install scikeras\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "n_reps = 10  # change that accordingly\n",
    "n_obs = 1000\n",
    "ATTE = 0.0\n",
    "\n",
    "# Enable GPU acceleration if available\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Function to create Keras model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=x.shape[1], activation=\"relu\"))  # Simplified\n",
    "    model.add(Dense(16, activation=\"relu\"))  # Simplified\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))  # Assuming binary classification\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize arrays to store statistics\n",
    "biases = np.full(n_reps, np.nan)\n",
    "variances = np.full(n_reps, np.nan)\n",
    "rmse_list = np.full(n_reps, np.nan)\n",
    "coverage = np.full(n_reps, np.nan)\n",
    "asymptotic_variance = np.full(n_reps, np.nan)\n",
    "ci_length = np.full(n_reps, np.nan)\n",
    "ATTE_estimates = np.full(n_reps, np.nan)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"loss\", patience=2, verbose=0)  # Early stopping\n",
    "\n",
    "for i_rep in range(n_reps):\n",
    "    x, y, d = make_did_SZ2020(\n",
    "        n_obs=n_obs,\n",
    "        dgp_type=4,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"array\",\n",
    "    )\n",
    "    dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "\n",
    "    # Wrap the Keras model with KerasClassifier\n",
    "    keras_classifier = KerasClassifier(\n",
    "        build_fn=create_model,\n",
    "        epochs=3,  # adjust as needed\n",
    "        batch_size=32,  # Smaller batch size\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    # Use StandardScaler to normalize data and then use the Keras classifier in a pipeline\n",
    "    ml_m = Pipeline([(\"scaler\", StandardScaler()), (\"nn\", keras_classifier)])\n",
    "\n",
    "    # Use LinearRegression for regression\n",
    "    ml_g = LinearRegression()\n",
    "\n",
    "    dml_plr = DoubleMLDID(dml_data, ml_g, ml_m)\n",
    "    dml_plr.fit()\n",
    "\n",
    "    ATTE_estimates[i_rep] = dml_plr.coef.squeeze()\n",
    "    confint = dml_plr.confint(level=0.95)\n",
    "    coverage[i_rep] = (confint[\"2.5 %\"].iloc[0] <= ATTE) & (\n",
    "        confint[\"97.5 %\"].iloc[0] >= ATTE\n",
    "    )\n",
    "    ci_length[i_rep] = confint[\"97.5 %\"].iloc[0] - confint[\"2.5 %\"].iloc[0]\n",
    "    # Extract standard error from the summary\n",
    "    summary_df = dml_plr.summary\n",
    "    std_err = summary_df.loc[\"d\", \"std err\"]\n",
    "    asymptotic_variance[i_rep] = std_err**2\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - ATTE)\n",
    "med_bias = np.median(ATTE_estimates - ATTE)\n",
    "rmse = np.sqrt(np.mean((ATTE_estimates - ATTE) ** 2))\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(coverage)\n",
    "avg_ci_length = np.mean(ci_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulaization Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_pa = pd.DataFrame(ATTE_estimates, columns=[\"Estimate\"])\n",
    "g = sns.kdeplot(df_pa, fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning without double robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the paper of the phd guys how they did the IPW or essentially the propensity score matching"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
