{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main model classic double robust DiD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- I caluzclate the average Variance not asymptotic Variance. i dont know what that is tbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from doubleml import DoubleMLData, DoubleMLDID\n",
    "from doubleml.datasets import make_did_SZ2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the new paper I found and this repository to replicate the results of sant'anna and Zhao (2021) paper. https://github.com/tommaso-manfe/DR-DIPW-Updated-Beyond-Regression-DiD/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "ml_g = LinearRegression()  # as in the paper, estimators not needed\n",
    "ml_m = LogisticRegression()  # as in the paper, estimators not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 1000\n",
    "n_rep = 100\n",
    "ATTE = 0.0\n",
    "\n",
    "ATTE_estimates = np.full((n_rep), np.nan)\n",
    "coverage = np.full((n_rep), np.nan)\n",
    "ci_length = np.full((n_rep), np.nan)\n",
    "asymptotic_variance = np.full(n_rep, np.nan)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "for i_rep in range(n_rep):\n",
    "    if (i_rep % int(n_rep / 10)) == 0:\n",
    "        print(f\"Iteration: {i_rep}/{n_rep}\")\n",
    "    dml_data = make_did_SZ2020(n_obs=n_obs, dgp_type=2, cross_sectional_data=False)\n",
    "\n",
    "    dml_did = DoubleMLDID(dml_data, ml_g=ml_g, ml_m=ml_m, n_folds=5)\n",
    "    dml_did.fit()\n",
    "\n",
    "    ATTE_estimates[i_rep] = dml_did.coef.squeeze()\n",
    "    confint = dml_did.confint(level=0.95)\n",
    "    coverage[i_rep] = (confint[\"2.5 %\"].iloc[0] <= ATTE) & (\n",
    "        confint[\"97.5 %\"].iloc[0] >= ATTE\n",
    "    )\n",
    "    ci_length[i_rep] = confint[\"97.5 %\"].iloc[0] - confint[\"2.5 %\"].iloc[0]\n",
    "\n",
    "    summary_df = dml_did.summary\n",
    "    std_err = summary_df.loc[\"d\", \"std err\"]\n",
    "    asymptotic_variance[i_rep] = std_err**2\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - ATTE)\n",
    "med_bias = np.median(ATTE_estimates - ATTE)\n",
    "rmse = np.sqrt(np.mean((ATTE_estimates - ATTE) ** 2))\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(coverage)\n",
    "avg_ci_length = np.mean(ci_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWFE Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import logistic\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Define parameters\n",
    "n = 1000  # Sample size\n",
    "Xsi_ps = 0.75  # pscore index\n",
    "_lambda = 0.5  # Proportion in each period\n",
    "nboot = 199  # Number of bootstrapped draws\n",
    "\n",
    "# Define means and standard deviations\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "# Loop for 1000 runs\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Propensity score\n",
    "    pi = logistic.cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "    index_unobs_het = d * (index_lin)\n",
    "    index_att = 0\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Generate unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Generate outcomes at time 0 and time 1\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "    y01 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    y11 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = np.where(\n",
    "        d & post,\n",
    "        y11,\n",
    "        np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "    )\n",
    "\n",
    "    # Generate id\n",
    "    id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "    time = np.tile([0, 1], n)\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"time\": time,\n",
    "            \"y\": np.tile(y, 2),\n",
    "            \"post\": np.tile(post.astype(int), 2),\n",
    "            \"d\": np.tile(d.astype(int), 2),\n",
    "            \"x1\": np.tile(z1, 2),\n",
    "            \"x2\": np.tile(z2, 2),\n",
    "            \"x3\": np.tile(z3, 2),\n",
    "            \"x4\": np.tile(z4, 2),\n",
    "        },\n",
    "    )\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "\n",
    "    dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "    # Perform TWFE estimation\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "\n",
    "    # Calculate asymptotic variance\n",
    "    asymptotic_variance.append(twfe_i.cov_params()[\"post:d\"])\n",
    "\n",
    "    # Bootstrap for confidence intervals\n",
    "    # (Note: This part is not included in the provided code, you'll need to implement it separately)\n",
    "\n",
    "    # Append TWFE estimate to the list\n",
    "    ATTE_estimates.append(twfe)\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "asymptotic_variance = asymptotic_variance.reshape(-1, 8)\n",
    "asymptotic_variance = asymptotic_variance[:, 0]\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(\n",
    "    np.mean((ATTE_estimates - 0) ** 2),\n",
    ")  # Assuming ATTE is 0 as mentioned in the code\n",
    "variance_ATT = np.var(ATTE_estimates)\n",
    "coverage_probability = np.mean(\n",
    "    ((ATTE_estimates - 1.96 * np.sqrt(variance_ATT)) <= 0)\n",
    "    & ((ATTE_estimates + 1.96 * np.sqrt(variance_ATT)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(variance_ATT),\n",
    ")  # Length of 95% confidence interval\n",
    "average_ATT_estimate = np.mean(ATTE_estimates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ATT estimate across all runs: {average_ATT_estimate}\")\n",
    "\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"variance_ATT: {variance_ATT}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import logistic\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "# Proportion in each period\n",
    "_lambda = 0.5\n",
    "# Number of bootstrapped draws\n",
    "nboot = 199\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Generate treatment groups\n",
    "    # Propensity score\n",
    "    pi = logistic.cdf(Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Create heterogenenous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * (index_lin)\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    # First let's generate potential outcomes: y_1_potential\n",
    "    y01 = (\n",
    "        index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    )  # This is the baseline\n",
    "    y11 = (\n",
    "        index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "    )  # This is the baseline\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = np.where(\n",
    "        d & post,\n",
    "        y11,\n",
    "        np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "    time = np.tile([0, 1], n)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"time\": time,\n",
    "            \"y\": np.tile(y, 2),\n",
    "            \"post\": np.tile(post.astype(int), 2),\n",
    "            \"d\": np.tile(d.astype(int), 2),\n",
    "            \"x1\": np.tile(z1, 2),\n",
    "            \"x2\": np.tile(z2, 2),\n",
    "            \"x3\": np.tile(z3, 2),\n",
    "            \"x4\": np.tile(z4, 2),\n",
    "        },\n",
    "    )\n",
    "    dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "    # Create interaction term 'post:d'\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "\n",
    "    asymptotic_variance.append(\n",
    "        twfe_i.HC0_se[\"post:d\"],\n",
    "    )  # Append the standard error for the interaction term\n",
    "    ATTE_estimates.append(twfe)\n",
    "\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "\n",
    "# Convert lists to arrays for calculations\n",
    "twfe = np.array(twfe)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(twfe - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(twfe - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(np.mean((twfe - 0) ** 2))  # Assuming ATTE is 0 as mentioned in the code\n",
    "variance_ATT = np.var(ATTE_estimates)\n",
    "coverage_probability = np.mean(\n",
    "    ((twfe - 1.96 * np.sqrt(variance_ATT)) <= 0)\n",
    "    & ((twfe + 1.96 * np.sqrt(variance_ATT)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(variance_ATT),\n",
    ")  # Length of 95% confidence interval\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"variance_ATT: {variance_ATT}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgp panel at work\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "# Proportion in each period\n",
    "_lambda = 0.5\n",
    "nboot = 199  # Number of bootstrapped draws\n",
    "\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Propensity score\n",
    "    pi = 1 / (1 + np.exp(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4)))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "    # Create heterogenenous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * index_lin\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    y01 = index_lin + v + np.random.normal(size=n) + index_trend  # This is the baseline\n",
    "    y11 = (\n",
    "        index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "    )  # This is the baseline\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = np.where(\n",
    "        d & post,\n",
    "        y11,\n",
    "        np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "    time = np.tile([0, 1], n)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"time\": time,\n",
    "            \"y\": np.tile(y, 2),\n",
    "            \"post\": np.tile(post.astype(int), 2),\n",
    "            \"d\": np.tile(d.astype(int), 2),\n",
    "            \"x1\": np.tile(z1, 2),\n",
    "            \"x2\": np.tile(z2, 2),\n",
    "            \"x3\": np.tile(z3, 2),\n",
    "            \"x4\": np.tile(z4, 2),\n",
    "        },\n",
    "    )\n",
    "    dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "\n",
    "    # Create interaction term 'post:d'\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "    # Standard TWFE\n",
    "\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "    asymptotic_variance.append(twfe_i.cov_params()[\"post:d\"])\n",
    "    # Convert lists to arrays for calculations\n",
    "    ATTE_estimates.append(twfe)\n",
    "\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "asymptotic_variance = asymptotic_variance.reshape(-1, 8)\n",
    "asymptotic_variance = asymptotic_variance[:, 0]\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(\n",
    "    np.mean((ATTE_estimates - 0) ** 2),\n",
    ")  # Assuming ATTE is 0 as mentioned in the code\n",
    "variance_ATT = np.var(ATTE_estimates)\n",
    "coverage_probability = np.mean(\n",
    "    ((ATTE_estimates - 1.96 * np.sqrt(variance_ATT)) <= 0)\n",
    "    & ((ATTE_estimates + 1.96 * np.sqrt(variance_ATT)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(variance_ATT),\n",
    ")  # Length of 95% confidence interval\n",
    "average_ATT_estimate = np.mean(ATTE_estimates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ATT estimate across all runs: {average_ATT_estimate}\")\n",
    "\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"variance_ATT: {variance_ATT}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "# Sample size\n",
    "n = 1000\n",
    "\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "\n",
    "# Proportion in each period\n",
    "lambda_ = 0.5\n",
    "\n",
    "# Number of bootstrapped draws\n",
    "nboot = 199\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Gen covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Gen treatment groups\n",
    "    # Propensity score\n",
    "    pi = 1 / (1 + np.exp(-Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4)))\n",
    "    d = np.random.rand(n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "    # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * index_lin\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1, n)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    # First let's generate potential outcomes: y_1_potential\n",
    "    y01 = index_lin + v + np.random.normal(size=n) + index_trend\n",
    "    y11 = index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.rand(n) <= ti\n",
    "\n",
    "    # Combine outcomes into panel data format\n",
    "    y = np.where(\n",
    "        d & post,\n",
    "        y11,\n",
    "        np.where(~d & post, y01, np.where(~d & ~post, y00, y10)),\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.repeat(np.arange(1, n + 1), 2)\n",
    "    time = np.tile([0, 1], n)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"time\": time,\n",
    "            \"y\": np.tile(y, 2),\n",
    "            \"post\": np.tile(post.astype(int), 2),\n",
    "            \"d\": np.tile(d.astype(int), 2),\n",
    "            \"x1\": np.tile(z1, 2),\n",
    "            \"x2\": np.tile(z2, 2),\n",
    "            \"x3\": np.tile(z3, 2),\n",
    "            \"x4\": np.tile(z4, 2),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    dta_long = dta_long.sort_values([\"id\", \"time\"])\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "\n",
    "    # Standard TWFE\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "    asymptotic_variance.append(twfe_i.cov_params()[\"post:d\"])\n",
    "    ATTE_estimates.append(twfe)\n",
    "\n",
    "# Convert lists to arrays for calculations\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "asymptotic_variance = asymptotic_variance.reshape(-1, 8)\n",
    "asymptotic_variance = asymptotic_variance[:, 0]\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(\n",
    "    np.mean((ATTE_estimates - 0) ** 2),\n",
    ")  # Assuming ATTE is 0 as mentioned in the code\n",
    "variance_ATT = np.var(ATTE_estimates)\n",
    "coverage_probability = np.mean(\n",
    "    ((ATTE_estimates - 1.96 * np.sqrt(variance_ATT)) <= 0)\n",
    "    & ((ATTE_estimates + 1.96 * np.sqrt(variance_ATT)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(variance_ATT),\n",
    ")  # Length of 95% confidence interval\n",
    "average_ATT_estimate = np.mean(ATTE_estimates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ATT estimate across all runs: {average_ATT_estimate}\")\n",
    "\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"variance_ATT: {variance_ATT}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning with a few runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from doubleml import DoubleMLData, DoubleMLDID\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from scikeras.wrappers import KerasClassifier  # pip install scikeras\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "n_reps = 10  # change that accordingly\n",
    "n_obs = 1000\n",
    "ATTE = 0.0\n",
    "\n",
    "# Enable GPU acceleration if available\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Function to create Keras model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=x.shape[1], activation=\"relu\"))  # Simplified\n",
    "    model.add(Dense(16, activation=\"relu\"))  # Simplified\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))  # Assuming binary classification\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize arrays to store statistics\n",
    "biases = np.full(n_reps, np.nan)\n",
    "variances = np.full(n_reps, np.nan)\n",
    "rmse_list = np.full(n_reps, np.nan)\n",
    "coverage = np.full(n_reps, np.nan)\n",
    "asymptotic_variance = np.full(n_reps, np.nan)\n",
    "ci_length = np.full(n_reps, np.nan)\n",
    "ATTE_estimates = np.full(n_reps, np.nan)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"loss\", patience=2, verbose=0)  # Early stopping\n",
    "\n",
    "for i_rep in range(n_reps):\n",
    "    x, y, d = make_did_SZ2020(\n",
    "        n_obs=n_obs,\n",
    "        dgp_type=4,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"array\",\n",
    "    )\n",
    "    dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "\n",
    "    # Wrap the Keras model with KerasClassifier\n",
    "    keras_classifier = KerasClassifier(\n",
    "        build_fn=create_model,\n",
    "        epochs=3,  # adjust as needed\n",
    "        batch_size=32,  # Smaller batch size\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    # Use StandardScaler to normalize data and then use the Keras classifier in a pipeline\n",
    "    ml_m = Pipeline([(\"scaler\", StandardScaler()), (\"nn\", keras_classifier)])\n",
    "\n",
    "    # Use LinearRegression for regression\n",
    "    ml_g = LinearRegression()\n",
    "\n",
    "    dml_plr = DoubleMLDID(dml_data, ml_g, ml_m)\n",
    "    dml_plr.fit()\n",
    "\n",
    "    ATTE_estimates[i_rep] = dml_plr.coef.squeeze()\n",
    "    confint = dml_plr.confint(level=0.95)\n",
    "    coverage[i_rep] = (confint[\"2.5 %\"].iloc[0] <= ATTE) & (\n",
    "        confint[\"97.5 %\"].iloc[0] >= ATTE\n",
    "    )\n",
    "    ci_length[i_rep] = confint[\"97.5 %\"].iloc[0] - confint[\"2.5 %\"].iloc[0]\n",
    "    # Extract standard error from the summary\n",
    "    summary_df = dml_plr.summary\n",
    "    std_err = summary_df.loc[\"d\", \"std err\"]\n",
    "    asymptotic_variance[i_rep] = std_err**2\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - ATTE)\n",
    "med_bias = np.median(ATTE_estimates - ATTE)\n",
    "rmse = np.sqrt(np.mean((ATTE_estimates - ATTE) ** 2))\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(coverage)\n",
    "avg_ci_length = np.mean(ci_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "be careful overfitting could be a problem here!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeplearning for Propensity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit run for Propensity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from scipy.stats import iqr, norm\n",
    "\n",
    "# Define std_ipw_did_rc function\n",
    "\n",
    "\n",
    "def std_ipw_did_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Convert inputs to numpy arrays\n",
    "    D = np.asarray(D)\n",
    "    n = len(D)\n",
    "    y = np.asarray(y)\n",
    "    post = np.asarray(post)\n",
    "\n",
    "    # Add constant to covariate vector\n",
    "    if covariates is None:\n",
    "        int_cov = np.ones((n, 1))\n",
    "    else:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.column_stack((np.ones(n), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "    i_weights = np.asarray(i_weights)\n",
    "\n",
    "    # Pscore estimation (logit) and fitted values\n",
    "    PS = sm.Logit(D, int_cov).fit(disp=0)\n",
    "    ps_fit = PS.predict(int_cov)\n",
    "    ps_fit = np.clip(ps_fit, 1e-16, 1 - 1e-16)\n",
    "\n",
    "    # Compute IPW estimator weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * y / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * y / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * y / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * y / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    # ATT estimator\n",
    "    ipw_att = (att_treat_post - att_treat_pre) - (att_cont_post - att_cont_pre)\n",
    "\n",
    "    # Influence function to compute standard error\n",
    "    score_ps = i_weights[:, None] * (D - ps_fit)[:, None] * int_cov\n",
    "    Hessian_ps = np.linalg.inv(np.dot(int_cov.T, score_ps))\n",
    "    asy_lin_rep_ps = np.dot(score_ps, Hessian_ps)\n",
    "\n",
    "    # Influence function of the \"treat\" component\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "    inf_treat = inf_treat_post - inf_treat_pre\n",
    "\n",
    "    # Influence function of the control component\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre[:, None] * (y - att_cont_pre)[:, None] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post[:, None] * (y - att_cont_post)[:, None] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "    inf_cont_ps = np.dot(asy_lin_rep_ps, (M2_post - M2_pre))\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont += inf_cont_ps\n",
    "\n",
    "    # Combine influence functions\n",
    "    att_inf_func = inf_treat - inf_cont\n",
    "\n",
    "    if not boot:\n",
    "        # Standard error\n",
    "        se_att = np.std(att_inf_func) / np.sqrt(n)\n",
    "        uci = ipw_att + 1.96 * se_att\n",
    "        lci = ipw_att - 1.96 * se_att\n",
    "        ipw_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Multiplier bootstrap\n",
    "            ipw_boot = mboot_did(att_inf_func, nboot)\n",
    "            se_att = iqr(ipw_boot) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.quantile(np.abs(ipw_boot / se_att), 0.95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "        else:\n",
    "            # Weighted bootstrap\n",
    "            ipw_boot = [\n",
    "                wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights)\n",
    "                for _ in range(nboot)\n",
    "            ]\n",
    "            ipw_boot = np.array(ipw_boot)\n",
    "            se_att = iqr(ipw_boot - ipw_att) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.quantile(np.abs((ipw_boot - ipw_att) / se_att), 0.95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "\n",
    "    if not inffunc:\n",
    "        att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": ipw_att,\n",
    "        \"se\": se_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": ipw_boot,\n",
    "        \"att_inf_func\": att_inf_func,\n",
    "    }\n",
    "\n",
    "\n",
    "def mboot_did(att_inf_func, nboot):\n",
    "    n = len(att_inf_func)\n",
    "    boots = []\n",
    "    for _ in range(nboot):\n",
    "        weights = np.random.exponential(scale=1.0, size=n)\n",
    "        boot_att = np.sum(weights * att_inf_func) / np.sum(weights)\n",
    "        boots.append(boot_att)\n",
    "    return np.array(boots)\n",
    "\n",
    "\n",
    "def wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights):\n",
    "    indices = np.random.choice(n, n, replace=True)\n",
    "    y_boot = y[indices]\n",
    "    post_boot = post[indices]\n",
    "    D_boot = D[indices]\n",
    "    int_cov_boot = int_cov[indices]\n",
    "    i_weights_boot = i_weights[indices]\n",
    "    return std_ipw_did_rc(\n",
    "        y_boot,\n",
    "        post_boot,\n",
    "        D_boot,\n",
    "        int_cov_boot,\n",
    "        i_weights_boot,\n",
    "        boot=False,\n",
    "    )[\"ATT\"]\n",
    "\n",
    "\n",
    "# Simulation with 100 runs\n",
    "n_runs = 100\n",
    "true_att = 0  # true average treatment effect\n",
    "\n",
    "att_estimates = []\n",
    "se_estimates = []\n",
    "uci_estimates = []\n",
    "lci_estimates = []\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    x, y, d = make_did_SZ2020(\n",
    "        n_obs=1000,\n",
    "        dgp_type=4,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"array\",\n",
    "    )\n",
    "    covariates = x\n",
    "    post = np.concatenate([np.zeros(500), np.ones(500)])\n",
    "\n",
    "    results = std_ipw_did_rc(y, post, d, covariates)\n",
    "\n",
    "    att_estimates.append(results[\"ATT\"])\n",
    "    se_estimates.append(results[\"se\"])\n",
    "    uci_estimates.append(results[\"uci\"])\n",
    "    lci_estimates.append(results[\"lci\"])\n",
    "\n",
    "att_estimates = np.array(att_estimates)\n",
    "se_estimates = np.array(se_estimates)\n",
    "uci_estimates = np.array(uci_estimates)\n",
    "lci_estimates = np.array(lci_estimates)\n",
    "\n",
    "# Calculate measures\n",
    "biases = att_estimates - true_att\n",
    "avg_bias = np.mean(biases)\n",
    "med_bias = np.median(biases)\n",
    "rmse = np.sqrt(np.mean(biases**2))\n",
    "var_att = np.var(att_estimates)\n",
    "ci_lengths = uci_estimates - lci_estimates\n",
    "coverage = np.mean((lci_estimates <= true_att) & (uci_estimates >= true_att))\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(var_att),\n",
    ")  # Length of 95% confidence interval\n",
    "\n",
    "\n",
    "measures = {\n",
    "    \"Average Bias\": avg_bias,\n",
    "    \"Median Bias\": med_bias,\n",
    "    \"RMSE\": rmse,\n",
    "    \"Variance of ATT\": var_att,\n",
    "    \"Coverage\": coverage,\n",
    "    \"Confidence Interval Length\": avg_ci_length,\n",
    "}\n",
    "\n",
    "# Print measures\n",
    "for measure, value in measures.items():\n",
    "    print(f\"{measure}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1:42 min for 100 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from scipy.stats import iqr, norm\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define std_ipw_did_rc function\n",
    "\n",
    "\n",
    "def std_ipw_did_rc(\n",
    "    y,\n",
    "    post,\n",
    "    D,\n",
    "    covariates=None,\n",
    "    i_weights=None,\n",
    "    boot=False,\n",
    "    boot_type=\"weighted\",\n",
    "    nboot=None,\n",
    "    inffunc=False,\n",
    "):\n",
    "    # Convert inputs to numpy arrays\n",
    "    D = np.asarray(D)\n",
    "    n = len(D)\n",
    "    y = np.asarray(y)\n",
    "    post = np.asarray(post)\n",
    "\n",
    "    # Add constant to covariate vector\n",
    "    if covariates is None:\n",
    "        int_cov = np.ones((n, 1))\n",
    "    else:\n",
    "        covariates = np.asarray(covariates)\n",
    "        if np.all(covariates[:, 0] == 1):\n",
    "            int_cov = covariates\n",
    "        else:\n",
    "            int_cov = np.column_stack((np.ones(n), covariates))\n",
    "\n",
    "    # Weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "    elif np.min(i_weights) < 0:\n",
    "        msg = \"i.weights must be non-negative\"\n",
    "        raise ValueError(msg)\n",
    "    i_weights = np.asarray(i_weights)\n",
    "\n",
    "    # Pscore estimation (neural network) and fitted values\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(int_cov.shape[1],)))\n",
    "    model.add(layers.Dense(10, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    model.fit(int_cov, D, epochs=50, verbose=0)\n",
    "\n",
    "    ps_fit = model.predict(int_cov).flatten()\n",
    "    ps_fit = np.clip(ps_fit, 1e-16, 1 - 1e-16)\n",
    "\n",
    "    # Compute IPW estimator weights\n",
    "    w_treat_pre = i_weights * D * (1 - post)\n",
    "    w_treat_post = i_weights * D * post\n",
    "    w_cont_pre = i_weights * ps_fit * (1 - D) * (1 - post) / (1 - ps_fit)\n",
    "    w_cont_post = i_weights * ps_fit * (1 - D) * post / (1 - ps_fit)\n",
    "\n",
    "    # Elements of the influence function (summands)\n",
    "    eta_treat_pre = w_treat_pre * y / np.mean(w_treat_pre)\n",
    "    eta_treat_post = w_treat_post * y / np.mean(w_treat_post)\n",
    "    eta_cont_pre = w_cont_pre * y / np.mean(w_cont_pre)\n",
    "    eta_cont_post = w_cont_post * y / np.mean(w_cont_post)\n",
    "\n",
    "    # Estimator of each component\n",
    "    att_treat_pre = np.mean(eta_treat_pre)\n",
    "    att_treat_post = np.mean(eta_treat_post)\n",
    "    att_cont_pre = np.mean(eta_cont_pre)\n",
    "    att_cont_post = np.mean(eta_cont_post)\n",
    "\n",
    "    # ATT estimator\n",
    "    ipw_att = (att_treat_post - att_treat_pre) - (att_cont_post - att_cont_pre)\n",
    "\n",
    "    # Influence function to compute standard error\n",
    "    score_ps = i_weights[:, None] * (D - ps_fit)[:, None] * int_cov\n",
    "    Hessian_ps = np.linalg.inv(np.dot(int_cov.T, score_ps))\n",
    "    asy_lin_rep_ps = np.dot(score_ps, Hessian_ps)\n",
    "\n",
    "    # Influence function of the \"treat\" component\n",
    "    inf_treat_pre = eta_treat_pre - w_treat_pre * att_treat_pre / np.mean(w_treat_pre)\n",
    "    inf_treat_post = eta_treat_post - w_treat_post * att_treat_post / np.mean(\n",
    "        w_treat_post,\n",
    "    )\n",
    "    inf_treat = inf_treat_post - inf_treat_pre\n",
    "\n",
    "    # Influence function of the control component\n",
    "    inf_cont_pre = eta_cont_pre - w_cont_pre * att_cont_pre / np.mean(w_cont_pre)\n",
    "    inf_cont_post = eta_cont_post - w_cont_post * att_cont_post / np.mean(w_cont_post)\n",
    "    inf_cont = inf_cont_post - inf_cont_pre\n",
    "\n",
    "    # Estimation effect from gamma hat (pscore)\n",
    "    M2_pre = np.mean(\n",
    "        w_cont_pre[:, None] * (y - att_cont_pre)[:, None] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_pre)\n",
    "    M2_post = np.mean(\n",
    "        w_cont_post[:, None] * (y - att_cont_post)[:, None] * int_cov,\n",
    "        axis=0,\n",
    "    ) / np.mean(w_cont_post)\n",
    "    inf_cont_ps = np.dot(asy_lin_rep_ps, (M2_post - M2_pre))\n",
    "\n",
    "    # Influence function for the control component\n",
    "    inf_cont += inf_cont_ps\n",
    "\n",
    "    # Combine influence functions\n",
    "    att_inf_func = inf_treat - inf_cont\n",
    "\n",
    "    if not boot:\n",
    "        # Standard error\n",
    "        se_att = np.std(att_inf_func) / np.sqrt(n)\n",
    "        uci = ipw_att + 1.96 * se_att\n",
    "        lci = ipw_att - 1.96 * se_att\n",
    "        ipw_boot = None\n",
    "    else:\n",
    "        if nboot is None:\n",
    "            nboot = 999\n",
    "        if boot_type == \"multiplier\":\n",
    "            # Multiplier bootstrap\n",
    "            ipw_boot = mboot_did(att_inf_func, nboot)\n",
    "            se_att = iqr(ipw_boot) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.quantile(np.abs(ipw_boot / se_att), 0.95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "        else:\n",
    "            # Weighted bootstrap\n",
    "            ipw_boot = [\n",
    "                wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights)\n",
    "                for _ in range(nboot)\n",
    "            ]\n",
    "            ipw_boot = np.array(ipw_boot)\n",
    "            se_att = iqr(ipw_boot - ipw_att) / (norm.ppf(0.75) - norm.ppf(0.25))\n",
    "            cv = np.quantile(np.abs((ipw_boot - ipw_att) / se_att), 0.95)\n",
    "            uci = ipw_att + cv * se_att\n",
    "            lci = ipw_att - cv * se_att\n",
    "\n",
    "    if not inffunc:\n",
    "        att_inf_func = None\n",
    "\n",
    "    return {\n",
    "        \"ATT\": ipw_att,\n",
    "        \"se\": se_att,\n",
    "        \"uci\": uci,\n",
    "        \"lci\": lci,\n",
    "        \"boots\": ipw_boot,\n",
    "        \"att_inf_func\": att_inf_func,\n",
    "    }\n",
    "\n",
    "\n",
    "def mboot_did(att_inf_func, nboot):\n",
    "    n = len(att_inf_func)\n",
    "    boots = []\n",
    "    for _ in range(nboot):\n",
    "        weights = np.random.exponential(scale=1.0, size=n)\n",
    "        boot_att = np.sum(weights * att_inf_func) / np.sum(weights)\n",
    "        boots.append(boot_att)\n",
    "    return np.array(boots)\n",
    "\n",
    "\n",
    "def wboot_std_ipw_rc(n, y, post, D, int_cov, i_weights):\n",
    "    indices = np.random.choice(n, n, replace=True)\n",
    "    y_boot = y[indices]\n",
    "    post_boot = post[indices]\n",
    "    D_boot = D[indices]\n",
    "    int_cov_boot = int_cov[indices]\n",
    "    i_weights_boot = i_weights[indices]\n",
    "    return std_ipw_did_rc(\n",
    "        y_boot,\n",
    "        post_boot,\n",
    "        D_boot,\n",
    "        int_cov_boot,\n",
    "        i_weights_boot,\n",
    "        boot=False,\n",
    "    )[\"ATT\"]\n",
    "\n",
    "\n",
    "# Simulation with 10 runs\n",
    "n_runs = 100\n",
    "true_att = 0  # true average treatment effect\n",
    "\n",
    "att_estimates = []\n",
    "se_estimates = []\n",
    "uci_estimates = []\n",
    "lci_estimates = []\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    x, y, d = make_did_SZ2020(\n",
    "        n_obs=1000,\n",
    "        dgp_type=4,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"array\",\n",
    "    )\n",
    "    covariates = x\n",
    "    post = np.concatenate([np.zeros(500), np.ones(500)])\n",
    "\n",
    "    results = std_ipw_did_rc(y, post, d, covariates)\n",
    "\n",
    "    att_estimates.append(results[\"ATT\"])\n",
    "    se_estimates.append(results[\"se\"])\n",
    "    uci_estimates.append(results[\"uci\"])\n",
    "    lci_estimates.append(results[\"lci\"])\n",
    "\n",
    "att_estimates = np.array(att_estimates)\n",
    "se_estimates = np.array(se_estimates)\n",
    "uci_estimates = np.array(uci_estimates)\n",
    "lci_estimates = np.array(lci_estimates)\n",
    "\n",
    "# Calculate measures\n",
    "biases = att_estimates - true_att\n",
    "avg_bias = np.mean(biases)\n",
    "med_bias = np.median(biases)\n",
    "rmse = np.sqrt(np.mean(biases**2))\n",
    "var_att = np.mean(np.var(att_estimates))\n",
    "ci_lengths = uci_estimates - lci_estimates\n",
    "coverage = np.mean((lci_estimates <= true_att) & (uci_estimates >= true_att))\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(var_att),\n",
    ")  # Length of 95% confidence interval\n",
    "\n",
    "measures = {\n",
    "    \"Average Bias\": avg_bias,\n",
    "    \"Median Bias\": med_bias,\n",
    "    \"RMSE\": rmse,\n",
    "    \"Variance of ATT\": var_att,\n",
    "    \"Coverage\": coverage,\n",
    "    \"Confidence Interval Length\": avg_ci_length,\n",
    "}\n",
    "\n",
    "# Print measures\n",
    "for measure, value in measures.items():\n",
    "    print(f\"{measure}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 3 epochs\n",
    "Average Bias: -0.7969437934813081\n",
    "Median Bias: -0.07284544806852011\n",
    "RMSE: 6.650149057545116\n",
    "Variance of ATT: 43.58936307759981\n",
    "Coverage: 0.91\n",
    "Confidence Interval Length: 25.880718475259332\n",
    "\n",
    "with 50epochs\n",
    "Average Bias: 0.1748598802427668\n",
    "Median Bias: 0.6697785194841401\n",
    "RMSE: 5.236229617457072\n",
    "Variance of ATT: 27.38752462901612\n",
    "Coverage: 0.94\n",
    "Confidence Interval Length: 20.514571856592898\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try it the code and deep learning and double robust from paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tommaso-manfe/DR-DIPW-Updated-Beyond-Regression-DiD/blob/main/Simulation%20Github/drdid_imp_rc.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulaization Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_pa = pd.DataFrame(ATTE_estimates, columns=[\"Estimate\"])\n",
    "g = sns.kdeplot(df_pa, fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning without double robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the paper of the phd guys how they did the IPW or essentially the propensity score matching, they recreated Abadie et al. 2005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the DGPs for cross sectional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import logistic\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Define parameters\n",
    "n = 1000  # Sample size\n",
    "Xsi_ps = 0.75  # pscore index\n",
    "_lambda = 0.5  # Proportion in each period\n",
    "nboot = 199  # Number of bootstrapped draws\n",
    "\n",
    "# Define means and standard deviations\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "# Loop for 1000 runs\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Propensity score\n",
    "    pi = logistic.cdf(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "    index_unobs_het = d * (index_lin)\n",
    "    index_att = 0\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Generate unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Generate outcomes at time 0 and time 1\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "    y01 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    y11 = index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = (\n",
    "        d * post * y11\n",
    "        + (1 - d) * post * y01\n",
    "        + (1 - d) * (1 - post) * y00\n",
    "        + (d) * (1 - post) * y10\n",
    "    )\n",
    "\n",
    "    # Generate id\n",
    "    id_ = np.arange(1, n + 1)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"y\": y,\n",
    "            \"post\": post.astype(int),\n",
    "            \"d\": d.astype(int),\n",
    "            \"x1\": z1,\n",
    "            \"x2\": z2,\n",
    "            \"x3\": z3,\n",
    "            \"x4\": z4,\n",
    "        },\n",
    "    )\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "\n",
    "    dta_long = dta_long.sort_values(by=\"id\")\n",
    "\n",
    "    # Perform TWFE estimation\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "\n",
    "    # Calculate asymptotic variance\n",
    "    asymptotic_variance.append(twfe_i.cov_params()[\"post:d\"])\n",
    "\n",
    "    # Bootstrap for confidence intervals\n",
    "    # (Note: This part is not included in the provided code, you'll need to implement it separately)\n",
    "\n",
    "    # Append TWFE estimate to the list\n",
    "    ATTE_estimates.append(twfe)\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "asymptotic_variance = asymptotic_variance.reshape(-1, 8)\n",
    "asymptotic_variance = asymptotic_variance[:, 0]\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(\n",
    "    np.mean((ATTE_estimates - 0) ** 2),\n",
    ")  # Assuming ATTE is 0 as mentioned in the code\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(\n",
    "    ((ATTE_estimates - 1.96 * np.sqrt(asymptotic_variance)) <= 0)\n",
    "    & ((ATTE_estimates + 1.96 * np.sqrt(asymptotic_variance)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(asymptotic_variance),\n",
    ")  # Length of 95% confidence interval\n",
    "average_ATT_estimate = np.mean(ATTE_estimates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ATT estimate across all runs: {average_ATT_estimate}\")\n",
    "\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import logistic\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "# Proportion in each period\n",
    "_lambda = 0.5\n",
    "# Number of bootstrapped draws\n",
    "nboot = 199\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Generate treatment groups\n",
    "    # Propensity score\n",
    "    pi = logistic.cdf(Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # Create heterogenenous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * (index_lin)\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * z1 + 13.7 * (z2 + z3 + z4)\n",
    "\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    # First let's generate potential outcomes: y_1_potential\n",
    "    y01 = (\n",
    "        index_lin + v + np.random.normal(scale=1, size=n) + index_trend\n",
    "    )  # This is the baseline\n",
    "    y11 = (\n",
    "        index_lin + v + np.random.normal(scale=1, size=n) + index_trend + index_att\n",
    "    )  # This is the baseline\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = (\n",
    "        d * post * y11\n",
    "        + (1 - d) * post * y01\n",
    "        + (1 - d) * (1 - post) * y00\n",
    "        + (d) * (1 - post) * y10\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.arange(1, n + 1)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"y\": y,\n",
    "            \"post\": post.astype(int),\n",
    "            \"d\": d.astype(int),\n",
    "            \"x1\": z1,\n",
    "            \"x2\": z2,\n",
    "            \"x3\": z3,\n",
    "            \"x4\": z4,\n",
    "        },\n",
    "    )\n",
    "    dta_long = dta_long.sort_values(by=\"id\")\n",
    "\n",
    "    # Create interaction term 'post:d'\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "\n",
    "    asymptotic_variance.append(\n",
    "        twfe_i.HC0_se[\"post:d\"],\n",
    "    )  # Append the standard error for the interaction term\n",
    "\n",
    "\n",
    "# Convert lists to arrays for calculations\n",
    "twfe = np.array(twfe)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(twfe - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(twfe - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(np.mean((twfe - 0) ** 2))  # Assuming ATTE is 0 as mentioned in the code\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(\n",
    "    ((twfe - 1.96 * np.sqrt(asymptotic_variance)) <= 0)\n",
    "    & ((twfe + 1.96 * np.sqrt(asymptotic_variance)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(asymptotic_variance),\n",
    ")  # Length of 95% confidence interval\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "\n",
    "# Sample size\n",
    "n = 1000\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "# Proportion in each period\n",
    "_lambda = 0.5\n",
    "nboot = 199  # Number of bootstrapped draws\n",
    "\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Generate covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Propensity score\n",
    "    pi = 1 / (1 + np.exp(Xsi_ps * (-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4)))\n",
    "    d = np.random.uniform(size=n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "    # Create heterogenenous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * index_lin\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    y01 = index_lin + v + np.random.normal(size=n) + index_trend  # This is the baseline\n",
    "    y11 = (\n",
    "        index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "    )  # This is the baseline\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.uniform(size=n) <= ti\n",
    "\n",
    "    y = (\n",
    "        d * post * y11\n",
    "        + (1 - d) * post * y01\n",
    "        + (1 - d) * (1 - post) * y00\n",
    "        + (d) * (1 - post) * y10\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.arange(1, n + 1)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"y\": y,\n",
    "            \"post\": post.astype(int),\n",
    "            \"d\": d.astype(int),\n",
    "            \"x1\": z1,\n",
    "            \"x2\": z2,\n",
    "            \"x3\": z3,\n",
    "            \"x4\": z4,\n",
    "        },\n",
    "    )\n",
    "    dta_long = dta_long.sort_values(by=\"id\")\n",
    "    # Create interaction term 'post:d'\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "    # Standard TWFE\n",
    "\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "    asymptotic_variance.append(twfe_i.cov_params()[\"post:d\"])\n",
    "    # Convert lists to arrays for calculations\n",
    "    ATTE_estimates.append(twfe)\n",
    "\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "asymptotic_variance = asymptotic_variance.reshape(-1, 8)\n",
    "asymptotic_variance = asymptotic_variance[:, 0]\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(\n",
    "    np.mean((ATTE_estimates - 0) ** 2),\n",
    ")  # Assuming ATTE is 0 as mentioned in the code\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(\n",
    "    ((ATTE_estimates - 1.96 * np.sqrt(asymptotic_variance)) <= 0)\n",
    "    & ((ATTE_estimates + 1.96 * np.sqrt(asymptotic_variance)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(asymptotic_variance),\n",
    ")  # Length of 95% confidence interval\n",
    "average_ATT_estimate = np.mean(ATTE_estimates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ATT estimate across all runs: {average_ATT_estimate}\")\n",
    "\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGP 4 cross sectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(42)  # You can use any integer value as the seed\n",
    "# Sample size\n",
    "n = 1000\n",
    "\n",
    "# pscore index (strength of common support)\n",
    "Xsi_ps = 0.75\n",
    "\n",
    "# Proportion in each period\n",
    "lambda_ = 0.5\n",
    "\n",
    "# Number of bootstrapped draws\n",
    "nboot = 199\n",
    "\n",
    "# Mean and Std deviation of Z's without truncation\n",
    "mean_z1 = np.exp(0.25 / 2)\n",
    "sd_z1 = np.sqrt((np.exp(0.25) - 1) * np.exp(0.25))\n",
    "mean_z2 = 10\n",
    "sd_z2 = 0.54164\n",
    "mean_z3 = 0.21887\n",
    "sd_z3 = 0.04453\n",
    "mean_z4 = 402\n",
    "sd_z4 = 56.63891\n",
    "\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "ATTE_estimates = []\n",
    "asymptotic_variance = []\n",
    "coverage = []\n",
    "ci_length = []\n",
    "\n",
    "\n",
    "for _i in range(1000):\n",
    "    # Gen covariates\n",
    "    x1 = np.random.normal(0, 1, n)\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "    x3 = np.random.normal(0, 1, n)\n",
    "    x4 = np.random.normal(0, 1, n)\n",
    "\n",
    "    z1 = np.exp(x1 / 2)\n",
    "    z2 = x2 / (1 + np.exp(x1)) + 10\n",
    "    z3 = (x1 * x3 / 25 + 0.6) ** 3\n",
    "    z4 = (x1 + x4 + 20) ** 2\n",
    "\n",
    "    z1 = (z1 - mean_z1) / sd_z1\n",
    "    z2 = (z2 - mean_z2) / sd_z2\n",
    "    z3 = (z3 - mean_z3) / sd_z3\n",
    "    z4 = (z4 - mean_z4) / sd_z4\n",
    "\n",
    "    x = np.column_stack((x1, x2, x3, x4))\n",
    "    z = np.column_stack((z1, z2, z3, z4))\n",
    "\n",
    "    # Gen treatment groups\n",
    "    # Propensity score\n",
    "    pi = 1 / (1 + np.exp(-Xsi_ps * (-x1 + 0.5 * x2 - 0.25 * x3 - 0.1 * x4)))\n",
    "    d = np.random.rand(n) <= pi\n",
    "\n",
    "    # Generate aux indexes for the potential outcomes\n",
    "    index_lin = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "    # Create heterogeneous effects for the ATT, which is set approximately equal to zero\n",
    "    index_unobs_het = d * index_lin\n",
    "    index_att = 0\n",
    "\n",
    "    # This is the key for consistency of outcome regression\n",
    "    index_trend = 210 + 27.4 * x1 + 13.7 * (x2 + x3 + x4)\n",
    "\n",
    "    # v is the unobserved heterogeneity\n",
    "    v = np.random.normal(index_unobs_het, 1, n)\n",
    "\n",
    "    # Gen realized outcome at time 0\n",
    "    y00 = index_lin + v + np.random.normal(size=n)\n",
    "    y10 = index_lin + v + np.random.normal(size=n)\n",
    "\n",
    "    # Gen outcomes at time 1\n",
    "    # First let's generate potential outcomes: y_1_potential\n",
    "    y01 = index_lin + v + np.random.normal(size=n) + index_trend\n",
    "    y11 = index_lin + v + np.random.normal(size=n) + index_trend + index_att\n",
    "\n",
    "    # Generate \"T\"\n",
    "    ti_nt = 0.5\n",
    "    ti_t = 0.5\n",
    "    ti = d * ti_t + (1 - d) * ti_nt\n",
    "    post = np.random.rand(n) <= ti\n",
    "\n",
    "    y = (\n",
    "        d * post * y11\n",
    "        + (1 - d) * post * y01\n",
    "        + (1 - d) * (1 - post) * y00\n",
    "        + d * (1 - post) * y10\n",
    "    )\n",
    "\n",
    "    # Gen id\n",
    "    id_ = np.arange(1, n + 1)\n",
    "\n",
    "    # Put in a long data frame\n",
    "    dta_long = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"y\": y,\n",
    "            \"post\": post.astype(int),\n",
    "            \"d\": d.astype(int),\n",
    "            \"x1\": z1,\n",
    "            \"x2\": z2,\n",
    "            \"x3\": z3,\n",
    "            \"x4\": z4,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    dta_long = dta_long.sort_values(\"id\")\n",
    "    dta_long[\"post:d\"] = dta_long[\"post\"] * dta_long[\"d\"]\n",
    "    # Standard TWFE\n",
    "    twfe_i = sm.OLS(\n",
    "        dta_long[\"y\"],\n",
    "        sm.add_constant(dta_long[[\"x1\", \"x2\", \"x3\", \"x4\", \"post\", \"d\", \"post:d\"]]),\n",
    "    ).fit()\n",
    "    twfe = twfe_i.params[\"post:d\"]\n",
    "    asymptotic_variance.append(twfe_i.cov_params()[\"post:d\"])\n",
    "    ATTE_estimates.append(twfe)\n",
    "# Convert lists to arrays for calculations\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "ATTE_estimates = np.array(ATTE_estimates)\n",
    "asymptotic_variance = np.array(asymptotic_variance)\n",
    "asymptotic_variance = asymptotic_variance.reshape(-1, 8)\n",
    "asymptotic_variance = asymptotic_variance[:, 0]\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "med_bias = np.median(ATTE_estimates - 0)  # Assuming ATTE is 0 as mentioned in the code\n",
    "rmse = np.sqrt(\n",
    "    np.mean((ATTE_estimates - 0) ** 2),\n",
    ")  # Assuming ATTE is 0 as mentioned in the code\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(\n",
    "    ((ATTE_estimates - 1.96 * np.sqrt(asymptotic_variance)) <= 0)\n",
    "    & ((ATTE_estimates + 1.96 * np.sqrt(asymptotic_variance)) >= 0),\n",
    ")\n",
    "avg_ci_length = np.mean(\n",
    "    2 * 1.96 * np.sqrt(asymptotic_variance),\n",
    ")  # Length of 95% confidence interval\n",
    "average_ATT_estimate = np.mean(ATTE_estimates)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ATT estimate across all runs: {average_ATT_estimate}\")\n",
    "\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
