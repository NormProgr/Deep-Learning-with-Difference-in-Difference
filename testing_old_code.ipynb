{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stopping Tensorflow from printing info messages and warnings.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.special import comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"tensorflow\").disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "The code was made for tensorflow 1 and not for tensorflow 2. I need to fix the parser so that it works for further code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_boolean(\"update\", False, \"\"\"Record the simulation results.\"\"\")\n",
    "tf.flags.DEFINE_boolean(\"plot_true\", False, \"\"\"Show plots.\"\"\")\n",
    "tf.flags.DEFINE_boolean(\"verbose\", True, \"\"\"Show detailed messages.\"\"\")\n",
    "tf.flags.DEFINE_integer(\"nsimulations\", 1, \"\"\"How many simulations to run.\"\"\")\n",
    "tf.flags.DEFINE_integer(\n",
    "    \"nconsumer_characteristics\",\n",
    "    100,\n",
    "    \"\"\"Number of consumer characteristics.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"treatment\",\n",
    "    \"not_random\",\n",
    "    \"\"\"Are customers treated at random or not.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"model\",\n",
    "    \"quadratic\",\n",
    "    \"\"\"Is the mapping from consumer characteristics\n",
    "                       to their preferences linear or quadratic.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"architecture\",\n",
    "    \"architecture_1_\",\n",
    "    \"\"\"Which NN architecture to use.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_integer(\"data_seed\", None, \"\"\"Seed to use to create fake data.\"\"\")\n",
    "\n",
    "# Manually set sys.argv to simulate command-line invocation\n",
    "sys.argv = [sys.argv[0]]\n",
    "\n",
    "# Parse flags\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "# Access remaining arguments after parsing flags\n",
    "remaining_args = [arg for arg in sys.argv[1:] if arg.startswith(\"--\")]\n",
    "assert remaining_args == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different architectures for the first NN\n",
    "if FLAGS.architecture == \"architecture_1_\":\n",
    "    hidden_layer_sizes = [20, 10, 5]\n",
    "    dropout_rates_train = [0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_2_\":\n",
    "    hidden_layer_sizes = [60, 30, 20]\n",
    "    dropout_rates_train = [0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_3_\":\n",
    "    hidden_layer_sizes = [80, 80, 80]\n",
    "    dropout_rates_train = [0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_4_\":\n",
    "    hidden_layer_sizes = [20, 15, 10, 5]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_5_\":\n",
    "    hidden_layer_sizes = [60, 30, 20, 10]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_6_\":\n",
    "    hidden_layer_sizes = [80, 80, 80, 80]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_7_\":\n",
    "    hidden_layer_sizes = [20, 15, 15, 10, 10, 5]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_8_\":\n",
    "    hidden_layer_sizes = [60, 30, 20, 20, 10, 5]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_9_\":\n",
    "    hidden_layer_sizes = [80, 80, 80, 80, 80, 80]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "else:\n",
    "    msg = \"Architecture not found! Check the spelling.\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "if FLAGS.nconsumer_characteristics < 20:\n",
    "    raise ValueError(\n",
    "        \"Number of consumer characteristics \" + \"should not be less than 20.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates_test = [0 for i in dropout_rates_train]\n",
    "\n",
    "# Architecture for the second NN that estimates\n",
    "# propensity scores\n",
    "hidden_layer_sizes_treatment = [50, 30]\n",
    "activation_functions_treatment = [\"relu\", \"relu\", \"none\"]\n",
    "dropout_rates_train_treatment = [0, 0, 0]\n",
    "dropout_rates_test_treatment = [0 for i in dropout_rates_train_treatment]\n",
    "\n",
    "# Setting parameters values for generating fake data\n",
    "nconsumers = 10000\n",
    "\n",
    "# Run parameters\n",
    "train_proportion = 0.9\n",
    "max_nepochs = 5000\n",
    "max_epochs_without_change = 30\n",
    "\n",
    "early_stopping = train_proportion != 1\n",
    "\n",
    "optimizer = \"Adam\"\n",
    "learning_rate = 0.009\n",
    "batch_size = 128\n",
    "batch_size_t = None\n",
    "\n",
    "# Regularization parameters\n",
    "alpha = 0.0\n",
    "r = 0.2\n",
    "\n",
    "# Checking for spelling errors\n",
    "if not (FLAGS.model == \"quadratic\" or FLAGS.model == \"simple\"):\n",
    "    msg = \"Check whether model type is spelled correctly!\"\n",
    "    raise ValueError(msg)\n",
    "if not (FLAGS.treatment == \"random\" or FLAGS.treatment == \"not_random\"):\n",
    "    msg = \"Check whether treatment type is spelled correctly!\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train = T_train = Y_train = X_valid = T_valid = Y_valid = X = T_real = Y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_time():\n",
    "    \"\"\"Print the time passed since start_time.\"\"\"\n",
    "    end_time = time.time()\n",
    "    hours = int((end_time - start_time) / 3600)\n",
    "    minutes = int((end_time - start_time) % 3600 / 60)\n",
    "    seconds = int(end_time - start_time - (3600 * hours + 60 * minutes))\n",
    "    print(\n",
    "        f\"Running time is: {hours} hours, {minutes} minutes and {seconds} seconds\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeData:\n",
    "    \"\"\"Create an artificial dataset with desired properties for testing\n",
    "    the NN method.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        N: int\n",
    "            Number of consumers.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "        data seed: int or None\n",
    "            Seed used to create fake dataset. If it is set to None\n",
    "            than random seed is used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        N=nconsumers,\n",
    "        nconsumer_characteristics=FLAGS.nconsumer_characteristics,\n",
    "        data_seed=FLAGS.data_seed,\n",
    "    ):\n",
    "        self.N = N\n",
    "        self.nconsumer_characteristics = nconsumer_characteristics\n",
    "        self.seed = data_seed\n",
    "\n",
    "        # Creating fake data variables\n",
    "        self.Y = None\n",
    "        self.X = None\n",
    "        self.mu0 = None\n",
    "        self.tau = None\n",
    "        self.T = None\n",
    "        self.prob_of_T = None\n",
    "        self.tau_true_mean = None\n",
    "\n",
    "    def _sum_polynomial_X_times_weights(self, weights):\n",
    "        \"\"\"Evaluate the non-linear part of a quadratic polynomial in\n",
    "        consumer characteristics, self.X, with prescribed weights, w.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            weights: ndarray, shape = (num_additional_poly_terms, )\n",
    "                Weights corresponding to quadratic terms.\n",
    "        Outputs:\n",
    "        -------\n",
    "            sum_x: ndarray, shape = (N, 1)\n",
    "                Non-linear part of the quadratic polynomial evaluated\n",
    "                for each consumer.\n",
    "        \"\"\"\n",
    "        my_polynomial_indices = combinations_with_replacement(\n",
    "            list(range(self.nconsumer_characteristics)),\n",
    "            2,\n",
    "        )\n",
    "        i = 0\n",
    "        sum_x = 0\n",
    "        for p in my_polynomial_indices:\n",
    "            sum_x = sum_x + weights[i] * np.multiply(self.X[:, p[0]], self.X[:, p[1]])\n",
    "            i += 1\n",
    "        return sum_x.reshape(-1, 1)\n",
    "\n",
    "    def _create_TE_coefs(self, model):\n",
    "        \"\"\"Create treatment effect coefficients.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            model: {'simple', 'quadratic'}\n",
    "                If 'simple' coefficients a and b in the artificial\n",
    "                dataset depend linearly on consumer characteristics.\n",
    "                Otherwise, the dependence is quadratic.\n",
    "        Outputs:\n",
    "        -------\n",
    "            bias_tau: float\n",
    "                Constant term in equation for tau.\n",
    "            alpha_tau: ndarray, shape = [nconsumer_characteristics, 1]\n",
    "                Linear coefficients in equation for tau.\n",
    "            beta_tau: ndarray, shape = [count]\n",
    "                Quadratic coefficients in equation for tau.\n",
    "                Count is the number of the second degree terms in a\n",
    "                quadratic polynomial where the number of variables is\n",
    "                equal to the number of consumer characteristics.\n",
    "        \"\"\"\n",
    "        np.random.seed(63)\n",
    "\n",
    "        # Calculating tau\n",
    "        alpha_tau = np.random.uniform(\n",
    "            low=0.1,\n",
    "            high=0.22,\n",
    "            size=[self.nconsumer_characteristics, 1],\n",
    "        )\n",
    "        bias_tau = -0.05\n",
    "        self.tau = np.dot(self.X, alpha_tau) + bias_tau\n",
    "\n",
    "        if model == \"quadratic\":\n",
    "            count = comb(self.nconsumer_characteristics, 2, True, True)\n",
    "            beta_tau = np.random.uniform(low=-0.05, high=0.06, size=count)\n",
    "            self.tau = self.tau + self._sum_polynomial_X_times_weights(beta_tau)\n",
    "        else:\n",
    "            beta_tau = None\n",
    "\n",
    "        # Calculating mu0\n",
    "        alpha_mu0 = np.random.normal(\n",
    "            loc=0.3,\n",
    "            scale=0.7,\n",
    "            size=[1, self.nconsumer_characteristics],\n",
    "        )\n",
    "        bias_mu0 = 0.09\n",
    "        self.mu0 = np.dot(self.X, alpha_mu0.T) + bias_mu0\n",
    "\n",
    "        if model == \"quadratic\":\n",
    "            beta_mu0 = np.random.normal(loc=0.01, scale=0.3, size=count)\n",
    "            self.mu0 = self.mu0 + self._sum_polynomial_X_times_weights(beta_mu0)\n",
    "        return alpha_tau, bias_tau, beta_tau\n",
    "\n",
    "    def _calculate_true_tau_mean(self, alpha_tau, bias_tau, beta_tau, model):\n",
    "        \"\"\"Calculate true average treatment effect.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            bias_tau: float\n",
    "                Constant term in equation for tau.\n",
    "            alpha_tau: ndarray, shape = [nconsumer_characteristics, 1]\n",
    "                Linear coefficients in equation for tau.\n",
    "            beta_tau: ndarray, shape = [count]\n",
    "                Quadratic coefficients in equation for tau.\n",
    "                Count is the number of the second degree terms in a\n",
    "                quadratic polynomial where the number of variables is\n",
    "                equal to the number of consumer characteristics.\n",
    "            model: {'simple', 'quadratic'}\n",
    "                If 'simple' coefficients a and b in the artificial\n",
    "                dataset depend linearly on consumer characteristics.\n",
    "                Otherwise, the dependence is quadratic.\n",
    "        \"\"\"\n",
    "        X = 0.5\n",
    "\n",
    "        self.tau_true_mean = np.sum(X * alpha_tau) + bias_tau\n",
    "\n",
    "        if model == \"quadratic\":\n",
    "            X_poly = 0.25 * np.ones(len(beta_tau))\n",
    "            s = 0\n",
    "            for i in range(self.nconsumer_characteristics):\n",
    "                X_poly[s] = 1 / 3.0\n",
    "                s = s + self.nconsumer_characteristics - i\n",
    "\n",
    "            self.tau_true_mean = self.tau_true_mean + np.sum(X_poly * beta_tau)\n",
    "\n",
    "    def _create_propensity_scores(self, treatment):\n",
    "        \"\"\"Calculate propensity scores and create treatment variable for\n",
    "        our fake dataset.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            treatment: {'random', 'not_random'}\n",
    "                    If 'random' consumers are being treated at random.\n",
    "                    Otherwise, probability of being treated is a function\n",
    "                    of consumer characteristics.\n",
    "        \"\"\"\n",
    "        if treatment == \"random\":\n",
    "            self.prob_of_T = 0.5\n",
    "            self.T = np.random.binomial(size=self.N, n=1, p=self.prob_of_T).reshape(\n",
    "                self.N,\n",
    "                1,\n",
    "            )\n",
    "        else:\n",
    "            bias_p = 0.09\n",
    "            np.random.seed(72)\n",
    "            alpha_p = np.random.uniform(low=-0.55, high=0.55, size=[20, 1])\n",
    "            # Probability of t only depends on the first 20 consumers\n",
    "            # features\n",
    "            p_of_t = np.dot(self.X[:, :20], alpha_p) + bias_p\n",
    "            p_of_t = p_of_t.reshape(-1)\n",
    "            self.prob_of_T = 1 / (1 + np.exp(-p_of_t))\n",
    "            self.T = np.random.binomial(size=self.N, n=1, p=self.prob_of_T).reshape(\n",
    "                self.N,\n",
    "                1,\n",
    "            )\n",
    "\n",
    "    def create_fake_data(\n",
    "        self,\n",
    "        model=FLAGS.model,\n",
    "        verbose=FLAGS.verbose,\n",
    "        treatment=FLAGS.treatment,\n",
    "    ):\n",
    "        \"\"\"Create an artificial dataset.\n",
    "\n",
    "        Consumer characteristics, X, and normal errors are generated for\n",
    "        each consumer randomly. Then coefficients mu0 and tau are\n",
    "        created as functions of consumer characteristics, X.\n",
    "\n",
    "        If treatment == 'random' all the consumers are treated with an\n",
    "        equal likelihood of 0.5. Otherwise, the propensity scores that\n",
    "        depend on consumer characteristics are calculated.\n",
    "\n",
    "        Finally, a target variable is created.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            model: {'simple', 'quadratic'}\n",
    "                If 'simple' coefficients a and b in the artificial\n",
    "                dataset depend linearly on consumer characteristics.\n",
    "                Otherwise, the dependence is quadratic.\n",
    "            verbose: bool\n",
    "                If True print detailed messages.\n",
    "            treatment: {'random', 'not_random'}\n",
    "                If 'random' consumers are being treated at random.\n",
    "                Otherwise, probability of being treated is a function\n",
    "                of consumer characteristics.\n",
    "        Outputs:\n",
    "        -------\n",
    "            self.Y: ndarray, shape = (N, 1)\n",
    "                Target value.\n",
    "            self.X: ndarray, shape = (N, nconsumer_characteristics)\n",
    "                Consumer characteristics.\n",
    "            self.mu0: ndarray, shape = (N, 1)\n",
    "                Mu0 for each consumer.\n",
    "            self.tau: ndarray, shape = (N, 1)\n",
    "                Tau for each consumer.\n",
    "            self.T: ndarray, shape = (N, 1)\n",
    "                Treatment for each consumer.\n",
    "            self.seed: int\n",
    "                Random seed used to create fake dataset.\n",
    "            self.prob_of_T: ndarray, shape = (N,)  or 0.5\n",
    "                Propensity scores for each consumer if treatment\n",
    "                variable is set to 'not_random'.\n",
    "            self.tau_true_mean: float\n",
    "                True average treatment effect.\n",
    "        \"\"\"\n",
    "        if self.seed is None:\n",
    "            self.seed = random.randint(1, 100000)\n",
    "        if verbose:\n",
    "            print(\"Seed number is: \", self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        self.X = np.random.uniform(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            size=[self.N, self.nconsumer_characteristics],\n",
    "        )\n",
    "        normal_errors = np.random.normal(size=[self.N, 1], loc=0.0, scale=1.0)\n",
    "        alpha_tau, bias_tau, beta_tau = self._create_TE_coefs(model)\n",
    "        self._calculate_true_tau_mean(alpha_tau, bias_tau, beta_tau, model)\n",
    "        self._create_propensity_scores(treatment)\n",
    "        self.Y = self.mu0 + self.tau * self.T + normal_errors\n",
    "        return (\n",
    "            self.Y,\n",
    "            self.X,\n",
    "            self.mu0,\n",
    "            self.tau,\n",
    "            self.T,\n",
    "            self.seed,\n",
    "            self.prob_of_T,\n",
    "            self.tau_true_mean,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_inds(t):\n",
    "    \"\"\"Split the dataset into training and validation sets while\n",
    "    preserving the proportion of targeted customers in both datasets.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        t: array-like, shape=(N, 1)\n",
    "            Treatment array.\n",
    "    Outputs:\n",
    "    -------\n",
    "        train_inds: array of bools\n",
    "            Indices of the training set.\n",
    "        valid_inds: array of bools\n",
    "            Indices of the validation set.\n",
    "    \"\"\"\n",
    "    t_array = np.array(t)\n",
    "    train_inds = np.zeros(len(t_array), dtype=bool)\n",
    "    valid_inds = np.zeros(len(t_array), dtype=bool)\n",
    "    values = np.unique(t_array)\n",
    "    for value in values:\n",
    "        value_inds = np.nonzero(t_array == value)[0]\n",
    "        np.random.shuffle(value_inds)\n",
    "        n = int(train_proportion * len(value_inds))\n",
    "        train_inds[value_inds[:n]] = True\n",
    "        valid_inds[value_inds[n:]] = True\n",
    "    return train_inds, valid_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_size(batch_size, X_train):\n",
    "    \"\"\"If batch_size is int than do nothing, else if batch_size is\n",
    "    equal to None, set batch size to be of a size equal to the\n",
    "    length of the training dataset.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        batch_size: int or None\n",
    "            Batch size.\n",
    "        X_train: ndarray\n",
    "            Array of consumer characteristics on which to\n",
    "            perform training.\n",
    "    Outputs:\n",
    "    -------\n",
    "        batch_size: int\n",
    "            Batch size.\n",
    "    \"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = len(X_train)\n",
    "    return batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plu_activation(input_value, alpha_=0.1, c=1):\n",
    "    \"\"\"Apply PLU activation function on the input value.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        input_value: Tensor\n",
    "            An input tensor on which to apply PLU activation function.\n",
    "        alpha: float\n",
    "            First parameter of PLU function.\n",
    "        c: float\n",
    "            Second parameter of PLU function.\n",
    "\n",
    "    Outputs:\n",
    "    -------\n",
    "            Transformed input values after applying PLU activation\n",
    "            function.\n",
    "    \"\"\"\n",
    "    return tf.maximum(\n",
    "        alpha_ * (input_value + c) - c,\n",
    "        tf.minimum(alpha_ * (input_value - c) + c, input_value),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srelu_activation(input_value, scope_name):\n",
    "    \"\"\"Apply S-shaped Rectified Linear activation function on the\n",
    "    input value.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        input_value: Tensor\n",
    "            An input tensor on which to apply SReLU activation.\n",
    "        scope_name: string\n",
    "            Scope name.\n",
    "\n",
    "    Outputs:\n",
    "    -------\n",
    "            Transformed input values after applying SReLU activation\n",
    "            function.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope_name):\n",
    "        t_right = tf.get_variable(\n",
    "            \"t_right\",\n",
    "            input_value.get_shape()[-1],\n",
    "            initializer=tf.constant_initializer(0.0),\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        a_right = tf.get_variable(\n",
    "            \"a_right\",\n",
    "            input_value.get_shape()[-1],\n",
    "            initializer=tf.initializers.random_uniform(minval=0, maxval=1),\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        t_left = tf.get_variable(\n",
    "            \"t_left\",\n",
    "            input_value.get_shape()[-1],\n",
    "            initializer=tf.initializers.random_uniform(minval=0, maxval=5),\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        a_left = tf.get_variable(\n",
    "            \"a_left\",\n",
    "            input_value.get_shape()[-1],\n",
    "            initializer=tf.constant_initializer(1.0),\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "    t_right_actual = t_left + tf.abs(t_right)\n",
    "    y_left_and_center = t_left + tf.keras.activations.relu(\n",
    "        input_value - t_left,\n",
    "        a_left,\n",
    "        t_right_actual - t_left,\n",
    "    )\n",
    "    y_right = tf.nn.relu(input_value - t_right_actual) * a_right\n",
    "    return y_left_and_center + y_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_loss_functions(loss1, loss2=None, add_title=\"\"):\n",
    "    \"\"\"Plot the loss functions.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        loss1: list of floats\n",
    "            First list of recorded losses through epochs.\n",
    "        loss2: list of floats\n",
    "            Second list of recorded losses through epochs.\n",
    "        add_title: string\n",
    "            Addition to the title of the graph.\n",
    "    \"\"\"\n",
    "    if loss2 is None:\n",
    "        loss2 = []\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.clf()\n",
    "    plt.plot(range(len(loss1)), loss1, \"r-\", lw=3)\n",
    "    if early_stopping:\n",
    "        plt.plot(range(len(loss2)), loss2, \"b-\", lw=3)\n",
    "        plt.legend([\"loss on training set\", \"loss on validation set\"])\n",
    "        plt.title(\"Loss on training and validation set\" + add_title, fontsize=14)\n",
    "    else:\n",
    "        plt.legend([\"loss on training set\"])\n",
    "        plt.title(\"Loss on training set\" + add_title, fontsize=14)\n",
    "    plt.xlabel(\"Epoch number\", fontsize=14)\n",
    "    plt.ylabel(\"Loss\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Create a neural network with specified properties.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        hidden_layer_sizes: list of ints\n",
    "            Length of the list defines the number of hidden layers.\n",
    "            Entries of the list define the number of hidden units in\n",
    "            each hidden layer.\n",
    "        activation_functions: list of {'relu', 'lrelu', 'prelu',\n",
    "                                       'srelu', 'plu', 'elu', 'none'}\n",
    "            Activation function for each layer.\n",
    "            Has to be of length len(hidden_layer_sizes) + 1.\n",
    "        dropout_rates_train:  list of floats\n",
    "            Dropout rate to be used during training for each layer.\n",
    "            Has to be of length len(hidden_layer_sizes) + 1.\n",
    "        batch_size: int\n",
    "            Batch size.\n",
    "        size_of_the_output: int\n",
    "            Number of units in the output layer.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "        alpha: float\n",
    "            Regularization strength parameter.\n",
    "        r_par: float\n",
    "            Mixing ratio of Ridge and Lasso regression.\n",
    "            Has to be between 0 and 1.\n",
    "        max_epochs_without_change: int\n",
    "            Number of epochs with no improvement on the validation loss\n",
    "            to wait before stopping the training.\n",
    "        max_nepochs: int\n",
    "            Maximum number of epochs for which NNs will be trained.\n",
    "        optimizer: string\n",
    "            Optimizer\n",
    "        learning_rate: scalar\n",
    "            Learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes,\n",
    "        activation_functions,\n",
    "        dropout_rates_train,\n",
    "        batch_size,\n",
    "        size_of_the_output,\n",
    "        nconsumer_characteristics=FLAGS.nconsumer_characteristics,\n",
    "        alpha=alpha,\n",
    "        r_par=r,\n",
    "        max_epochs_without_change=max_epochs_without_change,\n",
    "        max_nepochs=max_nepochs,\n",
    "        optimizer=optimizer,\n",
    "        learning_rate=learning_rate,\n",
    "    ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation_functions = activation_functions\n",
    "        self.dropout_rates_train = dropout_rates_train\n",
    "        self.dropout_rates_test = [0 for i in dropout_rates_train]\n",
    "        self.batch_size = batch_size\n",
    "        self.size_of_the_output = size_of_the_output\n",
    "        self.nconsumer_characteristics = nconsumer_characteristics\n",
    "        self.alpha = alpha\n",
    "        self.r_par = r_par\n",
    "        self.max_epochs_without_change = max_epochs_without_change\n",
    "        self.max_nepochs = max_nepochs\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _fully_connected_layer_builder(\n",
    "        self,\n",
    "        input_data,\n",
    "        hidden_layer_size,\n",
    "        total_num_features,\n",
    "        scope_name,\n",
    "        activation,\n",
    "        dropout_rate,\n",
    "    ):\n",
    "        \"\"\"Build a fully connected layer within the NN.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            input_data: Tensor\n",
    "                Output from the previous layer.\n",
    "            hidden_layer_size: int\n",
    "                Size of the current layer.\n",
    "            total_num_features: int\n",
    "                Number of units from the previous layer.\n",
    "            scope_name: string\n",
    "                Scope name.\n",
    "            activation: {'relu', 'lrelu', 'prelu', 'srelu',\n",
    "                         'plu', 'elu', 'none'}\n",
    "                Activation function.\n",
    "            dropout_rate: scalar\n",
    "                Dropout rate. Has to be between 0 and 1.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            hid_layer_activation: Tensor\n",
    "                The hidden layer output.\n",
    "        \"\"\"\n",
    "        # Dropout:\n",
    "        input_data = tf.contrib.layers.dropout(\n",
    "            inputs=input_data,\n",
    "            keep_prob=1 - dropout_rate,\n",
    "        )\n",
    "\n",
    "        # Creating weights and bias terms for our fully connected layer\n",
    "        with tf.variable_scope(scope_name):\n",
    "            weights = np.sqrt(2) * tf.get_variable(\n",
    "                \"weights\",\n",
    "                shape=[total_num_features, hidden_layer_size],\n",
    "                initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            )\n",
    "            b = tf.Variable(tf.zeros([hidden_layer_size]), name=\"biases\")\n",
    "\n",
    "        # Defining the fully connected neural network layer\n",
    "        hid_layer_activation = tf.matmul(input_data, weights) + b\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            hid_layer_activation = tf.nn.relu(hid_layer_activation)\n",
    "        elif activation == \"lrelu\":\n",
    "            hid_layer_activation = tf.nn.leaky_relu(\n",
    "                hid_layer_activation,\n",
    "                alpha=0.2,\n",
    "                name=\"lrelu\",\n",
    "            )\n",
    "        elif activation == \"prelu\":\n",
    "            prelu_act = tf.keras.layers.PReLU()\n",
    "            hid_layer_activation = prelu_act(hid_layer_activation)\n",
    "        elif activation == \"srelu\":\n",
    "            hid_layer_activation = srelu_activation(hid_layer_activation, scope_name)\n",
    "        elif activation == \"plu\":\n",
    "            hid_layer_activation = plu_activation(hid_layer_activation)\n",
    "        elif activation == \"elu\":\n",
    "            hid_layer_activation = tf.nn.elu(hid_layer_activation)\n",
    "        elif activation == \"none\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Activation function not recognized! \" + \"Check the spelling.\",\n",
    "            )\n",
    "        return hid_layer_activation\n",
    "\n",
    "    def _building_the_network(self, layer_input, dropout_rates):\n",
    "        \"\"\"Build the whole fully connected NN.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            layer_input: Tensor\n",
    "                Input layer.\n",
    "            dropout_rates: list of floats\n",
    "                Dropout rate for each layer. Each entry has to\n",
    "                be between 0 and 1. Has to be of length\n",
    "                len(hidden_layer_sizes) + 1.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            output_fc_layer: Tensor\n",
    "                Output layer.\n",
    "        \"\"\"\n",
    "        hidden_layer_sizes_expand = [\n",
    "            *self.hidden_layer_sizes,\n",
    "            self.size_of_the_output,\n",
    "            self.nconsumer_characteristics,\n",
    "        ]\n",
    "\n",
    "        for i in range(len(self.hidden_layer_sizes) + 1):\n",
    "            output_fc_layer = self._fully_connected_layer_builder(\n",
    "                input_data=layer_input,\n",
    "                hidden_layer_size=hidden_layer_sizes_expand[i],\n",
    "                total_num_features=hidden_layer_sizes_expand[i - 1],\n",
    "                scope_name=\"l\" + str(i + 1),\n",
    "                activation=self.activation_functions[i],\n",
    "                dropout_rate=dropout_rates[i],\n",
    "            )\n",
    "            layer_input = output_fc_layer\n",
    "        return output_fc_layer\n",
    "\n",
    "    def _building_the_network_estimates_TE(\n",
    "        self,\n",
    "        input_data,\n",
    "        t_var,\n",
    "        y_var,\n",
    "        dropout_rates,\n",
    "    ):\n",
    "        \"\"\"Build the neural network that estimates treatment\n",
    "        coefficients.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            layer_input: Tensor\n",
    "                Input layer.\n",
    "            t_var: Tensor\n",
    "                Treatment\n",
    "            y_var: Tensor\n",
    "                Target variable\n",
    "            dropout_rates: list of floats\n",
    "                Dropout rate for each layer. Each entry has to\n",
    "                be between 0 and 1. Has to be of length\n",
    "                len(hidden_layer_sizes) + 1.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            output: Tensor\n",
    "                Treatment coefficients.\n",
    "            loss: scalar\n",
    "                Loss without regularization.\n",
    "        \"\"\"\n",
    "        output = self._building_the_network(input_data, dropout_rates)\n",
    "        tau = output[:, 0:1]\n",
    "        mu0 = output[:, 1:2]\n",
    "        Y_predicted = tf.multiply(t_var, tau) + mu0\n",
    "\n",
    "        # Mean squared error loss:\n",
    "        loss = tf.losses.mean_squared_error(labels=y_var, predictions=Y_predicted)\n",
    "        return output, loss\n",
    "\n",
    "    def _building_the_network_estimates_PS(self, input_data, t_var, dropout_rates):\n",
    "        \"\"\"Build the neural network that estimates propensity\n",
    "        scores.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            layer_input: Tensor\n",
    "                Input layer.\n",
    "            t_var: Tensor\n",
    "                Treatment\n",
    "            dropout_rates: list of floats\n",
    "                Dropout rate for each layer. Each entry has to\n",
    "                be between 0 and 1. Has to be of length\n",
    "                len(hidden_layer_sizes) + 1.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            output: Tensor\n",
    "                Output of the NN.\n",
    "            loss: scalar\n",
    "                Loss without regularization.\n",
    "        \"\"\"\n",
    "        output = self._building_the_network(input_data, dropout_rates)\n",
    "\n",
    "        # Calculating cross entropy loss\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.reshape(\n",
    "                    t_var,\n",
    "                    [\n",
    "                        -1,\n",
    "                    ],\n",
    "                ),\n",
    "                logits=tf.reshape(\n",
    "                    output,\n",
    "                    [\n",
    "                        -1,\n",
    "                    ],\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        return output, loss\n",
    "\n",
    "    def _calc_the_loss_with_reg(self, loss_before_regularization):\n",
    "        \"\"\"Calculate loss with regularization.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            loss_before_regularization: scalar\n",
    "                Loss without regularization.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            total_loss: float\n",
    "                Loss with regularization.\n",
    "        \"\"\"\n",
    "        l1_l2_regularizer = tf.contrib.layers.l1_l2_regularizer(\n",
    "            scale_l1=self.alpha * self.r_par,\n",
    "            scale_l2=self.alpha * (1 - self.r_par),\n",
    "        )\n",
    "        regularization_term = tf.contrib.layers.apply_regularization(\n",
    "            l1_l2_regularizer,\n",
    "            tf.trainable_variables(scope=r\"l\\d+/weights*\"),\n",
    "        )\n",
    "\n",
    "        return loss_before_regularization + regularization_term\n",
    "\n",
    "    def _optimize_the_loss_function(self, loss_with_regularization):\n",
    "        \"\"\"Update the weights after one training step.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            loss_with_regularization: scalar\n",
    "                Loss with regularization.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            train_step: Operation that updates the weights\n",
    "        \"\"\"\n",
    "        if self.optimizer == \"RMSProp\":\n",
    "            train_step = tf.train.RMSPropOptimizer(self.learning_rate).minimize(\n",
    "                loss_with_regularization,\n",
    "            )\n",
    "        if self.optimizer == \"GradientDescent\":\n",
    "            train_step = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(\n",
    "                loss_with_regularization,\n",
    "            )\n",
    "        if self.optimizer == \"Adam\":\n",
    "            train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(\n",
    "                loss_with_regularization,\n",
    "            )\n",
    "        return train_step\n",
    "\n",
    "    def _create_minibatches(self, X, T, Y, rest, shuffle=False):\n",
    "        \"\"\"Create mini-batches generator. Yields a mini-batch of batch_size\n",
    "        length of consumer characteristics, X, treatments, T, target\n",
    "        values, Y, and the indices of the remaining dataset.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            X: ndarray, shape=(len(X_train), nconsumer_characteristics)\n",
    "                Array of consumer characteristics.\n",
    "            T: ndarray, shape=(len(X_train), 1)\n",
    "                Treatment array.\n",
    "            Y: ndarray, shape=(len(X_train), 1)\n",
    "                Target value array.\n",
    "            rest: list of ints\n",
    "                Indices of the remaining array from the previous run.\n",
    "            shuffle: bool\n",
    "                If True, shuffle the array.\n",
    "        Outputs:\n",
    "        -------\n",
    "            X[excerpt]: ndarray, shape=(batch_size,\n",
    "                                        nconsumer_characteristics)\n",
    "                Mini batch of consumer characteristics.\n",
    "            T[excerpt]: ndarray, shape=(batch_size, 1)\n",
    "                Mini batch of treatment values.\n",
    "            Y[excerpt]: ndarray, shape=(batch_size, 1)\n",
    "                Mini batch of target values.\n",
    "            rest: list of ints\n",
    "                Indices of the remaining array after current run.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            indices1 = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices1)\n",
    "            indices = np.array(rest + list(indices1))\n",
    "\n",
    "        for start_idx in range(0, len(indices) - self.batch_size + 1, self.batch_size):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx : start_idx + self.batch_size]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + self.batch_size)\n",
    "\n",
    "            rest = list(indices[start_idx + self.batch_size :])\n",
    "            yield X[excerpt], T[excerpt], Y[excerpt], rest\n",
    "\n",
    "    def _training_the_NN(self, estimating_TE):\n",
    "        \"\"\"Train a NN for max_nepochs or until early stopping criterion\n",
    "        is met.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            estimating_TE: bool\n",
    "                Is neural network used for estimating treatment\n",
    "                coefficients.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            best_loss: float\n",
    "                Minimum value of loss achieved on the validation set if\n",
    "                train_proportion less than 1. Otherwise, loss achieved\n",
    "                on the whole dataset during the last epoch.\n",
    "            epoch_best: int\n",
    "                Epoch at which minimum loss on validation set was\n",
    "                achieved if train_proportion less than 1. Otherwise,\n",
    "                equal to max_nepochs for which the NN is trained.\n",
    "            output_best: ndarray\n",
    "                Output of the NN at the epoch_best.\n",
    "             total_nparameters: int\n",
    "                 Number of neural network parameters\n",
    "        \"\"\"\n",
    "        # Placeholders\n",
    "        x = tf.placeholder(tf.float32, shape=[None, FLAGS.nconsumer_characteristics])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        dropout_rates = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "        if estimating_TE:\n",
    "            output, loss = self._building_the_network_estimates_TE(\n",
    "                x,\n",
    "                t,\n",
    "                y,\n",
    "                dropout_rates,\n",
    "            )\n",
    "        else:\n",
    "            output, loss = self._building_the_network_estimates_PS(x, t, dropout_rates)\n",
    "\n",
    "        total_loss = self._calc_the_loss_with_reg(loss)\n",
    "\n",
    "        train_step = self._optimize_the_loss_function(total_loss)\n",
    "\n",
    "        sess = tf.InteractiveSession()\n",
    "        # Initializing all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        epoch_without_change = 0\n",
    "        break_cond = False\n",
    "\n",
    "        loss_train_list = []\n",
    "        rest = []\n",
    "        if early_stopping:\n",
    "            loss_validation_list = []\n",
    "            validation_loss_min = 10e6\n",
    "            feed_dict_valid = {\n",
    "                x: X_valid,\n",
    "                t: T_valid,\n",
    "                y: Y_valid,\n",
    "                dropout_rates: dropout_rates_test,\n",
    "            }\n",
    "        else:\n",
    "            loss_whole_list = []\n",
    "\n",
    "        feed_dict_total = {x: X, t: T_real, y: Y, dropout_rates: dropout_rates_test}\n",
    "\n",
    "        for i in range(self.max_nepochs):\n",
    "            if early_stopping:\n",
    "                loss_valid = total_loss.eval(feed_dict=feed_dict_valid)\n",
    "                loss_validation_list.append(loss_valid)\n",
    "            else:\n",
    "                loss_whole = total_loss.eval(feed_dict=feed_dict_total)\n",
    "                loss_whole_list.append(loss_whole)\n",
    "\n",
    "            if early_stopping:\n",
    "                if validation_loss_min > loss_valid:\n",
    "                    validation_loss_min = loss_valid\n",
    "                    output_best = output.eval(feed_dict=feed_dict_total)\n",
    "                    epoch_best = i\n",
    "                    epoch_without_change = 0\n",
    "                else:\n",
    "                    epoch_without_change += 1\n",
    "\n",
    "            s = 0\n",
    "            for mini_batch in self._create_minibatches(\n",
    "                X_train,\n",
    "                T_train,\n",
    "                Y_train,\n",
    "                rest,\n",
    "                shuffle=True,\n",
    "            ):\n",
    "                x_batch, t_batch, y_batch, rest = mini_batch\n",
    "                feed_dict_train = {\n",
    "                    x: x_batch,\n",
    "                    t: t_batch,\n",
    "                    y: y_batch,\n",
    "                    dropout_rates: self.dropout_rates_train,\n",
    "                }\n",
    "                loss_train = sess.run(total_loss, feed_dict=feed_dict_train)\n",
    "                if s == 0:\n",
    "                    loss_train_list.append(loss_train)\n",
    "\n",
    "                if epoch_without_change > max_epochs_without_change:\n",
    "                    break_cond = True\n",
    "                    break\n",
    "                sess.run(train_step, feed_dict=feed_dict_train)\n",
    "                s += 1\n",
    "\n",
    "            if FLAGS.verbose and i % 25 == 0:\n",
    "                if early_stopping:\n",
    "                    print(\"%d epoch:\" % i, \"loss on validation set:\", loss_valid)\n",
    "                else:\n",
    "                    print(\"%d epoch:\" % i, \"loss on whole set:\", loss_whole)\n",
    "\n",
    "            # Check the stopping condition\n",
    "            if break_cond:\n",
    "                if FLAGS.verbose:\n",
    "                    print(\"Training is finished! \", end=\"\")\n",
    "                    print(\"Best validation loss achieved at %d epoch\" % epoch_best)\n",
    "                break\n",
    "\n",
    "        if not early_stopping:\n",
    "            output_best = output.eval(feed_dict=feed_dict_total)\n",
    "            epoch_best = i + 1\n",
    "            best_loss = loss_whole\n",
    "            loss_list = loss_whole_list\n",
    "        else:\n",
    "            best_loss = validation_loss_min\n",
    "            loss_list = loss_validation_list\n",
    "\n",
    "        # Num of N parameters\n",
    "        total_nparameters = np.sum(\n",
    "            [\n",
    "                np.prod([xi.value for xi in x.get_shape()])\n",
    "                for x in tf.trainable_variables()\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Plotting loss functions\n",
    "        if FLAGS.plot_true:\n",
    "            add_title = \" - first NN\" if estimating_TE else \" - second NN\"\n",
    "\n",
    "            # If train_proportion less than 1, than loss_list represents\n",
    "            # list of losses on validation set after each epoch. If\n",
    "            # train_proportion = 1, then it is list of losses on whole\n",
    "            # dataset after each epoch.\n",
    "            plotting_loss_functions(loss_train_list, loss_list, add_title)\n",
    "\n",
    "        # Close tf.InteractiveSession\n",
    "        sess.close()\n",
    "\n",
    "        return best_loss, epoch_best, output_best, total_nparameters\n",
    "\n",
    "    def training_the_NN_estimates_TE(self):\n",
    "        \"\"\"Train a NN that estimates treatment coefficients for\n",
    "        max_nepochs or until early stopping criterion is met.\n",
    "\n",
    "        Outputs are the same as in ._training_the_NN function when\n",
    "        estimating_TE argument is set to True.\n",
    "        \"\"\"\n",
    "        return self._training_the_NN(estimating_TE=True)\n",
    "\n",
    "    def training_the_NN_estimates_PS(self):\n",
    "        \"\"\"Train a NN that estimates propensity socres for\n",
    "        max_nepochs or until early stopping criterion is met.\n",
    "\n",
    "        Outputs are the same as in _training_the_NN function when\n",
    "        estimating_TE argument is set to False.\n",
    "        \"\"\"\n",
    "        return self._training_the_NN(estimating_TE=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def influence_functions(mu0_pred, tau_pred, Y, T, prob_t_pred):\n",
    "    \"\"\"Calculate the target value for each individual when treatment is\n",
    "    0 or 1.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        mu0_pred: ndarray, shape=(N, 1)\n",
    "        tau_pred: ndarray, shape=(N, 1)\n",
    "            Estimated conditional average treatment effect.\n",
    "        Y: ndarray, shape=(N,)\n",
    "            Target value array.\n",
    "        T: ndarray, shape=(N,)\n",
    "            Treatment array.\n",
    "        prob_t_pred: ndarray, shape=(N,)\n",
    "            Estimated propensity scores.\n",
    "    Outputs:\n",
    "    -------\n",
    "        psi_0: ndarray, shape=(N, 1)\n",
    "            Influence function for given x in case of no treatment.\n",
    "        psi_1: ndarray, shape=(N, 1)\n",
    "            Influence function for given x in case of treatment.\n",
    "    \"\"\"\n",
    "    first_part = (1 - T) * (Y - mu0_pred)\n",
    "    second_part = T * (Y - mu0_pred - tau_pred)\n",
    "\n",
    "    if FLAGS.treatment == \"not_random\":\n",
    "        prob_t_pred[prob_t_pred < 0.0001] = 0.0001\n",
    "        prob_t_pred[prob_t_pred > 0.9999] = 0.9999\n",
    "        psi_0 = (first_part / (1 - prob_t_pred)) + mu0_pred\n",
    "        psi_1 = (second_part / prob_t_pred) + mu0_pred + tau_pred\n",
    "    else:\n",
    "        psi_0 = (first_part / (1 - np.mean(T))) + mu0_pred\n",
    "        psi_1 = (second_part / np.mean(T)) + mu0_pred + tau_pred\n",
    "    return psi_0, psi_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_comparison_file(name, model_info, cols):\n",
    "    \"\"\"Update .csv file with new model results.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        name: string\n",
    "            File name. If the file does not already exist creates a\n",
    "            new file. Otherwise appends new model results to the\n",
    "            existing file.\n",
    "        model_info: list\n",
    "            Results of the current run.\n",
    "        cols: list\n",
    "            Names of columns within the .csv file.\n",
    "            Has to be of the same length as model_info.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(name):\n",
    "        df = pd.DataFrame(columns=cols)\n",
    "        df.to_csv(name, index=False)\n",
    "        print(\"File does not exist. Creating new file!\")\n",
    "    else:\n",
    "        print(\"File already exists. Appending model run!\")\n",
    "\n",
    "    Model_comparison_Catalog_dataset = pd.read_csv(name)\n",
    "    ind = len(Model_comparison_Catalog_dataset[\"Model number\"])\n",
    "    model_info[0][0] = ind\n",
    "    df = pd.DataFrame(model_info, columns=cols)\n",
    "    Model_comparison_Catalog_dataset = Model_comparison_Catalog_dataset.append(\n",
    "        df,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    Model_comparison_Catalog_dataset.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Running Monte Carlo simulations for the following case:\")\n",
    "    print(\"* %s treatment\" % FLAGS.treatment)\n",
    "    print(\"* %s model\" % FLAGS.model)\n",
    "    print(\"* %s consumer characteristics\" % FLAGS.nconsumer_characteristics, \"\\n\")\n",
    "    print(\"Using the following NN architectures:\")\n",
    "    print(\"First NN hidden layer sizes: \", hidden_layer_sizes)\n",
    "    print(\"First NN hidden activations: \", activation_functions)\n",
    "    print(\"First NN dropout rates: \", dropout_rates_train, \"\\n\")\n",
    "\n",
    "    print(\"Second NN hidden layer sizes: \", hidden_layer_sizes_treatment)\n",
    "    print(\"Second NN hidden activations: \", activation_functions_treatment)\n",
    "    print(\"Second NN dropout rates: \", dropout_rates_train_treatment)\n",
    "    print(\"-------------------------------------------------------\\n\")\n",
    "\n",
    "    count_in_interval = 0\n",
    "    for _ in range(FLAGS.nsimulations):\n",
    "        global X_train, T_train, Y_train, X_valid, T_valid, Y_valid, X, T_real, Y\n",
    "        # ---------------- Creating the fake dataset -----------------\n",
    "\n",
    "        (\n",
    "            Y,\n",
    "            X,\n",
    "            mu0_real,\n",
    "            tau_real,\n",
    "            T_real,\n",
    "            seed,\n",
    "            prob_of_T,\n",
    "            tau_true_mean,\n",
    "        ) = FakeData().create_fake_data()\n",
    "\n",
    "        # Setting the seed to prevent randomness:\n",
    "        tf.set_random_seed(77)\n",
    "        np.random.seed(61)\n",
    "\n",
    "        # Splitting the dataset into training and validation set\n",
    "        if early_stopping:\n",
    "            train_inds, valid_inds = get_train_test_inds(T_real)\n",
    "            T_train = T_real[train_inds]\n",
    "            Y_train = Y[train_inds]\n",
    "            X_train = X[train_inds]\n",
    "            T_valid = T_real[valid_inds]\n",
    "            Y_valid = Y[valid_inds]\n",
    "            X_valid = X[valid_inds]\n",
    "        else:\n",
    "            T_train = T_real\n",
    "            Y_train = Y\n",
    "            X_train = X\n",
    "\n",
    "        # Determining batch size\n",
    "        batch_size_ = calculate_batch_size(batch_size, X_train)\n",
    "        batch_size_t_ = calculate_batch_size(batch_size_t, X_train)\n",
    "\n",
    "        # ------------- Building and training the first NN -----------\n",
    "\n",
    "        if FLAGS.verbose:\n",
    "            print(\"\\nTraining of treatment coefficients neural network:\")\n",
    "        first_NN = NeuralNetwork(\n",
    "            hidden_layer_sizes,\n",
    "            activation_functions,\n",
    "            dropout_rates_train,\n",
    "            batch_size_,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        (\n",
    "            MSE_best,\n",
    "            epoch_best,\n",
    "            betas_pred_best,\n",
    "            total_nparameters,\n",
    "        ) = first_NN.training_the_NN_estimates_TE()\n",
    "\n",
    "        # ------------- Building and training the second NN ----------\n",
    "\n",
    "        if FLAGS.treatment == \"not_random\":\n",
    "            # Resetting the graph\n",
    "            tf.reset_default_graph()\n",
    "\n",
    "            # Setting the seed to prevent randomness in our model:\n",
    "            tf.set_random_seed(77)\n",
    "            np.random.seed(61)\n",
    "\n",
    "            if FLAGS.verbose:\n",
    "                print(\"\\nTraining of propensity score neural network:\")\n",
    "            second_NN = NeuralNetwork(\n",
    "                hidden_layer_sizes_treatment,\n",
    "                activation_functions_treatment,\n",
    "                dropout_rates_train_treatment,\n",
    "                batch_size_t_,\n",
    "                1,\n",
    "            )\n",
    "\n",
    "            (\n",
    "                CE_best,\n",
    "                epoch_best_t,\n",
    "                treat_best,\n",
    "                total_nparameters_t,\n",
    "            ) = second_NN.training_the_NN_estimates_PS()\n",
    "\n",
    "        # -------------------- Looking at the results ----------------\n",
    "\n",
    "        betas_pred = betas_pred_best\n",
    "        tau_pred = betas_pred[:, 0:1]\n",
    "        mu0_pred = betas_pred[:, 1:]\n",
    "\n",
    "        if FLAGS.treatment == \"not_random\":\n",
    "            prob_of_t_pred = 1 / (1 + np.exp(-treat_best))\n",
    "\n",
    "        # Coefficients statistic\n",
    "        mu0_mean_pred = np.mean(mu0_pred)\n",
    "        std_mu0_pred = np.std(mu0_pred)\n",
    "        tau_mean_pred = np.mean(tau_pred)\n",
    "        std_tau_pred = np.std(tau_pred)\n",
    "\n",
    "        mu0_mean_real = np.mean(mu0_real)\n",
    "        std_mu0_real = np.std(mu0_real)\n",
    "        tau_mean_real = np.mean(tau_real)\n",
    "        std_tau_real = np.std(tau_real)\n",
    "\n",
    "        if FLAGS.verbose:\n",
    "            print(\"\\n------------------ mu0 results ------------------\")\n",
    "            print(\n",
    "                [\n",
    "                    \"Mean mu0_pred = %0.3f\" % mu0_mean_pred,\n",
    "                    \"Std mu0_pred = %0.3f\" % std_mu0_pred,\n",
    "                ],\n",
    "            )\n",
    "            print(\n",
    "                [\n",
    "                    \"Mean mu0_real = %0.3f\" % mu0_mean_real,\n",
    "                    \"Std mu0_real = %0.3f\" % std_mu0_real,\n",
    "                ],\n",
    "                \"\\n\",\n",
    "            )\n",
    "\n",
    "            print(\"------------------ tau results ------------------\")\n",
    "            print(\n",
    "                [\n",
    "                    \"Mean tau_pred = %0.3f\" % tau_mean_pred,\n",
    "                    \"Std tau_pred = %0.3f\" % std_tau_pred,\n",
    "                ],\n",
    "            )\n",
    "            print(\n",
    "                [\n",
    "                    \"Mean tau_real = %0.3f\" % tau_mean_real,\n",
    "                    \"Std tau_real = %0.3f\" % std_tau_real,\n",
    "                ],\n",
    "                \"\\n\",\n",
    "            )\n",
    "\n",
    "            if FLAGS.treatment == \"not_random\":\n",
    "                print(\"------------------ t results ------------------\")\n",
    "                print(\n",
    "                    \"Mean prob_of_t_pred = %0.3f\" % np.mean(prob_of_t_pred),\n",
    "                    \"\\nMean prob_of_t_real = %0.3f\\n\" % np.mean(prob_of_T),\n",
    "                )\n",
    "\n",
    "        total_nparameters_t = np.sum(\n",
    "            [\n",
    "                np.prod([xi.value for xi in x.get_shape()])\n",
    "                for x in tf.trainable_variables()\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        if FLAGS.treatment == \"not_random\":\n",
    "            psi_0, psi_1 = influence_functions(\n",
    "                mu0_pred,\n",
    "                tau_pred,\n",
    "                Y,\n",
    "                T_real,\n",
    "                prob_of_t_pred,\n",
    "            )\n",
    "        else:\n",
    "            psi_0, psi_1 = influence_functions(\n",
    "                mu0_pred,\n",
    "                tau_pred,\n",
    "                Y,\n",
    "                T_real,\n",
    "                prob_t_pred=None,\n",
    "            )\n",
    "\n",
    "        # Calculating confidence interval for average treatment effect\n",
    "        mean_diff_psi1_psi0 = np.mean(psi_1 - psi_0)\n",
    "        std_diff_psi1_psi0 = np.std(psi_1 - psi_0)\n",
    "        CI_upper_bound = mean_diff_psi1_psi0 + 1.96 * std_diff_psi1_psi0 / np.sqrt(\n",
    "            nconsumers,\n",
    "        )\n",
    "        CI_lower_bound = mean_diff_psi1_psi0 - 1.96 * std_diff_psi1_psi0 / np.sqrt(\n",
    "            nconsumers,\n",
    "        )\n",
    "\n",
    "        in_95_conf_int = CI_lower_bound < tau_true_mean < CI_upper_bound\n",
    "\n",
    "        print(\"is tau_true_mean in interval:\", in_95_conf_int)\n",
    "        print(\n",
    "            \"CI lower and upper bound are: ({:0.3f}, {:0.3f})\".format(\n",
    "                CI_lower_bound,\n",
    "                CI_upper_bound,\n",
    "            ),\n",
    "        )\n",
    "        if in_95_conf_int:\n",
    "            count_in_interval += 1\n",
    "\n",
    "        Y_pred = mu0_pred + tau_pred * T_real\n",
    "\n",
    "        # ----------------- Saving the results! ----------------------\n",
    "        name = \"Results_{}_model_{}{}_{}_{}.csv\".format(\n",
    "            FLAGS.model,\n",
    "            FLAGS.architecture,\n",
    "            FLAGS.treatment,\n",
    "            nconsumers,\n",
    "            FLAGS.nconsumer_characteristics,\n",
    "        )\n",
    "\n",
    "        if FLAGS.treatment == \"random\":\n",
    "            parameters_dict = {\n",
    "                \"nconsumer_characteristics\": FLAGS.nconsumer_characteristics,\n",
    "                \"treatment\": FLAGS.treatment,\n",
    "                \"model\": FLAGS.model,\n",
    "                \"architecture\": FLAGS.architecture,\n",
    "                \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "                \"dropout_rates_train\": dropout_rates_train,\n",
    "                \"activation_functions\": activation_functions,\n",
    "                \"nconsumers\": nconsumers,\n",
    "                \"train_proportion\": train_proportion,\n",
    "                \"max_nepochs\": max_nepochs,\n",
    "                \"max_epochs_without_change\": max_epochs_without_change,\n",
    "                \"early_stopping\": early_stopping,\n",
    "                \"optimizer\": optimizer,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"alpha\": alpha,\n",
    "                \"r\": r,\n",
    "            }\n",
    "\n",
    "            cols = [\n",
    "                \"Model number\",\n",
    "                \"seed\",\n",
    "                \"best_epoch\",\n",
    "                \"total_nparameters\",\n",
    "                \"Loss best\",\n",
    "                \"Mean mu0_pred\",\n",
    "                \"Std mu0_pred\",\n",
    "                \"Mean mu0_real\",\n",
    "                \"Std mu0_real\",\n",
    "                \"Mean tau_pred\",\n",
    "                \"Std tau_pred\",\n",
    "                \"Mean tau_real\",\n",
    "                \"Std tau_real\",\n",
    "                \"tau_true_mean\",\n",
    "                \"Y_real_mean\",\n",
    "                \"Y_pred_mean\",\n",
    "                \"CI_lower_bound\",\n",
    "                \"CI_upper_bound\",\n",
    "                \"psi_0_mean\",\n",
    "                \"psi_1_mean\",\n",
    "                \"mean_diff_psi_1_psi_0\",\n",
    "                \"std_diff_psi_1_psi_0\",\n",
    "                \"in_interval\",\n",
    "                \"model_parameters_dict\",\n",
    "            ]\n",
    "\n",
    "            model_info = [\n",
    "                [\n",
    "                    0,\n",
    "                    seed,\n",
    "                    epoch_best,\n",
    "                    total_nparameters,\n",
    "                    MSE_best,\n",
    "                    mu0_mean_pred,\n",
    "                    std_mu0_pred,\n",
    "                    np.mean(mu0_real),\n",
    "                    std_mu0_real,\n",
    "                    tau_mean_pred,\n",
    "                    std_tau_pred,\n",
    "                    tau_mean_real,\n",
    "                    std_tau_real,\n",
    "                    tau_true_mean,\n",
    "                    np.mean(Y),\n",
    "                    np.mean(Y_pred),\n",
    "                    CI_lower_bound,\n",
    "                    CI_upper_bound,\n",
    "                    np.mean(psi_0),\n",
    "                    np.mean(psi_1),\n",
    "                    mean_diff_psi1_psi0,\n",
    "                    std_diff_psi1_psi0,\n",
    "                    in_95_conf_int,\n",
    "                    parameters_dict,\n",
    "                ],\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            parameters_dict = {\n",
    "                \"nconsumer_characteristics\": FLAGS.nconsumer_characteristics,\n",
    "                \"treatment\": FLAGS.treatment,\n",
    "                \"model\": FLAGS.model,\n",
    "                \"architecture\": FLAGS.architecture,\n",
    "                \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "                \"dropout_rates_train\": dropout_rates_train,\n",
    "                \"activation_functions\": activation_functions,\n",
    "                \"hidden_layer_sizes_treatment\": hidden_layer_sizes_treatment,\n",
    "                \"activation_functions_treatment\": activation_functions_treatment,\n",
    "                \"dropout_rates_train_treatment\": dropout_rates_train_treatment,\n",
    "                \"nconsumers\": nconsumers,\n",
    "                \"train_proportion\": train_proportion,\n",
    "                \"max_nepochs\": max_nepochs,\n",
    "                \"max_epochs_without_change\": max_epochs_without_change,\n",
    "                \"early_stopping\": early_stopping,\n",
    "                \"optimizer\": optimizer,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"batch_size_t\": batch_size_t,\n",
    "                \"alpha\": alpha,\n",
    "                \"r\": r,\n",
    "            }\n",
    "\n",
    "            cols = [\n",
    "                \"Model number\",\n",
    "                \"seed\",\n",
    "                \"best_epoch\",\n",
    "                \"best_epoch_t\",\n",
    "                \"total_nparameters\",\n",
    "                \"total_nparameters_t\",\n",
    "                \"Loss best\",\n",
    "                \"Loss best_treatment\",\n",
    "                \"Mean mu0_pred\",\n",
    "                \"Std mu0_pred\",\n",
    "                \"Mean mu0_real\",\n",
    "                \"Std mu0_real\",\n",
    "                \"Mean tau_pred\",\n",
    "                \"Std tau_pred\",\n",
    "                \"Mean tau_real\",\n",
    "                \"Std tau_real\",\n",
    "                \"tau_true_mean\",\n",
    "                \"Mean_prob_t_pred\",\n",
    "                \"Mean prob_of_t_real\",\n",
    "                \"mean_T_real\",\n",
    "                \"Y_real_mean\",\n",
    "                \"Y_pred_mean\",\n",
    "                \"CI_lower_bound\",\n",
    "                \"CI_upper_bound\",\n",
    "                \"psi_0_mean\",\n",
    "                \"psi_1_mean\",\n",
    "                \"mean_diff_psi_1_psi_0\",\n",
    "                \"std_diff_psi_1_psi_0\",\n",
    "                \"in_interval\",\n",
    "                \"model_parameters_dict\",\n",
    "            ]\n",
    "\n",
    "            model_info = [\n",
    "                [\n",
    "                    0,\n",
    "                    seed,\n",
    "                    epoch_best,\n",
    "                    epoch_best_t,\n",
    "                    total_nparameters,\n",
    "                    total_nparameters_t,\n",
    "                    MSE_best,\n",
    "                    CE_best,\n",
    "                    mu0_mean_pred,\n",
    "                    std_mu0_pred,\n",
    "                    mu0_mean_real,\n",
    "                    std_mu0_real,\n",
    "                    tau_mean_pred,\n",
    "                    std_tau_pred,\n",
    "                    tau_mean_real,\n",
    "                    std_tau_real,\n",
    "                    tau_true_mean,\n",
    "                    np.mean(prob_of_t_pred),\n",
    "                    np.mean(prob_of_T),\n",
    "                    np.mean(T_real),\n",
    "                    np.mean(Y),\n",
    "                    np.mean(Y_pred),\n",
    "                    CI_lower_bound,\n",
    "                    CI_upper_bound,\n",
    "                    np.mean(psi_0),\n",
    "                    np.mean(psi_1),\n",
    "                    mean_diff_psi1_psi0,\n",
    "                    std_diff_psi1_psi0,\n",
    "                    in_95_conf_int,\n",
    "                    parameters_dict,\n",
    "                ],\n",
    "            ]\n",
    "\n",
    "        if FLAGS.update:\n",
    "            update_model_comparison_file(name, model_info, cols)\n",
    "        tf.reset_default_graph()  # restarting a NN graph\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\n",
    "        \"%d out of %d simulations contain tau_true_mean in the CI\"\n",
    "        % (count_in_interval, FLAGS.nsimulations),\n",
    "    )\n",
    "\n",
    "    # Print running time\n",
    "    running_time()\n",
    "\n",
    "    if os.path.exists(name):\n",
    "        model_data = pd.read_csv(name)\n",
    "        print(model_data)\n",
    "        print(\n",
    "            \"%d out of %d simulations contain tau_true_mean in the CI\"\n",
    "            % (np.sum(model_data[\"in_interval\"]), len(model_data)),\n",
    "        )\n",
    "        print(\"Name of the file:\", name)\n",
    "    else:\n",
    "        print(\"File `\" + name + \"` is not yet created.\")\n",
    "\n",
    "    # Plot all the graphs\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
