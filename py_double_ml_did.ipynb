{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python: Difference-in-Differences\n",
    "\n",
    "In this example, we illustrate how the [DoubleML](https://docs.doubleml.org/stable/index.html) package can be used to estimate the average treatment effect on the treated (ATT) under the conditional parallel trend assumption. The estimation is based on [Chang (2020)](https://doi.org/10.1093/ectj/utaa001), [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) and [Zimmert et al. (2018)](https://arxiv.org/abs/1809.01643).\n",
    "\n",
    "In this example, we will adopt the notation of [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003).\n",
    "\n",
    "In the whole example our treatment and time variable $t\\in\\{0,1\\}$ will be binary. \n",
    "Let $D_i\\in\\{0,1\\}$ denote the treatment status of unit $i$ at time $t=1$ (at time $t=0$ all units are not treated) and let $Y_{it}$ be the outcome of interest of unit $i$ at time $t$.\n",
    "Using the potential outcome notation, we can write $Y_{it}(d)$ for the potential outcome of unit $i$ at time $t$ and treatment status $d$. Further, let $X_i$ denote a vector of pre-treatment covariates.\n",
    "In these difference-in-differences settings [Abadie (2005)](https://doi.org/10.1111/0034-6527.00321) showed that the ATTE\n",
    "\n",
    "$$\\theta = \\mathbb{E}[Y_{i1}(1)- Y_{i1}(0)|D_i=1]$$\n",
    "\n",
    "is identified when panel data are available or under stationarity assumptions for repeated cross-sections. Further, the basic assumptions are \n",
    "\n",
    " - **Parallel Trends:** We have $\\mathbb{E}[Y_{i1}(0) - Y_{i0}(0)|X_i, D_i=1] = \\mathbb{E}[Y_{i1}(0) - Y_{i0}(0)|X_i, D_i=0]\\quad a.s.$\n",
    "\n",
    "- **Overlap:** For some $\\epsilon > 0$, $P(D_i=1) > \\epsilon$ and $P(D_i=1|X_i) \\le 1-\\epsilon$ a.s.\n",
    "\n",
    "For a detailed explanation of the assumptions see e.g. [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) or [Zimmert et al. (2018)](https://arxiv.org/abs/1809.01643).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panel Data (Repeated Outcomes)\n",
    "\n",
    "At first, we will consider two-period panel data, where we observe i.i.d. data $W_i = (Y_{i0}, Y_{i1}, D_i, X_i)$.\n",
    "\n",
    "### Data\n",
    "\n",
    "We will use the implemented data generating process `make_did_SZ2020` to generate data according to the simulation in [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) (Section 4.1). \n",
    "\n",
    "In this example, we will use `dgp_tpye=4`, which corresponds to the misspecified settings in [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) (other data generating processes are also available via the `dgp_type` parameter). In all settings the true ATTE is zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify a corresponding `DoubleMLData` object, we have to specify a single outcome `y`. For panel data, the outcome consists of the difference of \n",
    "\n",
    "$$\\Delta Y_i = Y_{i1}- Y_{i0}.$$\n",
    "\n",
    "This difference will then be defined as outcome in our `DoubleMLData` object. The data generating process `make_did_SZ2020` already specifies the outcome `y` accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Description of dgp: \n",
    "https://docs.doubleml.org/stable/api/generated/doubleml.datasets.make_did_SZ2020.html#doubleml.datasets.make_did_SZ2020\n",
    "\n",
    "Which simulations to do?\n",
    "1. TWFE of DiD (todo)\n",
    "2. run the doubly robust of S'ant Anna with logistic and linear estimation (done)\n",
    "    - how do I get the results from the paper? (done)\n",
    "3. Just run with one method (linear or logistic) (todo)\n",
    "3. run the doubly robust of S'ant Anna with deep learning and linear estimation (done)\n",
    "4. Just run deep learning (todo)\n",
    "    - is it possible to just run one model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "x, y, d = make_did_SZ2020(\n",
    "    n_obs=n_obs,\n",
    "    dgp_type=4,\n",
    "    cross_sectional_data=False,\n",
    "    return_type=\"array\",\n",
    ")  # max 1- 6 dgp types\n",
    "dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "print(dml_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTE Estimation\n",
    "\n",
    "To estimate the ATTE with panel data, we will use the `DoubleMLDID` class. \n",
    "\n",
    "As for all `DoubleML` classes, we have to specify learners, which have to be initialized first.\n",
    "Here, we will just rely on a tree based method. \n",
    "\n",
    "The learner `ml_g` is used to fit conditional expectations of the outcome $\\mathbb{E}[\\Delta Y_i|D_i=0, X_i]$, whereas the learner `ml_m` will be used to estimate the propensity score $P(D_i=1|X_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "n_estimators = 30\n",
    "ml_g = LGBMRegressor(n_estimators=n_estimators)  # putcome regression\n",
    "ml_m = LGBMClassifier(n_estimators=n_estimators)  # propensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# not exactly the same as in the paper, but similar\n",
    "# results are more consiten than sklean library\n",
    "ml_m = lgb.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    metric=\"binary_logloss\",\n",
    "    n_estimators=n_estimators,\n",
    ")\n",
    "ml_g = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    metric=\"mse\",\n",
    "    n_estimators=n_estimators,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "ml_g = LinearRegression()  # as in the paper, estimators not needed\n",
    "ml_m = LogisticRegression()  # as in the paper, estimators not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main model that worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important main model\n",
    "\n",
    "import numpy as np\n",
    "from doubleml import DoubleMLData, DoubleMLDID\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_obs = 1000\n",
    "n_rep = 200\n",
    "ATTE = 0.0  # The true value of the ATTE\n",
    "\n",
    "# Storage for estimates\n",
    "ATTE_estimates = np.full(n_rep, np.nan)\n",
    "coverage = np.full(n_rep, np.nan)\n",
    "ci_length = np.full(n_rep, np.nan)\n",
    "asymptotic_variance = np.full(n_rep, np.nan)\n",
    "\n",
    "# Model definitions\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "linear_model = Pipeline([(\"poly\", poly_features), (\"linear\", LinearRegression())])\n",
    "logistic_model = LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "    if i_rep % int(n_rep / 10) == 0:\n",
    "        print(f\"Iteration: {i_rep}/{n_rep}\")\n",
    "\n",
    "    # Generate data\n",
    "    x, y, d = make_did_SZ2020(\n",
    "        n_obs=n_obs,\n",
    "        dgp_type=4,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"array\",\n",
    "    )\n",
    "    dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "\n",
    "    # Define DoubleML model\n",
    "    dml_did = DoubleMLDID(dml_data, ml_g=linear_model, ml_m=logistic_model, n_folds=5)\n",
    "    dml_did.fit()\n",
    "\n",
    "    # Store results\n",
    "    ATTE_estimates[i_rep] = dml_did.coef.squeeze()\n",
    "    confint = dml_did.confint(level=0.95)\n",
    "    coverage[i_rep] = (confint[\"2.5 %\"].iloc[0] <= ATTE) & (\n",
    "        confint[\"97.5 %\"].iloc[0] >= ATTE\n",
    "    )\n",
    "    ci_length[i_rep] = confint[\"97.5 %\"].iloc[0] - confint[\"2.5 %\"].iloc[0]\n",
    "    # Extract standard error from the summary\n",
    "    summary_df = dml_did.summary\n",
    "    std_err = summary_df.loc[\"d\", \"std err\"]\n",
    "    asymptotic_variance[i_rep] = std_err**2\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - ATTE)\n",
    "med_bias = np.median(ATTE_estimates - ATTE)\n",
    "rmse = np.sqrt(np.mean((ATTE_estimates - ATTE) ** 2))\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(coverage)\n",
    "avg_ci_length = np.mean(ci_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_did.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two way fixed effects approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_obs = 1000\n",
    "n_rep = 200\n",
    "ATTE = 0.0  # Adjust this to reflect the true treatment effect\n",
    "\n",
    "# Storage for estimates\n",
    "ATTE_estimates = np.full(n_rep, np.nan)\n",
    "coverage = np.full(n_rep, np.nan)\n",
    "ci_length = np.full(n_rep, np.nan)\n",
    "asymptotic_variance = np.full(n_rep, np.nan)\n",
    "\n",
    "biases = []\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "    if i_rep % int(n_rep / 10) == 0:\n",
    "        print(f\"Iteration: {i_rep}/{n_rep}\")\n",
    "\n",
    "    # Generate data\n",
    "    x, y, d = make_did_SZ2020(\n",
    "        n_obs=n_obs,\n",
    "        dgp_type=1,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"array\",\n",
    "    )\n",
    "    dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "\n",
    "    # Fit linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "\n",
    "    # Extract the treatment effect estimate (coefficient of the interaction term)\n",
    "    ATTE_estimates[i_rep] = model.coef_[-1]\n",
    "\n",
    "    # Calculate and store the bias\n",
    "    bias = model.coef_[-1] - ATTE\n",
    "    biases.append(bias)\n",
    "\n",
    "    # Store other results as needed\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(biases)\n",
    "med_bias = np.median(biases)\n",
    "rmse = np.sqrt(np.mean(np.square(biases)))\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_obs = 1000\n",
    "n_rep = 200\n",
    "ATTE = 0.0  # Adjust this to reflect the true treatment effect\n",
    "\n",
    "# Storage for estimates\n",
    "ATTE_estimates = np.full(n_rep, np.nan)\n",
    "coverage = np.full(n_rep, np.nan)\n",
    "ci_length = np.full(n_rep, np.nan)\n",
    "asymptotic_variance = np.full(n_rep, np.nan)\n",
    "\n",
    "biases = []\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "    if i_rep % int(n_rep / 10) == 0:\n",
    "        print(f\"Iteration: {i_rep}/{n_rep}\")\n",
    "\n",
    "    # Generate data\n",
    "    x, y, d = make_did_SZ2020(\n",
    "        n_obs=n_obs,\n",
    "        dgp_type=1,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"array\",\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(x, columns=[f\"X{i+1}\" for i in range(x.shape[1])])\n",
    "    df[\"y\"] = y\n",
    "    df[\"d\"] = d\n",
    "    df[\"time\"] = np.random.randint(\n",
    "        2,\n",
    "        size=len(df),\n",
    "    )  # Example time indicator (replace with your time indicator)\n",
    "\n",
    "    # Fit TWFE model using statsmodels\n",
    "    formula = \"y ~ d + time + \" + \" + \".join([f\"X{i+1}\" for i in range(x.shape[1])])\n",
    "    model = smf.ols(formula=formula, data=df).fit()\n",
    "\n",
    "    # Extract the treatment effect estimate (coefficient of the treatment variable)\n",
    "    tau_fe = model.params[\"d\"]\n",
    "    ATTE_estimates[i_rep] = tau_fe\n",
    "\n",
    "    # Calculate and store the bias\n",
    "    bias = tau_fe - ATTE\n",
    "    biases.append(bias)\n",
    "\n",
    "    # Confidence intervals\n",
    "    ci = model.conf_int().loc[\"d\"]\n",
    "    ci_length[i_rep] = ci[1] - ci[0]\n",
    "    coverage[i_rep] = ci[0] <= ATTE <= ci[1]\n",
    "    asymptotic_variance[i_rep] = model.bse[\"d\"] ** 2\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(biases)\n",
    "med_bias = np.median(biases)\n",
    "rmse = np.sqrt(np.mean(np.square(biases)))\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## workin on closest to being correct\n",
    "Problem: DiD works to well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_obs = 1000\n",
    "n_rep = 200\n",
    "ATTE = 0.0  # Adjust this to reflect the true treatment effect\n",
    "\n",
    "# Storage for estimates\n",
    "ATTE_estimates = np.full(n_rep, np.nan)\n",
    "coverage = np.full(n_rep, np.nan)\n",
    "ci_length = np.full(n_rep, np.nan)\n",
    "asymptotic_variance = np.full(n_rep, np.nan)\n",
    "\n",
    "biases = []\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "    if i_rep % int(n_rep / 10) == 0:\n",
    "        print(f\"Iteration: {i_rep}/{n_rep}\")\n",
    "\n",
    "    # Generate data\n",
    "    x, y, d = make_did_SZ2020(\n",
    "        n_obs=n_obs,\n",
    "        dgp_type=1,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"array\",\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(x, columns=[f\"X{i+1}\" for i in range(x.shape[1])])\n",
    "    df[\"y\"] = y\n",
    "    df[\"d\"] = d\n",
    "    df[\"time\"] = np.random.randint(\n",
    "        2,\n",
    "        size=len(df),\n",
    "    )  # Example time indicator (replace with your time indicator)\n",
    "\n",
    "    # Fit TWFE model using statsmodels\n",
    "    formula = \"y ~ d + time + d:time + \" + \" + \".join(\n",
    "        [f\"X{i+1}\" for i in range(x.shape[1])],\n",
    "    )\n",
    "    model = smf.ols(formula=formula, data=df).fit()\n",
    "\n",
    "    # Extract the treatment effect estimate (coefficient of the treatment variable)\n",
    "    tau_fe = model.params[\"d\"]\n",
    "    ATTE_estimates[i_rep] = tau_fe\n",
    "\n",
    "    # Calculate and store the bias\n",
    "    bias = tau_fe - ATTE\n",
    "    biases.append(bias)\n",
    "\n",
    "    # Confidence intervals\n",
    "    ci = model.conf_int().loc[\"d\"]\n",
    "    ci_length[i_rep] = ci[1] - ci[0]\n",
    "    coverage[i_rep] = ci[0] <= ATTE <= ci[1]\n",
    "    asymptotic_variance[i_rep] = model.bse[\"d\"] ** 2\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(biases)\n",
    "med_bias = np.median(biases)\n",
    "rmse = np.sqrt(np.mean(np.square(biases)))\n",
    "mean_coverage = np.mean(coverage)\n",
    "mean_ci_length = np.mean(ci_length)\n",
    "mean_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Coverage:{mean_coverage}\")\n",
    "print(f\"mean_ci_length:{mean_ci_length}\")\n",
    "print(f\"mean_asymptotic_variance:{mean_asymptotic_variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rumprobieren hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_obs = 1000\n",
    "n_rep = 200\n",
    "ATTE = 0.0  # Adjust this to reflect the true treatment effect\n",
    "\n",
    "# Storage for estimates\n",
    "ATTE_estimates = np.full(n_rep, np.nan)\n",
    "coverage = np.full(n_rep, np.nan)\n",
    "ci_length = np.full(n_rep, np.nan)\n",
    "asymptotic_variance = np.full(n_rep, np.nan)\n",
    "\n",
    "biases = []\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "    if i_rep % int(n_rep / 10) == 0:\n",
    "        print(f\"Iteration: {i_rep}/{n_rep}\")\n",
    "\n",
    "    # Generate data\n",
    "    data = make_did_SZ2020(\n",
    "        n_obs=n_obs,\n",
    "        dgp_type=1,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"DataFrame\",\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(x, columns=[f\"X{i+1}\" for i in range(x.shape[1])])\n",
    "    df[\"y\"] = y\n",
    "    df[\"d\"] = d\n",
    "    df[\"time\"] = np.random.randint(\n",
    "        2,\n",
    "        size=len(df),\n",
    "    )  # Example time indicator (replace with your time indicator)\n",
    "\n",
    "    # Fit TWFE model using statsmodels\n",
    "    formula = \"y ~ d\"\n",
    "    model = smf.ols(formula=formula, data=df).fit()\n",
    "\n",
    "    # Extract the treatment effect estimate (coefficient of the treatment variable)\n",
    "    tau_fe = model.params[\"d\"]\n",
    "    ATTE_estimates[i_rep] = tau_fe\n",
    "\n",
    "    # Calculate and store the bias\n",
    "    bias = tau_fe - ATTE\n",
    "    biases.append(bias)\n",
    "\n",
    "    # Confidence intervals\n",
    "    ci = model.conf_int().loc[\"d\"]\n",
    "    ci_length[i_rep] = ci[1] - ci[0]\n",
    "    coverage[i_rep] = ci[0] <= ATTE <= ci[1]\n",
    "    asymptotic_variance[i_rep] = model.bse[\"d\"] ** 2\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(biases)\n",
    "med_bias = np.median(biases)\n",
    "rmse = np.sqrt(np.mean(np.square(biases)))\n",
    "mean_coverage = np.mean(coverage)\n",
    "mean_ci_length = np.mean(ci_length)\n",
    "mean_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Coverage:{mean_coverage}\")\n",
    "print(f\"mean_ci_length:{mean_ci_length}\")\n",
    "print(f\"mean_asymptotic_variance:{mean_asymptotic_variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_did_SZ2020(\n",
    "    n_obs=n_obs,\n",
    "    dgp_type=1,\n",
    "    cross_sectional_data=False,\n",
    "    return_type=\"DataFrame\",\n",
    ")\n",
    "# data\n",
    "formula = \"y ~ d + Z1 + Z2 + Z3 + Z4\"\n",
    "model = smf.ols(formula=formula, data=data).fit()\n",
    "\n",
    "# Extract the treatment effect estimate (coefficient of the treatment variable)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doubleml as dml\n",
    "import numpy as np\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "ml_g = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_leaf=5)\n",
    "ml_m = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_leaf=5)\n",
    "data = make_did_SZ2020(n_obs=500, return_type=\"DataFrame\")\n",
    "obj_dml_data = dml.DoubleMLData(data, \"y\", \"d\")\n",
    "dml_did_obj = dml.DoubleMLDID(obj_dml_data, ml_g, ml_m)\n",
    "dml_did_obj.fit().summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_pa = pd.DataFrame(ATTE_estimates, columns=[\"Estimate\"])\n",
    "g = sns.kdeplot(df_pa, fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_did_SZ2020(\n",
    "    n_obs=n_obs,\n",
    "    dgp_type=4,\n",
    "    cross_sectional_data=False,\n",
    "    return_type=\"DataFrame\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for hetergoeneity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Visual Inspection: Pairplot for visualizing relationships between variables\n",
    "sns.pairplot(data)\n",
    "plt.show()\n",
    "\n",
    "# Descriptive Statistics: Mean comparison for each group (treated vs. control)\n",
    "mean_grouped = data.groupby(\"d\").mean()\n",
    "print(\"Mean comparison between groups:\")\n",
    "print(mean_grouped)\n",
    "\n",
    "# Statistical Tests: ANOVA for testing differences in 'y' across treatment groups\n",
    "group0_y = data[data[\"d\"] == 0][\"y\"]\n",
    "group1_y = data[data[\"d\"] == 1][\"y\"]\n",
    "f_stat, p_value = f_oneway(group0_y, group1_y)\n",
    "print(\"\\nANOVA F-statistic:\", f_stat)\n",
    "print(\"ANOVA p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def f_reg(W):\n",
    "    return 210 + 27.4 * W[0] + 13.7 * np.sum(W[1:])\n",
    "\n",
    "\n",
    "def f_ps(W):\n",
    "    return 0.75 * (-W[0] + 0.5 * W[1] - 0.25 * W[2] - 0.1 * W[3])\n",
    "\n",
    "\n",
    "def p_function(Z):\n",
    "    return np.exp(f_ps(Z)) / (1 + np.exp(f_ps(Z)))\n",
    "\n",
    "\n",
    "def generate_data(DGP, n_samples, U):\n",
    "    X = np.random.multivariate_normal(mean=np.zeros(4), cov=np.eye(4), size=n_samples)\n",
    "    Z = np.column_stack(\n",
    "        [\n",
    "            np.exp(0.5 * X[:, 0]),\n",
    "            10 + X[:, 1] / (1 + np.exp(X[:, 0])),\n",
    "            (0.6 + X[:, 0] * X[:, 2] / 25) ** 3,\n",
    "            (20 + X[:, 1] + X[:, 3]) ** 2,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if DGP == 1:\n",
    "        p = p_function(Z)\n",
    "        D = (p >= U).astype(int)\n",
    "        noise_Y0 = np.random.normal(size=(n_samples,))\n",
    "        noise_Y1 = np.random.normal(size=(n_samples,))\n",
    "        Y0 = f_reg(Z) + noise_Y0 + np.random.normal(scale=0.1, size=(n_samples,))\n",
    "        Y1 = 2 * f_reg(Z) + noise_Y1 + np.random.normal(scale=0.1, size=(n_samples,))\n",
    "        return X, Y0, Y1, D\n",
    "    elif DGP == 2:\n",
    "        p = p_function(X)\n",
    "        D = (p >= U).astype(int)\n",
    "        noise_Y0 = np.random.normal(size=(n_samples,))\n",
    "        noise_Y1 = np.random.normal(size=(n_samples,))\n",
    "        Y0 = f_reg(Z) + noise_Y0 + np.random.normal(scale=0.1, size=(n_samples,))\n",
    "        Y1 = 2 * f_reg(Z) + noise_Y1 + np.random.normal(scale=0.1, size=(n_samples,))\n",
    "        return X, Y0, Y1, D\n",
    "    elif DGP == 3:\n",
    "        p = p_function(Z)\n",
    "        D = (p >= U).astype(int)\n",
    "        noise_Y0 = np.random.normal(size=(n_samples,))\n",
    "        noise_Y1 = np.random.normal(size=(n_samples,))\n",
    "        Y0 = f_reg(X) + noise_Y0 + np.random.normal(scale=0.1, size=(n_samples,))\n",
    "        Y1 = 2 * f_reg(X) + noise_Y1 + np.random.normal(scale=0.1, size=(n_samples,))\n",
    "        return X, Y0, Y1, D\n",
    "    elif DGP == 4:\n",
    "        p = p_function(X)\n",
    "        D = (p >= U).astype(int)\n",
    "        noise_Y0 = np.random.normal(size=(n_samples,))\n",
    "        noise_Y1 = np.random.normal(size=(n_samples,))\n",
    "        Y0 = f_reg(X) + noise_Y0 + np.random.normal(scale=0.1, size=(n_samples,))\n",
    "        Y1 = 2 * f_reg(X) + noise_Y1 + np.random.normal(scale=0.1, size=(n_samples,))\n",
    "        return X, Y0, Y1, D\n",
    "    else:\n",
    "        msg = \"Invalid DGP number\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "U = 0.5\n",
    "\n",
    "X, Y0, Y1, D = generate_data(DGP=1, n_samples=n_samples, U=U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check tomorrow with gpt 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def twfe_did_panel(y1, y0, D, covariates, i_weights=None, inffunc=False):\n",
    "    # Check dimensions\n",
    "    n, k = covariates.shape\n",
    "    assert (\n",
    "        len(y1) == len(y0) == len(D) == n\n",
    "    ), \"Dimensions of y1, y0, and D must match with covariates\"\n",
    "\n",
    "    # Default weights\n",
    "    if i_weights is None:\n",
    "        i_weights = np.ones(n)\n",
    "\n",
    "    # Main regression model\n",
    "    X = np.concatenate((covariates, D.reshape(-1, 1)), axis=1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y1, sample_weight=i_weights)\n",
    "    y1_hat = model.predict(X)\n",
    "\n",
    "    model.fit(X, y0, sample_weight=i_weights)\n",
    "    y0_hat = model.predict(X)\n",
    "\n",
    "    ATT = np.mean((y1 - y1_hat) - (y0 - y0_hat))\n",
    "    se = np.std((y1 - y1_hat) - (y0 - y0_hat)) / np.sqrt(n)\n",
    "\n",
    "    if inffunc:\n",
    "        # Calculate influence function\n",
    "        att_inf_func = None  # Calculate influence function\n",
    "\n",
    "    return {\"ATT\": ATT, \"se\": se, \"att_inf_func\": att_inf_func if inffunc else None}\n",
    "\n",
    "\n",
    "# Example data\n",
    "df_pre = df[df[\"d\"] == 0]\n",
    "df_post = df[df[\"d\"] == 1]\n",
    "\n",
    "# Applying the function with pre-treatment and post-treatment periods\n",
    "result = twfe_did_panel(\n",
    "    df_post[\"y\"].values,\n",
    "    df_pre[\"y\"].values,\n",
    "    df_post[\"d\"].values,\n",
    "    df_post[[\"Z1\", \"Z2\", \"Z3\", \"Z4\"]].values,\n",
    ")\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTE_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWFE build by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "\n",
    "ATTE = 0.0  # Adjust this to reflect the true treatment effect\n",
    "\n",
    "custom_delta = 0\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "data = make_did_SZ2020(\n",
    "    n_obs=n_obs,\n",
    "    dgp_type=1,\n",
    "    cross_sectional_data=False,\n",
    "    return_type=\"pd.DataFrame\",\n",
    ")\n",
    "# data\n",
    "formula = \"y ~ d + Z1 + Z2 + Z3 + Z4\"\n",
    "twfe_model = smf.ols(formula=formula, data=data).fit()\n",
    "\n",
    "# Extract the treatment effect estimate (coefficient of the treatment variable)\n",
    "\n",
    "twfe_model.params[\"d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, d = make_did_SZ2020(\n",
    "    n_obs=n_obs,\n",
    "    dgp_type=4,\n",
    "    cross_sectional_data=False,\n",
    "    return_type=\"array\",\n",
    ")  # max 1- 6 dgp types\n",
    "dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "print(dml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "\n",
    "# Parameters for the simulation\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "n_simulations = 100\n",
    "true_treatment_effect = 0.0  # Adjust this based on your DGP\n",
    "\n",
    "# Storage for results\n",
    "biases = []\n",
    "ci_lengths = []\n",
    "coverages = []\n",
    "asymptotic_variances = []\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    # Generate data with heterogeneous treatment effects and non-parallel trends\n",
    "    data = make_did_SZ2020(\n",
    "        n_obs=n_obs,\n",
    "        dgp_type=2,\n",
    "        cross_sectional_data=False,\n",
    "        return_type=\"DataFrame\",\n",
    "    )  # Use dgp_type=2 to simulate more complex DGP\n",
    "\n",
    "    # Convert to panel data structure\n",
    "    data[\"time\"] = np.where(\n",
    "        data.index < n_obs / 2,\n",
    "        0,\n",
    "        1,\n",
    "    )  # Assume the first half is pre-treatment, the second half is post-treatment\n",
    "    data[\"id\"] = np.tile(\n",
    "        np.arange(n_obs // 2),\n",
    "        2,\n",
    "    )  # Assign unique IDs for each individual\n",
    "\n",
    "    # Prepare the data for the TWFE model\n",
    "    panel_data = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": data[\"id\"],\n",
    "            \"time\": data[\"time\"],\n",
    "            \"y\": data[\"y\"],\n",
    "            \"d\": data[\"d\"],\n",
    "            \"Z1\": data[\"Z1\"],\n",
    "            \"Z2\": data[\"Z2\"],\n",
    "            \"Z3\": data[\"Z3\"],\n",
    "            \"Z4\": data[\"Z4\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Fit the TWFE model\n",
    "    model = sm.OLS.from_formula(\"y ~ time * d + Z1 + Z2 + Z3 + Z4\", data=panel_data)\n",
    "    results = model.fit()\n",
    "\n",
    "    # Extract the estimated treatment effect and confidence interval\n",
    "    estimated_tau = results.params[\"time:d\"]\n",
    "    ci = results.conf_int().loc[\"time:d\"]\n",
    "    ci_length = ci[1] - ci[0]\n",
    "    asymptotic_variance = results.bse[\"time:d\"] ** 2\n",
    "\n",
    "    # Calculate bias and check if the true effect is within the confidence interval\n",
    "    bias = estimated_tau - true_treatment_effect\n",
    "    coverage = 1 if ci[0] <= true_treatment_effect <= ci[1] else 0\n",
    "\n",
    "    # Store the results\n",
    "    biases.append(bias)\n",
    "    ci_lengths.append(ci_length)\n",
    "    coverages.append(coverage)\n",
    "    asymptotic_variances.append(asymptotic_variance)\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(biases)\n",
    "med_bias = np.median(biases)\n",
    "rmse = np.sqrt(np.mean(np.array(biases) ** 2))\n",
    "mean_coverage = np.mean(coverages)\n",
    "mean_ci_length = np.mean(ci_lengths)\n",
    "mean_asymptotic_variance = np.mean(asymptotic_variances)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Coverage: {mean_coverage}\")\n",
    "print(f\"Mean CI Length: {mean_ci_length}\")\n",
    "print(f\"Mean Asymptotic Variance: {mean_asymptotic_variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install linearmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Model approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DoubleMLDID` class can be used as any other `DoubleML` class. \n",
    "\n",
    "The score is set to `score='observational'`, since the we generated data where the treatment probability depends on the pretreatment covariates. Further, we will use `in_sample_normalization=True`, since normalization generally improved the results in our simulations (both `score='observational'` and `in_sample_normalization=True` are default values).\n",
    "\n",
    "After initialization, we have to call the `fit()` method to estimate the nuisance elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubleml import DoubleMLDID\n",
    "\n",
    "dml_did = DoubleMLDID(\n",
    "    dml_data,\n",
    "    ml_g=ml_g,\n",
    "    ml_m=ml_m,\n",
    "    score=\"observational\",\n",
    "    in_sample_normalization=True,\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_did.fit()\n",
    "print(dml_did)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, confidence intervals at different levels can be obtained via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dml_did.confint(level=0.90))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Simulation\n",
    "\n",
    "Here, we add a small coverage simulation to highlight the difference to the linear implementation of [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003). We generate multiple datasets, estimate the ATTE and collect the results (this may take some time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rep = 200\n",
    "ATTE = 0.0\n",
    "\n",
    "ATTE_estimates = np.full((n_rep), np.nan)\n",
    "coverage = np.full((n_rep), np.nan)\n",
    "ci_length = np.full((n_rep), np.nan)\n",
    "\n",
    "np.random.seed(42)\n",
    "for i_rep in range(n_rep):\n",
    "    if (i_rep % int(n_rep / 10)) == 0:\n",
    "        print(f\"Iteration: {i_rep}/{n_rep}\")\n",
    "    dml_data = make_did_SZ2020(n_obs=n_obs, dgp_type=4, cross_sectional_data=False)\n",
    "\n",
    "    dml_did = DoubleMLDID(dml_data, ml_g=ml_g, ml_m=ml_m, n_folds=5)\n",
    "    dml_did.fit()\n",
    "\n",
    "    ATTE_estimates[i_rep] = dml_did.coef.squeeze()\n",
    "    confint = dml_did.confint(level=0.95)\n",
    "    coverage[i_rep] = (confint[\"2.5 %\"].iloc[0] <= ATTE) & (\n",
    "        confint[\"97.5 %\"].iloc[0] >= ATTE\n",
    "    )\n",
    "    ci_length[i_rep] = confint[\"97.5 %\"].iloc[0] - confint[\"2.5 %\"].iloc[0]\n",
    "\n",
    "    summary_df = dml_did.summary\n",
    "    std_err = summary_df.loc[\"d\", \"std err\"]\n",
    "    asymptotic_variance[i_rep] = std_err**2\n",
    "\n",
    "# Calculate metrics\n",
    "avg_bias = np.mean(ATTE_estimates - ATTE)\n",
    "med_bias = np.median(ATTE_estimates - ATTE)\n",
    "rmse = np.sqrt(np.mean((ATTE_estimates - ATTE) ** 2))\n",
    "avg_asymptotic_variance = np.mean(asymptotic_variance)\n",
    "coverage_probability = np.mean(coverage)\n",
    "avg_ci_length = np.mean(ci_length)\n",
    "\n",
    "# Print results\n",
    "print(f\"Av. Bias: {avg_bias}\")\n",
    "print(f\"Med. Bias: {med_bias}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Asy. V: {avg_asymptotic_variance}\")\n",
    "print(f\"Cover: {coverage_probability}\")\n",
    "print(f\"CIL: {avg_ci_length}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the corresponding coverage and the length of the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Coverage: {coverage.mean()}\")\n",
    "print(f\"Average CI length: {ci_length.mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can observe that the coverage is still valid, since we did not rely on linear learners, so the setting is not misspecified in this example. \n",
    "\n",
    "If we know the conditional expectation is correctly specified (linear form), we can use this to obtain smaller confidence intervals but in many applications, we may want to safeguard against misspecification and use flexible models such as random forest or boosting.\n",
    "\n",
    "The distribution of the estimates takes the following form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_pa = pd.DataFrame(ATTE_estimates, columns=[\"Estimate\"])\n",
    "g = sns.kdeplot(df_pa, fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backtracking:\n",
    "pip install keras==2.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from doubleml import DoubleMLData, DoubleMLDID\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "from lightgbm import LGBMRegressor\n",
    "from scikeras.wrappers import KerasClassifier  # pip install scikeras\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "x, y, d = make_did_SZ2020(\n",
    "    n_obs=n_obs,\n",
    "    dgp_type=4,\n",
    "    cross_sectional_data=False,\n",
    "    return_type=\"array\",\n",
    ")\n",
    "dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d)\n",
    "\n",
    "# Function to create Keras model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=x.shape[1], activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))  # Assuming binary classification\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Wrap the Keras model with KerasClassifier\n",
    "keras_classifier = KerasClassifier(\n",
    "    build_fn=create_model,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Use StandardScaler to normalize data and then use the Keras classifier in a pipeline\n",
    "ml_m = Pipeline([(\"scaler\", StandardScaler()), (\"nn\", keras_classifier)])\n",
    "\n",
    "# Use LGBMRegressor for regression\n",
    "n_estimators = 30\n",
    "ml_g = LGBMRegressor(n_estimators=n_estimators)\n",
    "\n",
    "dml_plr = DoubleMLDID(dml_data, ml_g, ml_m)\n",
    "dml_plr.fit()\n",
    "\n",
    "print(dml_plr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Cross-Sectional Data\n",
    "\n",
    "For repeated cross-sectional data, we assume that we observe i.i.d. data $W_i = (Y_{i}, D_i, X_i, T_i)$. \n",
    "\n",
    "Here $Y_i = T_i Y_{i1} + (1-T_i)Y_{i0}$ corresponds to the outcome of unit $i$ which is observed at time $T_i$.\n",
    "\n",
    "### Data\n",
    "\n",
    "As for panel data, we will use the implemented data generating process `make_did_SZ2020` to generate data according to the simulation in [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) (Section 4.2). \n",
    "\n",
    "In this example, we will use `dgp_tpye=4`, which corresponds to the misspecified settings in [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003) (other data generating processes are also available via the `dgp_type` parameter). In all settings the true ATTE is zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to other `DoubleMLData` objects, we have to specify which column corresponds to our time variable $T$.\n",
    "\n",
    "The time variable can be simply set via the argument `t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml.datasets import make_did_SZ2020\n",
    "\n",
    "np.random.seed(42)\n",
    "n_obs = 1000\n",
    "x, y, d, t = make_did_SZ2020(\n",
    "    n_obs=n_obs,\n",
    "    dgp_type=4,\n",
    "    cross_sectional_data=True,\n",
    "    return_type=\"array\",\n",
    ")\n",
    "dml_data = DoubleMLData.from_arrays(x=x, y=y, d=d, t=t)\n",
    "print(dml_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTE Estimation\n",
    "\n",
    "To estimate the ATTE with panel data, we will use the `DoubleMLDIDCS` class. \n",
    "\n",
    "As for all `DoubleML` classes, we have to specify learners, which have to be initialized first.\n",
    "Here, we will just rely on a tree based method. \n",
    "\n",
    "The learner `ml_g` is used to fit conditional expectations of the outcome $\\mathbb{E}[\\Delta Y_i| D_i=d, T_i =t, X_i]$ for all combinations of $d,t\\in\\{0,1\\}$, whereas the learner `ml_m` will be used to estimate the propensity score $P(D_i=1|X_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "n_estimators = 30\n",
    "ml_g = LGBMRegressor(n_estimators=n_estimators)\n",
    "ml_m = LGBMClassifier(n_estimators=n_estimators)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DoubleMLDIDCS` class can be used as any other `DoubleML` class. \n",
    "\n",
    "The score is set to `score='observational'`, since the we generated data where the treatment probability depends on the pretreatment covariates. Further, we will use `in_sample_normalization=True`, since normalization generally improved the results in our simulations (both `score='observational'` and `in_sample_normalization=True` are default values).\n",
    "\n",
    "After initialization, we have to call the `fit()` method to estimate the nuisance elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubleml import DoubleMLDIDCS\n",
    "\n",
    "dml_did = DoubleMLDIDCS(\n",
    "    dml_data,\n",
    "    ml_g=ml_g,\n",
    "    ml_m=ml_m,\n",
    "    score=\"observational\",\n",
    "    in_sample_normalization=True,\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_did.fit()\n",
    "print(dml_did)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, confidence intervals at different levels can be obtained via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dml_did.confint(level=0.90))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage Simulation\n",
    "\n",
    "Again, we add a small coverage simulation to highlight the difference to the linear implementation of [Sant'Anna and Zhao (2020)](https://doi.org/10.1016/j.jeconom.2020.06.003). We generate multiple datasets, estimate the ATTE and collect the results (this may take some time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rep = 200\n",
    "ATTE = 0.0\n",
    "\n",
    "ATTE_estimates = np.full((n_rep), np.nan)\n",
    "coverage = np.full((n_rep), np.nan)\n",
    "ci_length = np.full((n_rep), np.nan)\n",
    "\n",
    "np.random.seed(42)\n",
    "for i_rep in range(n_rep):\n",
    "    if (i_rep % int(n_rep / 10)) == 0:\n",
    "        print(f\"Iteration: {i_rep}/{n_rep}\")\n",
    "    dml_data = make_did_SZ2020(n_obs=n_obs, dgp_type=4, cross_sectional_data=True)\n",
    "\n",
    "    dml_did = DoubleMLDIDCS(dml_data, ml_g=ml_g, ml_m=ml_m, n_folds=5)\n",
    "    dml_did.fit()\n",
    "\n",
    "    ATTE_estimates[i_rep] = dml_did.coef.squeeze()\n",
    "    confint = dml_did.confint(level=0.95)\n",
    "    coverage[i_rep] = (confint[\"2.5 %\"].iloc[0] <= ATTE) & (\n",
    "        confint[\"97.5 %\"].iloc[0] >= ATTE\n",
    "    )\n",
    "    ci_length[i_rep] = confint[\"97.5 %\"].iloc[0] - confint[\"2.5 %\"].iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the corresponding coverage and the length of the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Coverage: {coverage.mean()}\")\n",
    "print(f\"Average CI length: {ci_length.mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for panel data the coverage is still valid, since we did not rely on linear learners, so the setting is not misspecified in this example. \n",
    "\n",
    "If we know the conditional expectation is correctly specified (linear form), we can use this to obtain smaller confidence intervals but in many applications, we may want to safeguard against misspecification and use flexible models such as random forest or boosting.\n",
    "\n",
    "The distribution of the estimates takes the following form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_pa = pd.DataFrame(ATTE_estimates, columns=[\"Estimate\"])\n",
    "g = sns.kdeplot(df_pa, fill=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
