{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"src/difference_in_difference_with_deep_learning/data/testdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\n",
    "    \"src/difference_in_difference_with_deep_learning/data_management/data_info.yaml\",\n",
    ") as file:\n",
    "    data_info = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_select_1 = data[data_info[\"categorical_columns\"][0]]\n",
    "data_select_2 = data[data_info[\"categorical_columns\"][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, data_info[\"causal_effect\"]] = data_select_1 * data_select_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_regression(data, data_info):\n",
    "    # Extract variables from data_info\n",
    "    outcome_variable = data_info[\"outcome\"]\n",
    "    causal_effect_variable = data_info[\"causal_effect\"]\n",
    "    categorical_columns = data_info[\"categorical_columns\"]\n",
    "    control_columns = data_info[\"control_columns\"]\n",
    "\n",
    "    # Create formula string\n",
    "    formula = f\"{outcome_variable} ~ {causal_effect_variable} + {' + '.join(categorical_columns)}  + {' + '.join(control_columns)}\"\n",
    "\n",
    "    # Set up the regression model\n",
    "    reg_model = smf.ols(formula=formula, data=data)\n",
    "\n",
    "    # Fit the regression model\n",
    "    results = reg_model.fit()\n",
    "\n",
    "    # Return the summary of the regression results\n",
    "    return results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of how to change summary output for my need (postponed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = did_regression(data, data_info)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = summary.tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r_squared_row = data0.loc[data0[0] == \"Adj. R-squared:\"]\n",
    "no_observations_row = data0.loc[data0[3] == \"No. Observations:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = summary.tables[1]\n",
    "index = [\"Intercept\", \"interaction\", \"FQ\", \"Reform\", \"Age\", \"WagePartner\"]\n",
    "summary_df = pd.DataFrame(data, index=index)\n",
    "summary_df = summary_df.round(3)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.concat(\n",
    "    [summary_df, adj_r_squared_row, no_observations_row],\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = summary_df.to_latex()\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in summary.tables:\n",
    "    print(table.as_latex_tabular())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby richtig machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_regression(data, data_info):\n",
    "    \"\"\"Estimate regression models for each time period and summarize the results.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): The dataset containing all variables.\n",
    "        data_info (dict): Dictionary containing data configuration information.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing summary statistics for each time period.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    for time_period in data[data_info[\"time\"]].unique():\n",
    "        # Filter the data for the current time period\n",
    "        data_time_period = data[data[data_info[\"time\"]] == time_period]\n",
    "\n",
    "        # Define the regression formula\n",
    "        formula = f\"{data_info['outcome']} ~ {data_info['causal_effect']} + {' + '.join(data_info['categorical_columns'] )} + {' + '.join(data_info['control_columns'])}\"\n",
    "\n",
    "        # Fit the regression model\n",
    "        reg_model = smf.ols(formula=formula, data=data_time_period)\n",
    "        results = reg_model.fit()\n",
    "\n",
    "        # Extract coefficient, std. error, and p-value\n",
    "        coefficient = results.params[data_info[\"causal_effect\"]]\n",
    "        std_error = results.bse[data_info[\"causal_effect\"]]\n",
    "        p_value = results.pvalues[data_info[\"causal_effect\"]]\n",
    "\n",
    "        # Calculate the control mean\n",
    "        control_mean = data_time_period[data_info[\"outcome\"]].mean()\n",
    "\n",
    "        # Calculate the difference between treatment and control groups\n",
    "        difference_tc = (\n",
    "            data_time_period[data_time_period[data_info[\"causal_effect\"]] == 1][\n",
    "                data_info[\"outcome\"]\n",
    "            ].mean()\n",
    "            - control_mean\n",
    "        )\n",
    "\n",
    "        # Calculate the difference with controls\n",
    "        # Assume mean of interaction term for simplicity\n",
    "        data_time_period[data_info[\"causal_effect\"]].mean()\n",
    "        difference_tc_controls = (\n",
    "            difference_tc\n",
    "            - coefficient\n",
    "            * (\n",
    "                data_time_period[data_info[\"control_columns\"]]\n",
    "                - data_time_period[data_info[\"control_columns\"]].mean()\n",
    "            )\n",
    "            .mean()\n",
    "            .sum()\n",
    "        )\n",
    "\n",
    "        # Append the results to the list\n",
    "        results_list.append(\n",
    "            {\n",
    "                \"Time Period\": time_period,\n",
    "                \"Control Mean\": control_mean,\n",
    "                \"Difference T-C\": difference_tc,\n",
    "                \"Difference T-C with Controls\": difference_tc_controls,\n",
    "                \"Coefficient\": coefficient,\n",
    "                \"Std. Error\": std_error,\n",
    "                \"P-value\": p_value,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    return pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = estimate_regression(data, data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "time_order = [\"t-2\", \"t-1\", \"t+1\", \"t+2\", \"t+3\"]\n",
    "\n",
    "# Convert Time column to categorical with defined order\n",
    "df[\"time\"] = pd.Categorical(df[\"time\"], categories=time_order, ordered=True)\n",
    "\n",
    "\n",
    "# Calculate average wage for each group over time\n",
    "grouped = df.groupby([\"FQ\", \"Reform\", \"time\"])[\"wage_year\"].mean().reset_index()\n",
    "\n",
    "# Separate data for the two groups\n",
    "group0 = grouped[(grouped[\"FQ\"] == 0) & (grouped[\"Reform\"] == 0)]\n",
    "group1 = grouped[(grouped[\"FQ\"] == 1) & (grouped[\"Reform\"] == 1)]\n",
    "\n",
    "# Calculate counterfactual average wage for each time period\n",
    "counterfactual = group0.copy()\n",
    "counterfactual[\"wage_year\"] += group1[\"wage_year\"].mean() - group0[\"wage_year\"].mean()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(group0[\"time\"], group0[\"wage_year\"], label=\"Group 0\", marker=\"o\")\n",
    "plt.plot(group1[\"time\"], group1[\"wage_year\"], label=\"Group 1\", marker=\"o\")\n",
    "plt.plot(\n",
    "    counterfactual[\"time\"],\n",
    "    counterfactual[\"wage_year\"],\n",
    "    label=\"Counterfactual\",\n",
    "    linestyle=\"--\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Average Wage\")\n",
    "plt.title(\"Difference-in-Differences Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep learning estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stopping Tensorflow from printing info messages and warnings.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_boolean(\"update\", False, \"\"\"Record the simulation results.\"\"\")\n",
    "tf.flags.DEFINE_boolean(\"plot_true\", False, \"\"\"Show plots.\"\"\")\n",
    "tf.flags.DEFINE_boolean(\"verbose\", True, \"\"\"Show detailed messages.\"\"\")\n",
    "tf.flags.DEFINE_integer(\"nsimulations\", 1, \"\"\"How many simulations to run.\"\"\")\n",
    "tf.flags.DEFINE_integer(\n",
    "    \"nconsumer_characteristics\",\n",
    "    100,\n",
    "    \"\"\"Number of consumer characteristics.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"treatment\",\n",
    "    \"not_random\",\n",
    "    \"\"\"Are customers treated at random or not.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"model\",\n",
    "    \"quadratic\",\n",
    "    \"\"\"Is the mapping from consumer characteristics\n",
    "                       to their preferences linear or quadratic.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"architecture\",\n",
    "    \"architecture_1_\",\n",
    "    \"\"\"Which NN architecture to use.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_integer(\"data_seed\", None, \"\"\"Seed to use to create fake data.\"\"\")\n",
    "\n",
    "# Manually set sys.argv to simulate command-line invocation\n",
    "sys.argv = [sys.argv[0]]\n",
    "\n",
    "# Parse flags\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "# Access remaining arguments after parsing flags\n",
    "remaining_args = [arg for arg in sys.argv[1:] if arg.startswith(\"--\")]\n",
    "assert remaining_args == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different architectures for the first NN\n",
    "if FLAGS.architecture == \"architecture_1_\":\n",
    "    hidden_layer_sizes = [20, 10, 5]\n",
    "    dropout_rates_train = [0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_2_\":\n",
    "    hidden_layer_sizes = [60, 30, 20]\n",
    "    dropout_rates_train = [0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_3_\":\n",
    "    hidden_layer_sizes = [80, 80, 80]\n",
    "    dropout_rates_train = [0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_4_\":\n",
    "    hidden_layer_sizes = [20, 15, 10, 5]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_5_\":\n",
    "    hidden_layer_sizes = [60, 30, 20, 10]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_6_\":\n",
    "    hidden_layer_sizes = [80, 80, 80, 80]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_7_\":\n",
    "    hidden_layer_sizes = [20, 15, 15, 10, 10, 5]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_8_\":\n",
    "    hidden_layer_sizes = [60, 30, 20, 20, 10, 5]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_9_\":\n",
    "    hidden_layer_sizes = [80, 80, 80, 80, 80, 80]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "else:\n",
    "    msg = \"Architecture not found! Check the spelling.\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "if FLAGS.nconsumer_characteristics < 20:\n",
    "    raise ValueError(\n",
    "        \"Number of consumer characteristics \" + \"should not be less than 20.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates_test = [0 for i in dropout_rates_train]\n",
    "\n",
    "# Architecture for the second NN that estimates\n",
    "# propensity scores\n",
    "hidden_layer_sizes_treatment = [50, 30]\n",
    "activation_functions_treatment = [\"relu\", \"relu\", \"none\"]\n",
    "dropout_rates_train_treatment = [0, 0, 0]\n",
    "dropout_rates_test_treatment = [0 for i in dropout_rates_train_treatment]\n",
    "\n",
    "# Setting parameters values for generating fake data\n",
    "nconsumers = 10000\n",
    "\n",
    "# Run parameters\n",
    "train_proportion = 0.9\n",
    "max_nepochs = 5000\n",
    "max_epochs_without_change = 30\n",
    "\n",
    "early_stopping = train_proportion != 1\n",
    "\n",
    "optimizer = \"Adam\"\n",
    "learning_rate = 0.009\n",
    "batch_size = 128\n",
    "batch_size_t = None\n",
    "\n",
    "# Regularization parameters\n",
    "alpha = 0.0\n",
    "r = 0.2\n",
    "\n",
    "# Checking for spelling errors\n",
    "if not (FLAGS.model == \"quadratic\" or FLAGS.model == \"simple\"):\n",
    "    msg = \"Check whether model type is spelled correctly!\"\n",
    "    raise ValueError(msg)\n",
    "if not (FLAGS.treatment == \"random\" or FLAGS.treatment == \"not_random\"):\n",
    "    msg = \"Check whether treatment type is spelled correctly!\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train = T_train = Y_train = X_valid = T_valid = Y_valid = X = T_real = Y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_inds(t):\n",
    "    \"\"\"Split the dataset into training and validation sets while\n",
    "    preserving the proportion of targeted customers in both datasets.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        t: array-like, shape=(N, 1)\n",
    "            Treatment array.\n",
    "    Outputs:\n",
    "    -------\n",
    "        train_inds: array of bools\n",
    "            Indices of the training set.\n",
    "        valid_inds: array of bools\n",
    "            Indices of the validation set.\n",
    "    \"\"\"\n",
    "    t_array = np.array(t)\n",
    "    train_inds = np.zeros(len(t_array), dtype=bool)\n",
    "    valid_inds = np.zeros(len(t_array), dtype=bool)\n",
    "    values = np.unique(t_array)\n",
    "    for value in values:\n",
    "        value_inds = np.nonzero(t_array == value)[0]\n",
    "        np.random.shuffle(value_inds)\n",
    "        n = int(train_proportion * len(value_inds))\n",
    "        train_inds[value_inds[:n]] = True\n",
    "        valid_inds[value_inds[n:]] = True\n",
    "    return train_inds, valid_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatgpt approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the real data\n",
    "data = pd.read_csv(\n",
    "    \"bld/difference_in_difference_with_deep_learning/data/cleaned_data.csv\",\n",
    ")  # Change the filename to your actual data file\n",
    "\n",
    "# Assuming 'wage_year' is the variable of interest\n",
    "X_train = data[[\"Individual\", \"Age\", \"WagePartner\", \"interaction\"]]  # Features\n",
    "T_train = data[\"Reform\"]  # Treatment assignment\n",
    "Y_train = data[\"wage_year\"]  # Outcome\n",
    "\n",
    "# For demonstration, assuming no validation data and using all data for training\n",
    "X_valid = X_train\n",
    "T_valid = T_train\n",
    "Y_valid = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [100, 50, 20]  # Adjust based on the complexity of your problem\n",
    "activation_functions = [\"relu\", \"relu\", \"none\"]\n",
    "dropout_rates_train = [0.1, 0.2, 0.0]\n",
    "\n",
    "# Adjust run parameters\n",
    "train_proportion = 0.8  # Adjust based on your dataset size and computational resources\n",
    "max_nepochs = 1000\n",
    "max_epochs_without_change = 20\n",
    "optimizer = \"Adam\"\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Prepare real data (preprocessing steps)\n",
    "# You might need to normalize numerical features, encode categorical variables, handle missing values, etc.\n",
    "# For example, using sklearn.preprocessing.StandardScaler for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealData:\n",
    "    \"\"\"Class to handle real data for testing the NN method.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        X: pandas DataFrame\n",
    "            Features.\n",
    "        T: pandas Series\n",
    "            Treatment assignment.\n",
    "        Y: pandas Series\n",
    "            Outcome.\n",
    "        treatment: {'random', 'not_random'}\n",
    "            If 'random' consumers are being treated at random.\n",
    "            Otherwise, probability of being treated is a function\n",
    "            of consumer characteristics.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        T,\n",
    "        Y,\n",
    "        treatment=\"not_random\",\n",
    "        nconsumer_characteristics=FLAGS.nconsumer_characteristics,\n",
    "    ):\n",
    "        self.X = X\n",
    "        self.T = T\n",
    "        self.Y = Y\n",
    "        self.treatment = treatment\n",
    "        self.nconsumer_characteristics = nconsumer_characteristics\n",
    "\n",
    "        # Placeholder for other variables\n",
    "        self.seed = None\n",
    "        self.prob_of_T = None\n",
    "        self.tau_true_mean = None\n",
    "\n",
    "    def prepare_real_data(self):\n",
    "        \"\"\"Prepare the real data.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            Tuple containing the real data attributes:\n",
    "            (Y, X, mu0, tau, T, seed, prob_of_T, tau_true_mean)\n",
    "        \"\"\"\n",
    "        # You may need to perform additional preprocessing steps here\n",
    "        # For example, encoding categorical variables, handling missing values, etc.\n",
    "\n",
    "        # Placeholder for the data attributes\n",
    "        mu0 = None  # Placeholder for mu0\n",
    "        tau = None  # Placeholder for tau\n",
    "        seed = None  # Placeholder for seed\n",
    "        prob_of_T = None  # Placeholder for propensity scores\n",
    "        tau_true_mean = None  # Placeholder for true average treatment effect\n",
    "\n",
    "        return (self.Y, self.X, mu0, tau, self.T, seed, prob_of_T, tau_true_mean)\n",
    "\n",
    "\n",
    "# Usage with real data\n",
    "real_data = RealData(X_train_scaled, T_train, Y_train)\n",
    "Y, X, mu0, tau, T, seed, prob_of_T, tau_true_mean = real_data.prepare_real_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_inds(t, train_proportion):\n",
    "    \"\"\"Split the dataset into training and validation sets while\n",
    "    preserving the proportion of targeted customers in both datasets.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        t: array-like, shape=(N, 1)\n",
    "            Treatment array.\n",
    "        train_proportion: float\n",
    "            Proportion of the dataset to be used for training.\n",
    "\n",
    "    Outputs:\n",
    "    -------\n",
    "        train_inds: array of bools\n",
    "            Indices of the training set.\n",
    "        valid_inds: array of bools\n",
    "            Indices of the validation set.\n",
    "    \"\"\"\n",
    "    t_array = np.array(t)\n",
    "    train_inds = np.zeros(len(t_array), dtype=bool)\n",
    "    valid_inds = np.zeros(len(t_array), dtype=bool)\n",
    "    values = np.unique(t_array)\n",
    "    for value in values:\n",
    "        value_inds = np.nonzero(t_array == value)[0]\n",
    "        np.random.shuffle(value_inds)\n",
    "        n = int(train_proportion * len(value_inds))\n",
    "        train_inds[value_inds[:n]] = True\n",
    "        valid_inds[value_inds[n:]] = True\n",
    "    return train_inds, valid_inds\n",
    "\n",
    "\n",
    "def calculate_batch_size(batch_size, X_train):\n",
    "    \"\"\"If batch_size is int than do nothing, else if batch_size is\n",
    "    equal to None, set batch size to be of a size equal to the\n",
    "    length of the training dataset.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        batch_size: int or None\n",
    "            Batch size.\n",
    "        X_train: ndarray\n",
    "            Array of consumer characteristics on which to\n",
    "            perform training.\n",
    "    Outputs:\n",
    "    -------\n",
    "        batch_size: int\n",
    "            Batch size.\n",
    "    \"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = len(X_train)\n",
    "    return batch_size\n",
    "\n",
    "\n",
    "def plotting_loss_functions(loss1, loss2=None, add_title=\"\"):\n",
    "    \"\"\"Plot the loss functions.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        loss1: list of floats\n",
    "            First list of recorded losses through epochs.\n",
    "        loss2: list of floats\n",
    "            Second list of recorded losses through epochs.\n",
    "        add_title: string\n",
    "            Addition to the title of the graph.\n",
    "    \"\"\"\n",
    "    if loss2 is None:\n",
    "        loss2 = []\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.clf()\n",
    "    plt.plot(range(len(loss1)), loss1, \"r-\", lw=3)\n",
    "    if early_stopping:\n",
    "        plt.plot(range(len(loss2)), loss2, \"b-\", lw=3)\n",
    "        plt.legend([\"loss on training set\", \"loss on validation set\"])\n",
    "        plt.title(\"Loss on training and validation set\" + add_title, fontsize=14)\n",
    "    else:\n",
    "        plt.legend([\"loss on training set\"])\n",
    "        plt.title(\"Loss on training set\" + add_title, fontsize=14)\n",
    "    plt.xlabel(\"Epoch number\", fontsize=14)\n",
    "    plt.ylabel(\"Loss\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from here errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Create a neural network with specified properties.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        hidden_layer_sizes: list of ints\n",
    "            Length of the list defines the number of hidden layers.\n",
    "            Entries of the list define the number of hidden units in\n",
    "            each hidden layer.\n",
    "        activation_functions: list of {'relu', 'lrelu', 'prelu',\n",
    "                                       'srelu', 'plu', 'elu', 'none'}\n",
    "            Activation function for each layer.\n",
    "            Has to be of length len(hidden_layer_sizes) + 1.\n",
    "        dropout_rates_train:  list of floats\n",
    "            Dropout rate to be used during training for each layer.\n",
    "            Has to be of length len(hidden_layer_sizes) + 1.\n",
    "        batch_size: int\n",
    "            Batch size.\n",
    "        size_of_the_output: int\n",
    "            Number of units in the output layer.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "        alpha: float\n",
    "            Regularization strength parameter.\n",
    "        r_par: float\n",
    "            Mixing ratio of Ridge and Lasso regression.\n",
    "            Has to be between 0 and 1.\n",
    "        max_epochs_without_change: int\n",
    "            Number of epochs with no improvement on the validation loss\n",
    "            to wait before stopping the training.\n",
    "        max_nepochs: int\n",
    "            Maximum number of epochs for which NNs will be trained.\n",
    "        optimizer: string\n",
    "            Optimizer\n",
    "        learning_rate: scalar\n",
    "            Learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes,\n",
    "        activation_functions,\n",
    "        dropout_rates_train,\n",
    "        batch_size,\n",
    "        size_of_the_output,\n",
    "        nconsumer_characteristics,\n",
    "        alpha,\n",
    "        r_par,\n",
    "        max_epochs_without_change,\n",
    "        max_nepochs,\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "    ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation_functions = activation_functions\n",
    "        self.dropout_rates_train = dropout_rates_train\n",
    "        self.dropout_rates_test = [0 for i in dropout_rates_train]\n",
    "        self.batch_size = batch_size\n",
    "        self.size_of_the_output = size_of_the_output\n",
    "        self.nconsumer_characteristics = nconsumer_characteristics\n",
    "        self.alpha = alpha\n",
    "        self.r_par = r_par\n",
    "        self.max_epochs_without_change = max_epochs_without_change\n",
    "        self.max_nepochs = max_nepochs\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _fully_connected_layer_builder(\n",
    "        self,\n",
    "        input_data,\n",
    "        hidden_layer_size,\n",
    "        total_num_features,\n",
    "        scope_name,\n",
    "        activation,\n",
    "        dropout_rate,\n",
    "    ):\n",
    "        \"\"\"Build a fully connected layer within the NN.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            input_data: Tensor\n",
    "                Output from the previous layer.\n",
    "            hidden_layer_size: int\n",
    "                Size of the current layer.\n",
    "            total_num_features: int\n",
    "                Number of units from the previous layer.\n",
    "            scope_name: string\n",
    "                Scope name.\n",
    "            activation: {'relu', 'lrelu', 'prelu', 'srelu',\n",
    "                        'plu', 'elu', 'none'}\n",
    "                Activation function.\n",
    "            dropout_rate: scalar\n",
    "                Dropout rate. Has to be between 0 and 1.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            hid_layer_activation: Tensor\n",
    "                The hidden layer output.\n",
    "        \"\"\"\n",
    "        # Dropout:\n",
    "        input_data = tf.keras.layers.Dropout(rate=dropout_rate)(input_data)\n",
    "\n",
    "        # Creating weights and bias terms for our fully connected layer\n",
    "        with tf.name_scope(scope_name):\n",
    "            weights = np.sqrt(2) * tf.Variable(\n",
    "                initial_value=tf.random.normal(\n",
    "                    [total_num_features, hidden_layer_size],\n",
    "                    mean=0.0,\n",
    "                    stddev=1.0,\n",
    "                ),\n",
    "                name=\"weights\",\n",
    "            )\n",
    "            b = tf.Variable(tf.zeros([hidden_layer_size]), name=\"biases\")\n",
    "\n",
    "        # Defining the fully connected neural network layer\n",
    "        hid_layer_activation = tf.matmul(input_data, weights) + b\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            hid_layer_activation = tf.nn.relu(hid_layer_activation)\n",
    "        elif activation == \"lrelu\":\n",
    "            hid_layer_activation = tf.nn.leaky_relu(\n",
    "                hid_layer_activation,\n",
    "                alpha=0.2,\n",
    "                name=\"lrelu\",\n",
    "            )\n",
    "        elif activation == \"prelu\":\n",
    "            hid_layer_activation = tf.keras.layers.PReLU()(hid_layer_activation)\n",
    "        elif activation == \"srelu\":\n",
    "            hid_layer_activation = srelu_activation(hid_layer_activation, scope_name)\n",
    "        elif activation == \"plu\":\n",
    "            hid_layer_activation = plu_activation(hid_layer_activation)\n",
    "        elif activation == \"elu\":\n",
    "            hid_layer_activation = tf.nn.elu(hid_layer_activation)\n",
    "        elif activation == \"none\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Activation function not recognized! \" + \"Check the spelling.\",\n",
    "            )\n",
    "        return hid_layer_activation\n",
    "\n",
    "    def _building_the_network(self, layer_input, dropout_rates):\n",
    "        \"\"\"Build the whole fully connected NN.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            layer_input: Tensor\n",
    "                Input layer.\n",
    "            dropout_rates: list of floats\n",
    "                Dropout rate for each layer. Each entry has to\n",
    "                be between 0 and 1. Has to be of length\n",
    "                len(hidden_layer_sizes) + 1.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            output_fc_layer: Tensor\n",
    "                Output layer.\n",
    "        \"\"\"\n",
    "        hidden_layer_sizes_expand = [\n",
    "            *self.hidden_layer_sizes,\n",
    "            self.size_of_the_output,\n",
    "            self.nconsumer_characteristics,\n",
    "        ]\n",
    "\n",
    "        for i in range(len(self.hidden_layer_sizes) + 1):\n",
    "            output_fc_layer = self._fully_connected_layer_builder(\n",
    "                input_data=layer_input,\n",
    "                hidden_layer_size=hidden_layer_sizes_expand[i],\n",
    "                total_num_features=hidden_layer_sizes_expand[i - 1],\n",
    "                scope_name=\"l\" + str(i + 1),\n",
    "                activation=self.activation_functions[i],\n",
    "                dropout_rate=dropout_rates[i],\n",
    "            )\n",
    "            layer_input = output_fc_layer\n",
    "        return output_fc_layer\n",
    "\n",
    "    def _building_the_network_estimates_TE(\n",
    "        self,\n",
    "        input_data,\n",
    "        t_var,\n",
    "        y_var,\n",
    "        dropout_rates,\n",
    "    ):\n",
    "        \"\"\"Build the neural network that estimates treatment\n",
    "        coefficients.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            input_data: Tensor\n",
    "                Input layer.\n",
    "            t_var: Tensor\n",
    "                Treatment\n",
    "            y_var: Tensor\n",
    "                Target variable\n",
    "            dropout_rates: list of floats\n",
    "                Dropout rate for each layer. Each entry has to\n",
    "                be between 0 and 1. Has to be of length\n",
    "                len(hidden_layer_sizes) + 1.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            output: Tensor\n",
    "                Treatment coefficients.\n",
    "            loss: scalar\n",
    "                Loss without regularization.\n",
    "        \"\"\"\n",
    "        output = self._building_the_network(input_data, dropout_rates)\n",
    "        tau = output[:, 0:1]\n",
    "        mu0 = output[:, 1:2]\n",
    "        Y_predicted = tf.multiply(t_var, tau) + mu0\n",
    "\n",
    "        # Mean squared error loss:\n",
    "        loss = tf.keras.losses.mean_squared_error(y_var, Y_predicted)\n",
    "        return output, loss\n",
    "\n",
    "    def _building_the_network_estimates_PS(self, input_data, t_var, dropout_rates):\n",
    "        \"\"\"Build the neural network that estimates propensity\n",
    "        scores.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            input_data: Tensor\n",
    "                Input layer.\n",
    "            t_var: Tensor\n",
    "                Treatment\n",
    "            dropout_rates: list of floats\n",
    "                Dropout rate for each layer. Each entry has to\n",
    "                be between 0 and 1. Has to be of length\n",
    "                len(hidden_layer_sizes) + 1.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            output: Tensor\n",
    "                Output of the NN.\n",
    "            loss: scalar\n",
    "                Loss without regularization.\n",
    "        \"\"\"\n",
    "        output = self._building_the_network(input_data, dropout_rates)\n",
    "\n",
    "        # Calculating binary crossentropy loss\n",
    "        loss = tf.keras.losses.binary_crossentropy(\n",
    "            t_var,\n",
    "            tf.squeeze(output),\n",
    "            from_logits=True,\n",
    "        )\n",
    "        return output, loss\n",
    "\n",
    "    def _calc_the_loss_with_reg(self, loss_before_regularization):\n",
    "        \"\"\"Calculate loss with regularization.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            loss_before_regularization: scalar\n",
    "                Loss without regularization.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            total_loss: float\n",
    "                Loss with regularization.\n",
    "        \"\"\"\n",
    "        l1_l2_regularizer = tf.keras.regularizers.L1L2(\n",
    "            l1=self.alpha * self.r_par,\n",
    "            l2=self.alpha * (1 - self.r_par),\n",
    "        )\n",
    "        regularization_term = l1_l2_regularizer(tf.trainable_variables())\n",
    "\n",
    "        return loss_before_regularization + regularization_term\n",
    "\n",
    "    def _optimize_the_loss_function(self, loss_with_regularization):\n",
    "        \"\"\"Update the weights after one training step.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            loss_with_regularization: scalar\n",
    "                Loss with regularization.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            train_step: Operation that updates the weights\n",
    "        \"\"\"\n",
    "        if self.optimizer == \"RMSProp\":\n",
    "            optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n",
    "        elif self.optimizer == \"GradientDescent\":\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        elif self.optimizer == \"Adam\":\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Optimizer not recognized! Please choose from \"RMSProp\", \"GradientDescent\", or \"Adam\".',\n",
    "            )\n",
    "\n",
    "        return optimizer.minimize(loss_with_regularization)\n",
    "\n",
    "    def _create_minibatches(self, X, T, Y, shuffle=False):\n",
    "        \"\"\"Create mini-batches generator. Yields a mini-batch of batch_size\n",
    "        length of consumer characteristics, X, treatments, T, target\n",
    "        values, Y.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            X: ndarray, shape=(len(X_train), nconsumer_characteristics)\n",
    "                Array of consumer characteristics.\n",
    "            T: ndarray, shape=(len(X_train), 1)\n",
    "                Treatment array.\n",
    "            Y: ndarray, shape=(len(X_train), 1)\n",
    "                Target value array.\n",
    "            shuffle: bool\n",
    "                If True, shuffle the array.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            X[excerpt]: ndarray, shape=(batch_size,\n",
    "                                        nconsumer_characteristics)\n",
    "                Mini batch of consumer characteristics.\n",
    "            T[excerpt]: ndarray, shape=(batch_size, 1)\n",
    "                Mini batch of treatment values.\n",
    "            Y[excerpt]: ndarray, shape=(batch_size, 1)\n",
    "                Mini batch of target values.\n",
    "        \"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "        indices = np.arange(num_samples)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for start_idx in range(0, num_samples, self.batch_size):\n",
    "            excerpt = indices[start_idx : start_idx + self.batch_size]\n",
    "            yield X[excerpt], T[excerpt], Y[excerpt]\n",
    "\n",
    "    def _training_the_NN(\n",
    "        self,\n",
    "        X_train,\n",
    "        T_train,\n",
    "        Y_train,\n",
    "        X_valid=None,\n",
    "        T_valid=None,\n",
    "        Y_valid=None,\n",
    "    ):\n",
    "        \"\"\"Train a NN for max_nepochs or until early stopping criterion\n",
    "        is met.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            X_train: ndarray, shape=(num_train_samples, num_features)\n",
    "                Training input data.\n",
    "            T_train: ndarray, shape=(num_train_samples, 1)\n",
    "                Training treatment array.\n",
    "            Y_train: ndarray, shape=(num_train_samples, 1)\n",
    "                Training target value array.\n",
    "            X_valid: ndarray, shape=(num_valid_samples, num_features), optional\n",
    "                Validation input data.\n",
    "            T_valid: ndarray, shape=(num_valid_samples, 1), optional\n",
    "                Validation treatment array.\n",
    "            Y_valid: ndarray, shape=(num_valid_samples, 1), optional\n",
    "                Validation target value array.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            best_loss: float\n",
    "                Minimum value of loss achieved on the validation set if\n",
    "                early stopping is enabled and validation data is provided.\n",
    "                Otherwise, loss achieved on the training set during the last epoch.\n",
    "            epoch_best: int\n",
    "                Epoch at which the minimum loss on the validation set was\n",
    "                achieved if early stopping is enabled and validation data is provided.\n",
    "                Otherwise, equal to max_nepochs for which the NN is trained.\n",
    "            output_best: ndarray\n",
    "                Output of the NN at the epoch_best.\n",
    "            total_nparameters: int\n",
    "                Number of neural network parameters.\n",
    "        \"\"\"\n",
    "        num_features = X_train.shape[1]\n",
    "        X_train.shape[0]\n",
    "\n",
    "        # Placeholders\n",
    "        x = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        dropout_rates = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "        output, loss = self._building_the_network(x, dropout_rates)\n",
    "\n",
    "        total_loss = self._calc_the_loss_with_reg(loss)\n",
    "\n",
    "        train_step = self._optimize_the_loss_function(total_loss)\n",
    "\n",
    "        sess = tf.InteractiveSession()\n",
    "        # Initializing all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        epoch_without_change = 0\n",
    "        break_cond = False\n",
    "\n",
    "        loss_train_list = []\n",
    "\n",
    "        if (\n",
    "            self.early_stopping\n",
    "            and X_valid is not None\n",
    "            and T_valid is not None\n",
    "            and Y_valid is not None\n",
    "        ):\n",
    "            loss_validation_list = []\n",
    "            validation_loss_min = float(\"inf\")\n",
    "            feed_dict_valid = {\n",
    "                x: X_valid,\n",
    "                t: T_valid,\n",
    "                y: Y_valid,\n",
    "                dropout_rates: self.dropout_rates_test,\n",
    "            }\n",
    "\n",
    "        for i in range(self.max_nepochs):\n",
    "            if (\n",
    "                self.early_stopping\n",
    "                and X_valid is not None\n",
    "                and T_valid is not None\n",
    "                and Y_valid is not None\n",
    "            ):\n",
    "                loss_valid = total_loss.eval(feed_dict=feed_dict_valid)\n",
    "                loss_validation_list.append(loss_valid)\n",
    "\n",
    "                if validation_loss_min > loss_valid:\n",
    "                    validation_loss_min = loss_valid\n",
    "                    output_best = output.eval(\n",
    "                        feed_dict={x: X_train, dropout_rates: self.dropout_rates_test},\n",
    "                    )\n",
    "                    epoch_best = i\n",
    "                    epoch_without_change = 0\n",
    "                else:\n",
    "                    epoch_without_change += 1\n",
    "\n",
    "            s = 0\n",
    "            for mini_batch in self._create_minibatches(\n",
    "                X_train,\n",
    "                T_train,\n",
    "                Y_train,\n",
    "                shuffle=True,\n",
    "            ):\n",
    "                x_batch, t_batch, y_batch = mini_batch\n",
    "                feed_dict_train = {\n",
    "                    x: x_batch,\n",
    "                    t: t_batch,\n",
    "                    y: y_batch,\n",
    "                    dropout_rates: self.dropout_rates_train,\n",
    "                }\n",
    "                loss_train = sess.run(total_loss, feed_dict=feed_dict_train)\n",
    "                if s == 0:\n",
    "                    loss_train_list.append(loss_train)\n",
    "\n",
    "                if epoch_without_change > self.max_epochs_without_change:\n",
    "                    break_cond = True\n",
    "                    break\n",
    "                sess.run(train_step, feed_dict=feed_dict_train)\n",
    "                s += 1\n",
    "\n",
    "            if self.verbose and i % 25 == 0:\n",
    "                if (\n",
    "                    self.early_stopping\n",
    "                    and X_valid is not None\n",
    "                    and T_valid is not None\n",
    "                    and Y_valid is not None\n",
    "                ):\n",
    "                    print(\"%d epoch:\" % i, \"loss on validation set:\", loss_valid)\n",
    "                else:\n",
    "                    print(\"%d epoch:\" % i, \"loss on training set:\", loss_train)\n",
    "\n",
    "            # Check the stopping condition\n",
    "            if break_cond:\n",
    "                if self.verbose:\n",
    "                    print(\"Training is finished! \", end=\"\")\n",
    "                    if (\n",
    "                        self.early_stopping\n",
    "                        and X_valid is not None\n",
    "                        and T_valid is not None\n",
    "                        and Y_valid is not None\n",
    "                    ):\n",
    "                        print(\"Best validation loss achieved at %d epoch\" % epoch_best)\n",
    "                break\n",
    "\n",
    "        if (\n",
    "            not self.early_stopping\n",
    "            or X_valid is None\n",
    "            or T_valid is None\n",
    "            or Y_valid is None\n",
    "        ):\n",
    "            output_best = output.eval(\n",
    "                feed_dict={x: X_train, dropout_rates: self.dropout_rates_test},\n",
    "            )\n",
    "            epoch_best = i + 1\n",
    "            best_loss = loss_train\n",
    "            loss_list = loss_train_list\n",
    "        else:\n",
    "            best_loss = validation_loss_min\n",
    "            loss_list = loss_validation_list\n",
    "\n",
    "        # Num of N parameters\n",
    "        total_nparameters = np.sum(\n",
    "            [\n",
    "                np.product([xi.value for xi in x.get_shape()])\n",
    "                for x in tf.trainable_variables()\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Plotting loss functions\n",
    "        if self.plot_true:\n",
    "            plotting_loss_functions(loss_train_list, loss_list)\n",
    "\n",
    "        # Close tf.InteractiveSession\n",
    "        sess.close()\n",
    "\n",
    "        return best_loss, epoch_best, output_best, total_nparameters\n",
    "\n",
    "    def training_the_NN_estimates_TE(\n",
    "        self,\n",
    "        X_train,\n",
    "        T_train,\n",
    "        Y_train,\n",
    "        X_valid=None,\n",
    "        T_valid=None,\n",
    "        Y_valid=None,\n",
    "    ):\n",
    "        \"\"\"Train a neural network that estimates treatment coefficients for\n",
    "        max_nepochs or until early stopping criterion is met.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            X_train: ndarray, shape=(num_train_samples, num_features)\n",
    "                Training input data.\n",
    "            T_train: ndarray, shape=(num_train_samples, 1)\n",
    "                Training treatment array.\n",
    "            Y_train: ndarray, shape=(num_train_samples, 1)\n",
    "                Training target value array.\n",
    "            X_valid: ndarray, shape=(num_valid_samples, num_features), optional\n",
    "                Validation input data.\n",
    "            T_valid: ndarray, shape=(num_valid_samples, 1), optional\n",
    "                Validation treatment array.\n",
    "            Y_valid: ndarray, shape=(num_valid_samples, 1), optional\n",
    "                Validation target value array.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            best_loss: float\n",
    "                Minimum value of loss achieved on the validation set if\n",
    "                early stopping is enabled and validation data is provided.\n",
    "                Otherwise, loss achieved on the training set during the last epoch.\n",
    "            epoch_best: int\n",
    "                Epoch at which the minimum loss on the validation set was\n",
    "                achieved if early stopping is enabled and validation data is provided.\n",
    "                Otherwise, equal to max_nepochs for which the NN is trained.\n",
    "            output_best: ndarray\n",
    "                Output of the NN at the epoch_best.\n",
    "            total_nparameters: int\n",
    "                Number of neural network parameters.\n",
    "        \"\"\"\n",
    "        return self._training_the_NN(\n",
    "            X_train,\n",
    "            T_train,\n",
    "            Y_train,\n",
    "            X_valid,\n",
    "            T_valid,\n",
    "            Y_valid,\n",
    "        )\n",
    "\n",
    "    def training_the_NN_estimates_PS(\n",
    "        self,\n",
    "        X_train,\n",
    "        T_train,\n",
    "        X_valid=None,\n",
    "        T_valid=None,\n",
    "    ):\n",
    "        \"\"\"Train a neural network that estimates propensity scores for\n",
    "        max_nepochs or until early stopping criterion is met.\n",
    "\n",
    "        Inputs:\n",
    "        -------\n",
    "            X_train: ndarray, shape=(num_train_samples, num_features)\n",
    "                Training input data.\n",
    "            T_train: ndarray, shape=(num_train_samples, 1)\n",
    "                Training treatment array.\n",
    "            X_valid: ndarray, shape=(num_valid_samples, num_features), optional\n",
    "                Validation input data.\n",
    "            T_valid: ndarray, shape=(num_valid_samples, 1), optional\n",
    "                Validation treatment array.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "            best_loss: float\n",
    "                Minimum value of loss achieved on the validation set if\n",
    "                early stopping is enabled and validation data is provided.\n",
    "                Otherwise, loss achieved on the training set during the last epoch.\n",
    "            epoch_best: int\n",
    "                Epoch at which the minimum loss on the validation set was\n",
    "                achieved if early stopping is enabled and validation data is provided.\n",
    "                Otherwise, equal to max_nepochs for which the NN is trained.\n",
    "            output_best: ndarray\n",
    "                Output of the NN at the epoch_best.\n",
    "            total_nparameters: int\n",
    "                Number of neural network parameters.\n",
    "        \"\"\"\n",
    "        return self._training_the_NN(X_train, T_train, X_valid=X_valid, T_valid=T_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def influence_functions(mu0_pred, tau_pred, Y, T, prob_t_pred, treatment=\"not_random\"):\n",
    "    \"\"\"Calculate the target value for each individual when treatment is\n",
    "    0 or 1.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        mu0_pred: ndarray, shape=(N, 1)\n",
    "        tau_pred: ndarray, shape=(N, 1)\n",
    "            Estimated conditional average treatment effect.\n",
    "        Y: ndarray, shape=(N,)\n",
    "            Target value array.\n",
    "        T: ndarray, shape=(N,)\n",
    "            Treatment array.\n",
    "        prob_t_pred: ndarray, shape=(N,)\n",
    "            Estimated propensity scores.\n",
    "        treatment: str, default='not_random'\n",
    "            Type of treatment. Options: 'not_random', 'random'.\n",
    "\n",
    "    Outputs:\n",
    "    -------\n",
    "        psi_0: ndarray, shape=(N, 1)\n",
    "            Influence function for given x in case of no treatment.\n",
    "        psi_1: ndarray, shape=(N, 1)\n",
    "            Influence function for given x in case of treatment.\n",
    "    \"\"\"\n",
    "    first_part = (1 - T) * (Y - mu0_pred)\n",
    "    second_part = T * (Y - mu0_pred - tau_pred)\n",
    "\n",
    "    if treatment == \"not_random\":\n",
    "        prob_t_pred = np.clip(prob_t_pred, 0.0001, 0.9999)\n",
    "        psi_0 = (first_part / (1 - prob_t_pred)) + mu0_pred\n",
    "        psi_1 = (second_part / prob_t_pred) + mu0_pred + tau_pred\n",
    "    else:\n",
    "        psi_0 = (first_part / (1 - np.mean(T))) + mu0_pred\n",
    "        psi_1 = (second_part / np.mean(T)) + mu0_pred + tau_pred\n",
    "\n",
    "    return psi_0, psi_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def update_model_comparison_file(name, model_info, cols):\n",
    "    \"\"\"Update .csv file with new model results.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        name: string\n",
    "            File name. If the file does not already exist creates a\n",
    "            new file. Otherwise appends new model results to the\n",
    "            existing file.\n",
    "        model_info: list\n",
    "            Results of the current run.\n",
    "        cols: list\n",
    "            Names of columns within the .csv file.\n",
    "            Has to be of the same length as model_info.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(name):\n",
    "        df = pd.DataFrame(columns=cols)\n",
    "        df.to_csv(name, index=False)\n",
    "        print(\"File does not exist. Creating new file!\")\n",
    "    else:\n",
    "        print(\"File already exists. Appending model run!\")\n",
    "\n",
    "    Model_comparison_Catalog_dataset = pd.read_csv(name)\n",
    "    ind = len(Model_comparison_Catalog_dataset[\"Model number\"])\n",
    "    model_info[0][0] = ind\n",
    "    df = pd.DataFrame(model_info, columns=cols)\n",
    "    Model_comparison_Catalog_dataset = Model_comparison_Catalog_dataset.append(\n",
    "        df,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    Model_comparison_Catalog_dataset.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Running Monte Carlo simulations for the following case:\")\n",
    "    print(\"* Real data\")\n",
    "    print(\"* Using the following NN architectures:\")\n",
    "    print(\"First NN hidden layer sizes: \", hidden_layer_sizes)\n",
    "    print(\"First NN hidden activations: \", activation_functions)\n",
    "    print(\"First NN dropout rates: \", dropout_rates_train, \"\\n\")\n",
    "\n",
    "    print(\"Second NN hidden layer sizes: \", hidden_layer_sizes_treatment)\n",
    "    print(\"Second NN hidden activations: \", activation_functions_treatment)\n",
    "    print(\"Second NN dropout rates: \", dropout_rates_train_treatment)\n",
    "    print(\"-------------------------------------------------------\\n\")\n",
    "\n",
    "    # Assuming you have loaded your real data into X, T_real, and Y\n",
    "    global X_train, T_train, Y_train\n",
    "    X_valid = X_train_scaled\n",
    "    T_valid = T_train\n",
    "    Y_valid = Y_train\n",
    "\n",
    "    count_in_interval = 0\n",
    "\n",
    "    # --------------------- Training the model ---------------------\n",
    "    # global X_train, T_train, Y_train\n",
    "    print(\"Shapes of X, T_real, Y:\", X.shape, T_real.shape, Y.shape)\n",
    "    X_train, X_valid, T_train, T_valid, Y_train, Y_valid = train_test_split(\n",
    "        X,\n",
    "        T_real,\n",
    "        Y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Shapes of X_train, X_valid, T_train, T_valid, Y_train, Y_valid:\",\n",
    "        X_train.shape,\n",
    "        X_valid.shape,\n",
    "        T_train.shape,\n",
    "        T_valid.shape,\n",
    "        Y_train.shape,\n",
    "        Y_valid.shape,\n",
    "    )\n",
    "\n",
    "    batch_size_ = calculate_batch_size(batch_size, X_train)\n",
    "    batch_size_t_ = calculate_batch_size(batch_size_t, X_train)\n",
    "\n",
    "    first_NN = NeuralNetwork(\n",
    "        hidden_layer_sizes,\n",
    "        activation_functions,\n",
    "        dropout_rates_train,\n",
    "        batch_size_,\n",
    "        2,\n",
    "    )\n",
    "    (\n",
    "        MSE_best,\n",
    "        epoch_best,\n",
    "        betas_pred_best,\n",
    "        total_nparameters,\n",
    "    ) = first_NN.training_the_NN_estimates_TE()\n",
    "\n",
    "    if FLAGS.treatment == \"not_random\":\n",
    "        second_NN = NeuralNetwork(\n",
    "            hidden_layer_sizes_treatment,\n",
    "            activation_functions_treatment,\n",
    "            dropout_rates_train_treatment,\n",
    "            batch_size_t_,\n",
    "            1,\n",
    "        )\n",
    "        (\n",
    "            CE_best,\n",
    "            epoch_best_t,\n",
    "            treat_best,\n",
    "            total_nparameters_t,\n",
    "        ) = second_NN.training_the_NN_estimates_PS()\n",
    "\n",
    "    # -------------------- Looking at the results ----------------\n",
    "    betas_pred = betas_pred_best\n",
    "    tau_pred = betas_pred[:, 0:1]\n",
    "    mu0_pred = betas_pred[:, 1:]\n",
    "\n",
    "    if FLAGS.treatment == \"not_random\":\n",
    "        prob_of_t_pred = 1 / (1 + np.exp(-treat_best))\n",
    "\n",
    "    np.mean(mu0_pred)\n",
    "    np.std(mu0_pred)\n",
    "    np.mean(tau_pred)\n",
    "    np.std(tau_pred)\n",
    "\n",
    "    # Calculating confidence interval for average treatment effect\n",
    "    psi_0, psi_1 = influence_functions(\n",
    "        mu0_pred,\n",
    "        tau_pred,\n",
    "        Y,\n",
    "        T_real,\n",
    "        prob_of_t_pred if FLAGS.treatment == \"not_random\" else None,\n",
    "    )\n",
    "    mean_diff_psi1_psi0 = np.mean(psi_1 - psi_0)\n",
    "    std_diff_psi1_psi0 = np.std(psi_1 - psi_0)\n",
    "    CI_upper_bound = mean_diff_psi1_psi0 + 1.96 * std_diff_psi1_psi0 / np.sqrt(len(Y))\n",
    "    CI_lower_bound = mean_diff_psi1_psi0 - 1.96 * std_diff_psi1_psi0 / np.sqrt(len(Y))\n",
    "\n",
    "    in_95_conf_int = CI_lower_bound < tau_true_mean < CI_upper_bound\n",
    "\n",
    "    print(\"is tau_true_mean in interval:\", in_95_conf_int)\n",
    "    print(\n",
    "        f\"CI lower and upper bound are: ({CI_lower_bound:0.3f}, {CI_upper_bound:0.3f})\",\n",
    "    )\n",
    "    if in_95_conf_int:\n",
    "        count_in_interval += 1\n",
    "\n",
    "    mu0_pred + tau_pred * T_real\n",
    "\n",
    "    # ----------------- Saving the results! ----------------------\n",
    "    # You can adjust this part based on how you want to save the results\n",
    "    # It seems you already have a function update_model_comparison_file\n",
    "\n",
    "    print(\n",
    "        \"%d out of %d simulations contain tau_true_mean in the CI\"\n",
    "        % (count_in_interval, FLAGS.nsimulations),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
