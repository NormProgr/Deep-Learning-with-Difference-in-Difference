{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"src/difference_in_difference_with_deep_learning/data/testdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\n",
    "    \"src/difference_in_difference_with_deep_learning/data_management/data_info.yaml\",\n",
    ") as file:\n",
    "    data_info = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_select_1 = data[data_info[\"categorical_columns\"][0]]\n",
    "data_select_2 = data[data_info[\"categorical_columns\"][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, data_info[\"causal_effect\"]] = data_select_1 * data_select_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_regression(data, data_info):\n",
    "    # Extract variables from data_info\n",
    "    outcome_variable = data_info[\"outcome\"]\n",
    "    causal_effect_variable = data_info[\"causal_effect\"]\n",
    "    categorical_columns = data_info[\"categorical_columns\"]\n",
    "    control_columns = data_info[\"control_columns\"]\n",
    "\n",
    "    # Create formula string\n",
    "    formula = f\"{outcome_variable} ~ {causal_effect_variable} + {' + '.join(categorical_columns)}  + {' + '.join(control_columns)}\"\n",
    "\n",
    "    # Set up the regression model\n",
    "    reg_model = smf.ols(formula=formula, data=data)\n",
    "\n",
    "    # Fit the regression model\n",
    "    results = reg_model.fit()\n",
    "\n",
    "    # Return the summary of the regression results\n",
    "    return results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of how to change summary output for my need (postponed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = did_regression(data, data_info)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = summary.tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r_squared_row = data0.loc[data0[0] == \"Adj. R-squared:\"]\n",
    "no_observations_row = data0.loc[data0[3] == \"No. Observations:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = summary.tables[1]\n",
    "index = [\"Intercept\", \"interaction\", \"FQ\", \"Reform\", \"Age\", \"WagePartner\"]\n",
    "summary_df = pd.DataFrame(data, index=index)\n",
    "summary_df = summary_df.round(3)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.concat(\n",
    "    [summary_df, adj_r_squared_row, no_observations_row],\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = summary_df.to_latex()\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in summary.tables:\n",
    "    print(table.as_latex_tabular())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby richtig machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_regression(data, data_info):\n",
    "    \"\"\"Estimate regression models for each time period and summarize the results.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): The dataset containing all variables.\n",
    "        data_info (dict): Dictionary containing data configuration information.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing summary statistics for each time period.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    for time_period in data[data_info[\"time\"]].unique():\n",
    "        # Filter the data for the current time period\n",
    "        data_time_period = data[data[data_info[\"time\"]] == time_period]\n",
    "\n",
    "        # Define the regression formula\n",
    "        formula = f\"{data_info['outcome']} ~ {data_info['causal_effect']} + {' + '.join(data_info['categorical_columns'] )} + {' + '.join(data_info['control_columns'])}\"\n",
    "\n",
    "        # Fit the regression model\n",
    "        reg_model = smf.ols(formula=formula, data=data_time_period)\n",
    "        results = reg_model.fit()\n",
    "\n",
    "        # Extract coefficient, std. error, and p-value\n",
    "        coefficient = results.params[data_info[\"causal_effect\"]]\n",
    "        std_error = results.bse[data_info[\"causal_effect\"]]\n",
    "        p_value = results.pvalues[data_info[\"causal_effect\"]]\n",
    "\n",
    "        # Calculate the control mean\n",
    "        control_mean = data_time_period[data_info[\"outcome\"]].mean()\n",
    "\n",
    "        # Calculate the difference between treatment and control groups\n",
    "        difference_tc = (\n",
    "            data_time_period[data_time_period[data_info[\"causal_effect\"]] == 1][\n",
    "                data_info[\"outcome\"]\n",
    "            ].mean()\n",
    "            - control_mean\n",
    "        )\n",
    "\n",
    "        # Calculate the difference with controls\n",
    "        # Assume mean of interaction term for simplicity\n",
    "        data_time_period[data_info[\"causal_effect\"]].mean()\n",
    "        difference_tc_controls = (\n",
    "            difference_tc\n",
    "            - coefficient\n",
    "            * (\n",
    "                data_time_period[data_info[\"control_columns\"]]\n",
    "                - data_time_period[data_info[\"control_columns\"]].mean()\n",
    "            )\n",
    "            .mean()\n",
    "            .sum()\n",
    "        )\n",
    "\n",
    "        # Append the results to the list\n",
    "        results_list.append(\n",
    "            {\n",
    "                \"Time Period\": time_period,\n",
    "                \"Control Mean\": control_mean,\n",
    "                \"Difference T-C\": difference_tc,\n",
    "                \"Difference T-C with Controls\": difference_tc_controls,\n",
    "                \"Coefficient\": coefficient,\n",
    "                \"Std. Error\": std_error,\n",
    "                \"P-value\": p_value,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    return pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = estimate_regression(data, data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "time_order = [\"t-2\", \"t-1\", \"t+1\", \"t+2\", \"t+3\"]\n",
    "\n",
    "# Convert Time column to categorical with defined order\n",
    "df[\"time\"] = pd.Categorical(df[\"time\"], categories=time_order, ordered=True)\n",
    "\n",
    "\n",
    "# Calculate average wage for each group over time\n",
    "grouped = df.groupby([\"FQ\", \"Reform\", \"time\"])[\"wage_year\"].mean().reset_index()\n",
    "\n",
    "# Separate data for the two groups\n",
    "group0 = grouped[(grouped[\"FQ\"] == 0) & (grouped[\"Reform\"] == 0)]\n",
    "group1 = grouped[(grouped[\"FQ\"] == 1) & (grouped[\"Reform\"] == 1)]\n",
    "\n",
    "# Calculate counterfactual average wage for each time period\n",
    "counterfactual = group0.copy()\n",
    "counterfactual[\"wage_year\"] += group1[\"wage_year\"].mean() - group0[\"wage_year\"].mean()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(group0[\"time\"], group0[\"wage_year\"], label=\"Group 0\", marker=\"o\")\n",
    "plt.plot(group1[\"time\"], group1[\"wage_year\"], label=\"Group 1\", marker=\"o\")\n",
    "plt.plot(\n",
    "    counterfactual[\"time\"],\n",
    "    counterfactual[\"wage_year\"],\n",
    "    label=\"Counterfactual\",\n",
    "    linestyle=\"--\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Average Wage\")\n",
    "plt.title(\"Difference-in-Differences Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep learning estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stopping Tensorflow from printing info messages and warnings.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_boolean(\"update\", False, \"\"\"Record the simulation results.\"\"\")\n",
    "tf.flags.DEFINE_boolean(\"plot_true\", False, \"\"\"Show plots.\"\"\")\n",
    "tf.flags.DEFINE_boolean(\"verbose\", True, \"\"\"Show detailed messages.\"\"\")\n",
    "tf.flags.DEFINE_integer(\"nsimulations\", 1, \"\"\"How many simulations to run.\"\"\")\n",
    "tf.flags.DEFINE_integer(\n",
    "    \"nconsumer_characteristics\",\n",
    "    100,\n",
    "    \"\"\"Number of consumer characteristics.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"treatment\",\n",
    "    \"not_random\",\n",
    "    \"\"\"Are customers treated at random or not.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"model\",\n",
    "    \"quadratic\",\n",
    "    \"\"\"Is the mapping from consumer characteristics\n",
    "                       to their preferences linear or quadratic.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"architecture\",\n",
    "    \"architecture_1_\",\n",
    "    \"\"\"Which NN architecture to use.\"\"\",\n",
    ")\n",
    "tf.flags.DEFINE_integer(\"data_seed\", None, \"\"\"Seed to use to create fake data.\"\"\")\n",
    "\n",
    "# Manually set sys.argv to simulate command-line invocation\n",
    "sys.argv = [sys.argv[0]]\n",
    "\n",
    "# Parse flags\n",
    "FLAGS(sys.argv)\n",
    "\n",
    "# Access remaining arguments after parsing flags\n",
    "remaining_args = [arg for arg in sys.argv[1:] if arg.startswith(\"--\")]\n",
    "assert remaining_args == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different architectures for the first NN\n",
    "if FLAGS.architecture == \"architecture_1_\":\n",
    "    hidden_layer_sizes = [20, 10, 5]\n",
    "    dropout_rates_train = [0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_2_\":\n",
    "    hidden_layer_sizes = [60, 30, 20]\n",
    "    dropout_rates_train = [0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_3_\":\n",
    "    hidden_layer_sizes = [80, 80, 80]\n",
    "    dropout_rates_train = [0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_4_\":\n",
    "    hidden_layer_sizes = [20, 15, 10, 5]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_5_\":\n",
    "    hidden_layer_sizes = [60, 30, 20, 10]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_6_\":\n",
    "    hidden_layer_sizes = [80, 80, 80, 80]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_7_\":\n",
    "    hidden_layer_sizes = [20, 15, 15, 10, 10, 5]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_8_\":\n",
    "    hidden_layer_sizes = [60, 30, 20, 20, 10, 5]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "\n",
    "elif FLAGS.architecture == \"architecture_9_\":\n",
    "    hidden_layer_sizes = [80, 80, 80, 80, 80, 80]\n",
    "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
    "    activation_functions = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"none\"]\n",
    "else:\n",
    "    msg = \"Architecture not found! Check the spelling.\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "if FLAGS.nconsumer_characteristics < 20:\n",
    "    raise ValueError(\n",
    "        \"Number of consumer characteristics \" + \"should not be less than 20.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates_test = [0 for i in dropout_rates_train]\n",
    "\n",
    "# Architecture for the second NN that estimates\n",
    "# propensity scores\n",
    "hidden_layer_sizes_treatment = [50, 30]\n",
    "activation_functions_treatment = [\"relu\", \"relu\", \"none\"]\n",
    "dropout_rates_train_treatment = [0, 0, 0]\n",
    "dropout_rates_test_treatment = [0 for i in dropout_rates_train_treatment]\n",
    "\n",
    "# Setting parameters values for generating fake data\n",
    "nconsumers = 10000\n",
    "\n",
    "# Run parameters\n",
    "train_proportion = 0.9\n",
    "max_nepochs = 5000\n",
    "max_epochs_without_change = 30\n",
    "\n",
    "early_stopping = train_proportion != 1\n",
    "\n",
    "optimizer = \"Adam\"\n",
    "learning_rate = 0.009\n",
    "batch_size = 128\n",
    "batch_size_t = None\n",
    "\n",
    "# Regularization parameters\n",
    "alpha = 0.0\n",
    "r = 0.2\n",
    "\n",
    "# Checking for spelling errors\n",
    "if not (FLAGS.model == \"quadratic\" or FLAGS.model == \"simple\"):\n",
    "    msg = \"Check whether model type is spelled correctly!\"\n",
    "    raise ValueError(msg)\n",
    "if not (FLAGS.treatment == \"random\" or FLAGS.treatment == \"not_random\"):\n",
    "    msg = \"Check whether treatment type is spelled correctly!\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train = T_train = Y_train = X_valid = T_valid = Y_valid = X = T_real = Y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_inds(t):\n",
    "    \"\"\"Split the dataset into training and validation sets while\n",
    "    preserving the proportion of targeted customers in both datasets.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        t: array-like, shape=(N, 1)\n",
    "            Treatment array.\n",
    "    Outputs:\n",
    "    -------\n",
    "        train_inds: array of bools\n",
    "            Indices of the training set.\n",
    "        valid_inds: array of bools\n",
    "            Indices of the validation set.\n",
    "    \"\"\"\n",
    "    t_array = np.array(t)\n",
    "    train_inds = np.zeros(len(t_array), dtype=bool)\n",
    "    valid_inds = np.zeros(len(t_array), dtype=bool)\n",
    "    values = np.unique(t_array)\n",
    "    for value in values:\n",
    "        value_inds = np.nonzero(t_array == value)[0]\n",
    "        np.random.shuffle(value_inds)\n",
    "        n = int(train_proportion * len(value_inds))\n",
    "        train_inds[value_inds[:n]] = True\n",
    "        valid_inds[value_inds[n:]] = True\n",
    "    return train_inds, valid_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
