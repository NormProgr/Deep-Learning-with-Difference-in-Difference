{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo System for simple 2x2 DiFnDif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATE\n",
    "\n",
    "$ Y_{it} = \\beta_0 + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\sum_{k=1}^{K} \\beta_{k+3} (\\text{X}_i = k \\times \\text{Treatment}_i) + \\epsilon_{it} $\n",
    "\n",
    "Where:\n",
    "- $ Y_{it} $ represents the outcome variable (e.g., wages) for individual $ i $ at time $ t $.\n",
    "- $ \\text{Post}_t $ is a binary variable indicating whether the observation is from the post-treatment period.\n",
    "- $ \\text{Treatment}_i $ is a binary variable indicating whether individual $ i $ is in the treatment group.\n",
    "- $ \\text{X}_i $ is a categorical variable representing a conditioning variable (e.g., education level) for individual $ i $.\n",
    "- $ K $ is the total number of levels of the conditioning variable.\n",
    "- $ \\beta_{k+3} $ represents the coefficient for the interaction between the conditioning variable level $ k $ and the treatment indicator.\n",
    "- $ \\epsilon_{it} $ is the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Estimated Treatment Effect ($ \\beta $)**:\n",
    "   The treatment effect ($ \\beta $) is the coefficient associated with the interaction term between the treatment indicator and the post-treatment period indicator. Mathematically, it is given by:\n",
    "\n",
    "   $ \\beta = \\text{Coefficient of } (\\text{Post} \\times \\text{Treatment}) $\n",
    "\n",
    "2. **Standard Error ($ SE $)**:\n",
    "   The standard error ($ SE $) of the treatment effect estimates how much the estimated treatment effect varies across different samples. It can be calculated as the square root of the variance of the coefficient estimate. \n",
    "\n",
    "3. **t-statistic ($ t $)**:\n",
    "   The t-statistic ($ t $) is a measure of the signal-to-noise ratio in the estimated treatment effect. It is calculated by dividing the estimated treatment effect by its standard error. Mathematically, it can be expressed as:\n",
    "\n",
    "   $ t = \\frac{\\beta}{SE} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data generating process fertig machen mit richtiger verteilung\n",
    "- monte carlo machen\n",
    "- simulationsergebnisse und true values vergleichen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for homogenous Treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set a seed value for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the coefficients (betas)\n",
    "true_beta_A = 2\n",
    "true_beta_B = 3\n",
    "true_beta_C = 6\n",
    "\n",
    "# Set the number of simulations\n",
    "num_simulations = 1000\n",
    "\n",
    "# Initialize lists to store estimated coefficients\n",
    "estimated_beta_A_list = []\n",
    "estimated_beta_B_list = []\n",
    "estimated_beta_C_list = []\n",
    "\n",
    "# Perform the simulations\n",
    "for _ in range(num_simulations):\n",
    "    # Generate random binary variables A, B, and C\n",
    "    A = np.random.randint(0, 2, size=num_simulations)\n",
    "    B = np.random.randint(0, 2, size=num_simulations)\n",
    "    C = A * B  # Interaction term of A and B\n",
    "\n",
    "    # Generate random control variables\n",
    "    control_1 = np.random.normal(0, 1, size=num_simulations)\n",
    "    control_2 = np.random.normal(0, 1, size=num_simulations)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=num_simulations)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    std_wage = 10  # Standard deviation of wage\n",
    "    wage = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame for the variables\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"A\": A,\n",
    "            \"B\": B,\n",
    "            \"C\": C,\n",
    "            \"Control_1\": control_1,\n",
    "            \"Control_2\": control_2,\n",
    "            \"Wage\": wage,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Create the model\n",
    "    X = sm.add_constant(data[[\"A\", \"B\", \"C\", \"Control_1\", \"Control_2\"]])\n",
    "    y = data[\"Wage\"]\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    # Extract the estimated coefficients and append to the lists\n",
    "    estimated_beta_A_list.append(results.params[\"A\"])\n",
    "    estimated_beta_B_list.append(results.params[\"B\"])\n",
    "    estimated_beta_C_list.append(results.params[\"C\"])\n",
    "\n",
    "# Calculate the average estimated coefficients\n",
    "average_estimated_beta_A = np.mean(estimated_beta_A_list)\n",
    "average_estimated_beta_B = np.mean(estimated_beta_B_list)\n",
    "average_estimated_beta_C = np.mean(estimated_beta_C_list)\n",
    "\n",
    "# Print the true coefficients and the average estimated coefficients\n",
    "print(\"True Coefficients:\")\n",
    "print(\"Beta_A:\", true_beta_A)\n",
    "print(\"Beta_B:\", true_beta_B)\n",
    "print(\"Beta_C:\", true_beta_C)\n",
    "print(\"\\nAverage Estimated Coefficients:\")\n",
    "print(\"Beta_A (Average Estimated):\", average_estimated_beta_A)\n",
    "print(\"Beta_B (Average Estimated):\", average_estimated_beta_B)\n",
    "print(\"Beta_C (Average Estimated):\", average_estimated_beta_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine the estimated coefficients into a single DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": estimated_beta_A_list,\n",
    "        \"B\": estimated_beta_B_list,\n",
    "        \"C\": estimated_beta_C_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Plot kernel density estimates for each coefficient\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=df, fill=True, palette=\"Set1\")\n",
    "plt.axvline(x=true_beta_A, color=\"red\", linestyle=\"--\", label=\"True Beta A\")\n",
    "plt.axvline(x=true_beta_B, color=\"blue\", linestyle=\"--\", label=\"True Beta B\")\n",
    "plt.axvline(x=true_beta_C, color=\"green\", linestyle=\"--\", label=\"True Beta C\")\n",
    "plt.title(\"Kernel Density Plot of Estimated Coefficients\")\n",
    "plt.xlabel(\"Estimated Coefficient Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for hetergenous Treatment effects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set a seed value for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the true coefficients\n",
    "true_beta_A = 2\n",
    "true_beta_B = 3\n",
    "true_beta_C = 6\n",
    "true_beta_A_X = -20  # Interaction effect of A and covariate X\n",
    "true_beta_B_X = 40  # Interaction effect of B and covariate X\n",
    "\n",
    "# Set the number of simulations\n",
    "num_simulations = 1000\n",
    "\n",
    "# Initialize lists to store estimated coefficients for both models\n",
    "estimated_beta_homogeneous_list = []\n",
    "estimated_beta_heterogeneous_list = []\n",
    "\n",
    "# Perform the simulations\n",
    "for _ in range(num_simulations):\n",
    "    # Generate random binary variables A, B, and C\n",
    "    A = np.random.randint(0, 2, size=num_simulations)\n",
    "    B = np.random.randint(0, 2, size=num_simulations)\n",
    "    C = np.random.randint(0, 2, size=num_simulations)\n",
    "\n",
    "    # Generate random covariate X\n",
    "    X = np.random.normal(0, 5, size=num_simulations)\n",
    "\n",
    "    # Generate random control variables\n",
    "    control_1 = np.random.normal(0, 1, size=num_simulations)\n",
    "    control_2 = np.random.normal(0, 1, size=num_simulations)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=num_simulations)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    std_wage = 10  # Standard deviation of wage\n",
    "    wage = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + true_beta_A_X * A * X\n",
    "        + true_beta_B_X * B * X\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame for the variables\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"A\": A,\n",
    "            \"B\": B,\n",
    "            \"C\": C,\n",
    "            \"X\": X,\n",
    "            \"Control_1\": control_1,\n",
    "            \"Control_2\": control_2,\n",
    "            \"Wage\": wage,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Create the homogenous treatment effect model\n",
    "    X_homogeneous = sm.add_constant(\n",
    "        data[[\"A\", \"B\", \"C\", \"X\", \"Control_1\", \"Control_2\"]],\n",
    "    )\n",
    "    y_homogeneous = data[\"Wage\"]\n",
    "    model_homogeneous = sm.OLS(y_homogeneous, X_homogeneous)\n",
    "    results_homogeneous = model_homogeneous.fit()\n",
    "    estimated_beta_homogeneous_list.append(results_homogeneous.params[\"C\"])\n",
    "\n",
    "    # Create the heterogeneous treatment effect model\n",
    "    data[\"A_X\"] = data[\"A\"] * data[\"X\"]\n",
    "    data[\"B_X\"] = data[\"B\"] * data[\"X\"]\n",
    "    X_heterogeneous = sm.add_constant(\n",
    "        data[[\"A\", \"B\", \"C\", \"X\", \"A_X\", \"B_X\", \"Control_1\", \"Control_2\"]],\n",
    "    )\n",
    "    y_heterogeneous = data[\"Wage\"]\n",
    "    model_heterogeneous = sm.OLS(y_heterogeneous, X_heterogeneous)\n",
    "    results_heterogeneous = model_heterogeneous.fit()\n",
    "    estimated_beta_heterogeneous_list.append(results_heterogeneous.params[\"C\"])\n",
    "\n",
    "# Calculate the average estimated coefficients for both models\n",
    "average_estimated_beta_homogeneous = np.mean(estimated_beta_homogeneous_list)\n",
    "average_estimated_beta_heterogeneous = np.mean(estimated_beta_heterogeneous_list)\n",
    "\n",
    "# Print the true coefficient and the average estimated coefficients for both models\n",
    "print(\"True Coefficient (Heterogeneous Treatment Effect):\", true_beta_C)\n",
    "print(\n",
    "    \"Average Estimated Coefficient (Homogeneous Treatment Effect Model):\",\n",
    "    average_estimated_beta_homogeneous,\n",
    ")\n",
    "print(\n",
    "    \"Average Estimated Coefficient (Heterogeneous Treatment Effect Model):\",\n",
    "    average_estimated_beta_heterogeneous,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define custom colors for coefficients\n",
    "homogeneous_color = \"blue\"\n",
    "heterogeneous_color = \"orange\"\n",
    "\n",
    "# Combine the estimated coefficients into DataFrames\n",
    "homogeneous_df = pd.DataFrame(\n",
    "    {\"Coefficient\": estimated_beta_homogeneous_list, \"Model\": \"Homogeneous\"},\n",
    ")\n",
    "heterogeneous_df = pd.DataFrame(\n",
    "    {\"Coefficient\": estimated_beta_heterogeneous_list, \"Model\": \"Heterogeneous\"},\n",
    ")\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df = pd.concat([homogeneous_df, heterogeneous_df])\n",
    "\n",
    "# Plot kernel density estimates for each coefficient\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(\n",
    "    data=df,\n",
    "    x=\"Coefficient\",\n",
    "    hue=\"Model\",\n",
    "    fill=True,\n",
    "    palette={\"Homogeneous\": homogeneous_color, \"Heterogeneous\": heterogeneous_color},\n",
    ")\n",
    "plt.axvline(x=true_beta_C, color=\"green\", linestyle=\"--\", label=\"True Beta C\")\n",
    "plt.title(\"Kernel Density Plot of Estimated Coefficients\")\n",
    "plt.xlabel(\"Estimated Coefficient Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 60)           660         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 60)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           1830        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            62          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3)            0           ['dense_2[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,552\n",
      "Trainable params: 2,552\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norma\\.conda\\envs\\causal_net\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of the causal coefficients neural network:\n",
      "1 epoch - loss: 2.5663  val loss: 2.2654\n",
      "26 epoch - loss: 1.3224  val loss: 1.3331\n",
      "51 epoch - loss: 1.2147  val loss: 1.1992\n",
      "76 epoch - loss: 1.1504  val loss: 1.1360\n",
      "101 epoch - loss: 1.0953  val loss: 1.0788\n",
      "126 epoch - loss: 1.0587  val loss: 1.0437\n",
      "151 epoch - loss: 1.0332  val loss: 1.0208\n",
      "176 epoch - loss: 1.0138  val loss: 1.0056\n",
      "201 epoch - loss: 1.0029  val loss: 0.9965\n",
      "226 epoch - loss: 0.9980  val loss: 0.9941\n",
      "251 epoch - loss: 0.9951  val loss: 0.9942\n",
      "276 epoch - loss: 0.9931  val loss: 0.9936\n",
      "301 epoch - loss: 0.9915  val loss: 0.9933\n",
      "326 epoch - loss: 0.9901  val loss: 0.9923\n",
      "351 epoch - loss: 0.9884  val loss: 0.9921\n",
      "Training is finished.\n",
      "\n",
      "313/313 [==============================] - 0s 987us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from causal_nets import causal_net_estimate\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION = \"python\"  # add this\n",
    "from causal_nets import causal_net_estimate\n",
    "\n",
    "# Setting the seeds\n",
    "np.random.seed(3)\n",
    "\n",
    "# Generating the fake data\n",
    "N = 10000\n",
    "X = np.random.uniform(low=0, high=1, size=[N, 10])\n",
    "mu0_real = (\n",
    "    1.5\n",
    "    + 0.012 * X[:, 3]\n",
    "    - 0.75 * X[:, 5] * X[:, 7]\n",
    "    - 0.9 * X[:, 4]\n",
    "    - np.mean(X, axis=1)\n",
    ")\n",
    "tau_real = X[:, 2] + 0.04 * X[:, 9] - 0.35 * np.log(X[:, 3])\n",
    "prob_of_T = 0.5\n",
    "T = np.random.binomial(size=N, n=1, p=prob_of_T)\n",
    "normal_errors = np.random.normal(\n",
    "    size=[\n",
    "        N,\n",
    "    ],\n",
    "    loc=0.0,\n",
    "    scale=1.0,\n",
    ")\n",
    "Y = mu0_real + tau_real * T + normal_errors\n",
    "\n",
    "# Creating training and validation dataset\n",
    "X_train, X_valid, T_train, T_valid, Y_train, Y_valid = train_test_split(\n",
    "    X,\n",
    "    T,\n",
    "    Y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "# Getting causal estimates\n",
    "(\n",
    "    tau_pred,\n",
    "    mu0_pred,\n",
    "    prob_t_pred,\n",
    "    psi_0,\n",
    "    psi_1,\n",
    "    history,\n",
    "    history_ps,\n",
    ") = causal_net_estimate(\n",
    "    [X_train, T_train, Y_train],\n",
    "    [X_valid, T_valid, Y_valid],\n",
    "    [X, T, Y],\n",
    "    [60, 30],\n",
    "    dropout_rates=None,\n",
    "    batch_size=None,\n",
    "    alpha=0.0,\n",
    "    r_par=0.2,\n",
    "    optimizer=\"Adam\",\n",
    "    learning_rate=0.0009,\n",
    "    max_epochs_without_change=30,\n",
    "    max_nepochs=5000,\n",
    "    seed=None,\n",
    "    estimate_ps=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Plotting estimated coefficient vs true coefficients\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.clf()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "bins = np.linspace(\n",
    "    min(float(min(tau_pred)), float(min(tau_real))),\n",
    "    max(float(max(tau_pred)), float(max(tau_real))),\n",
    "    15,\n",
    ")\n",
    "plt.hist(tau_pred, alpha=0.6, label=r\"$\\tau~_{pred}$\", density=True, bins=bins)\n",
    "plt.hist(\n",
    "    tau_real,\n",
    "    label=r\"$\\tau~_{ real}$\",\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    linewidth=2.5,\n",
    "    bins=bins,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"CATE(Conditional average treatment effect)\")\n",
    "plt.xlabel(r\"$\\tau$\", fontsize=14)\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "bins = np.linspace(\n",
    "    min(float(min(mu0_pred)), float(min(mu0_real))),\n",
    "    max(float(max(mu0_pred)), float(max(mu0_real))),\n",
    "    15,\n",
    ")\n",
    "plt.hist(mu0_pred, alpha=0.7, label=r\"$\\mu_{0~pred}$\", density=True, bins=bins)\n",
    "plt.hist(\n",
    "    mu0_real,\n",
    "    label=r\"$\\mu_{0~real}$\",\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    linewidth=2.5,\n",
    "    bins=bins,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(r\"$\\mu_0(x)$\")\n",
    "plt.xlabel(r\"$\\mu_0$\", fontsize=14)\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the average treatment effect\n",
    "ate = np.mean(psi_1 - psi_0)\n",
    "\n",
    "# Calculate the 95% confidence interval for average treatment effect\n",
    "CI_lowerbound = ate - norm.ppf(0.975) * np.std(psi_1 - psi_0) / np.sqrt(len(psi_0))\n",
    "CI_upperbound = ate + norm.ppf(0.975) * np.std(psi_1 - psi_0) / np.sqrt(len(psi_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_lowerbound = ate - norm.ppf(0.975) * np.std(psi_1 - psi_0) / np.sqrt(len(psi_0))\n",
    "CI_upperbound = ate + norm.ppf(0.975) * np.std(psi_1 - psi_0) / np.sqrt(len(psi_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip install tensoflow==2.10.0\n",
    "- pip install protobuf==3.11.3\n",
    "- pip uninstall protobuf\n",
    "- conda install protobuf\n",
    "- pip install \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for Deep Learning and propensity scores - self made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Run Monte Carlo simulation\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m true_values, estimated_values \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Calculate average absolute errors\u001b[39;00m\n\u001b[0;32m    163\u001b[0m errors_A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39marray(true_values[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(estimated_values[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[3], line 122\u001b[0m, in \u001b[0;36mmonte_carlo_simulation\u001b[1;34m(num_simulations, N, num_features, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m    108\u001b[0m (\n\u001b[0;32m    109\u001b[0m     X,\n\u001b[0;32m    110\u001b[0m     treatment_assignment,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     true_beta_C,\n\u001b[0;32m    115\u001b[0m ) \u001b[38;5;241m=\u001b[39m generate_data(N, num_features)\n\u001b[0;32m    116\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m    117\u001b[0m     X,\n\u001b[0;32m    118\u001b[0m     y,\n\u001b[0;32m    119\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m    120\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m    121\u001b[0m )\n\u001b[1;32m--> 122\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m true_beta_A_list\u001b[38;5;241m.\u001b[39mappend(true_beta_A)\n\u001b[0;32m    131\u001b[0m true_beta_B_list\u001b[38;5;241m.\u001b[39mappend(true_beta_B)\n",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m, in \u001b[0;36mtrain_nn\u001b[1;34m(X_train, y_train, num_features, epochs, batch_size, learning_rate, l2_penalty)\u001b[0m\n\u001b[0;32m     77\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     78\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\norma\\.conda\\envs\\causal_net\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\norma\\.conda\\envs\\causal_net\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to generate synthetic data\n",
    "\n",
    "\n",
    "def generate_data(N, num_features):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(N, num_features)\n",
    "    A = np.random.randint(0, 2, size=N)\n",
    "    B = np.random.randint(0, 2, size=N)\n",
    "    C = A * B  # Interaction term of A and B\n",
    "    true_beta_A = 2\n",
    "    true_beta_B = 3\n",
    "    true_beta_C = 6\n",
    "\n",
    "    control_1 = np.random.normal(0, 1, size=N)\n",
    "    control_2 = np.random.normal(0, 1, size=N)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=N)\n",
    "\n",
    "    propensity_scores = 1 / (1 + np.exp(-C))\n",
    "    treatment_assignment = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    y = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    return X, treatment_assignment, y, true_beta_A, true_beta_B, true_beta_C\n",
    "\n",
    "\n",
    "# Function to train the neural network\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "def train_nn(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    l2_penalty=0.01,\n",
    "):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                input_shape=(num_features,),\n",
    "                kernel_regularizer=regularizers.l2(l2_penalty),\n",
    "            ),\n",
    "            Dropout(0.2),\n",
    "            Dense(\n",
    "                32,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(l2_penalty),\n",
    "            ),\n",
    "            Dropout(0.2),\n",
    "            Dense(1),\n",
    "        ],\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to estimate coefficients\n",
    "\n",
    "\n",
    "def estimate_coefficients(model):\n",
    "    return model.layers[-1].get_weights()[0].flatten()\n",
    "\n",
    "\n",
    "# Function for Monte Carlo simulation\n",
    "\n",
    "\n",
    "def monte_carlo_simulation(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "):\n",
    "    true_beta_A_list = []\n",
    "    true_beta_B_list = []\n",
    "    true_beta_C_list = []\n",
    "    estimated_beta_A_list = []\n",
    "    estimated_beta_B_list = []\n",
    "    estimated_beta_C_list = []\n",
    "    for _ in range(num_simulations):\n",
    "        (\n",
    "            X,\n",
    "            treatment_assignment,\n",
    "            y,\n",
    "            true_beta_A,\n",
    "            true_beta_B,\n",
    "            true_beta_C,\n",
    "        ) = generate_data(N, num_features)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model = train_nn(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            num_features,\n",
    "            epochs,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "        )\n",
    "        true_beta_A_list.append(true_beta_A)\n",
    "        true_beta_B_list.append(true_beta_B)\n",
    "        true_beta_C_list.append(true_beta_C)\n",
    "        estimated_coefficients = estimate_coefficients(model)\n",
    "        estimated_beta_A_list.append(estimated_coefficients[0])\n",
    "        estimated_beta_B_list.append(estimated_coefficients[1])\n",
    "        estimated_beta_C_list.append(estimated_coefficients[2])\n",
    "    return (true_beta_A_list, true_beta_B_list, true_beta_C_list), (\n",
    "        estimated_beta_A_list,\n",
    "        estimated_beta_B_list,\n",
    "        estimated_beta_C_list,\n",
    "    )\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_simulations = 10  # Number of simulations\n",
    "N = 1000  # Number of samples\n",
    "num_features = 5  # Number of features\n",
    "epochs = 5000  # Number of epochs\n",
    "batch_size = 32  # Batch size\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Run Monte Carlo simulation\n",
    "true_values, estimated_values = monte_carlo_simulation(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    ")\n",
    "\n",
    "# Calculate average absolute errors\n",
    "errors_A = np.abs(np.array(true_values[0]) - np.array(estimated_values[0]))\n",
    "errors_B = np.abs(np.array(true_values[1]) - np.array(estimated_values[1]))\n",
    "errors_C = np.abs(np.array(true_values[2]) - np.array(estimated_values[2]))\n",
    "\n",
    "# Calculate average estimated coefficients\n",
    "avg_estimated_beta_A = np.mean(estimated_values[0])\n",
    "avg_estimated_beta_B = np.mean(estimated_values[1])\n",
    "avg_estimated_beta_C = np.mean(estimated_values[2])\n",
    "\n",
    "print(\"Average Absolute Error for Beta_A:\", np.mean(errors_A))\n",
    "print(\"Average Absolute Error for Beta_B:\", np.mean(errors_B))\n",
    "print(\"Average Absolute Error for Beta_C:\", np.mean(errors_C))\n",
    "\n",
    "print(\"\\nAverage Estimated Coefficients:\")\n",
    "print(\"Average Beta_A:\", avg_estimated_beta_A)\n",
    "print(\"Average Beta_B:\", avg_estimated_beta_B)\n",
    "print(\"Average Beta_C:\", avg_estimated_beta_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract true and estimated beta values\n",
    "true_beta_A_list, true_beta_B_list, true_beta_C_list = true_values\n",
    "estimated_beta_A_list, estimated_beta_B_list, estimated_beta_C_list = estimated_values\n",
    "\n",
    "# Create kernel density plot for beta coefficients\n",
    "sns.kdeplot(true_beta_A_list, label=\"True Beta A\", fill=True, color=\"blue\")\n",
    "sns.kdeplot(true_beta_B_list, label=\"True Beta B\", fill=True, color=\"orange\")\n",
    "sns.kdeplot(true_beta_C_list, label=\"True Beta C\", fill=True, color=\"green\")\n",
    "sns.kdeplot(estimated_beta_A_list, label=\"Estimated Beta A\", fill=True)\n",
    "sns.kdeplot(estimated_beta_B_list, label=\"Estimated Beta B\", fill=True)\n",
    "sns.kdeplot(estimated_beta_C_list, label=\"Estimated Beta C\", fill=True)\n",
    "\n",
    "# Add vertical lines for true coefficients\n",
    "plt.axvline(\n",
    "    np.mean(true_beta_A_list),\n",
    "    color=\"blue\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean True Beta A\",\n",
    ")\n",
    "plt.axvline(\n",
    "    np.mean(true_beta_B_list),\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean True Beta B\",\n",
    ")\n",
    "plt.axvline(\n",
    "    np.mean(true_beta_C_list),\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean True Beta C\",\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Beta Coefficients\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Kernel Density Plot of Beta Coefficients\")\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# propensity scores approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_propensity_scores(N, num_features):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(N, num_features)\n",
    "    A = np.random.randint(0, 2, size=N)\n",
    "    B = np.random.randint(0, 2, size=N)\n",
    "    C = A * B  # Interaction term of A and B\n",
    "    true_beta_A = 2\n",
    "    true_beta_B = 3\n",
    "    true_beta_C = 6\n",
    "\n",
    "    control_1 = np.random.normal(0, 1, size=N)\n",
    "    control_2 = np.random.normal(0, 1, size=N)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=N)\n",
    "\n",
    "    propensity_scores = 1 / (1 + np.exp(-C))\n",
    "    treatment_assignment = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    y = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    # Include propensity scores as features\n",
    "    X = np.column_stack((X, propensity_scores))\n",
    "\n",
    "    return X, treatment_assignment, y, true_beta_A, true_beta_B, true_beta_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_with_propensity_scores(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                input_shape=(num_features,),\n",
    "            ),  # Input shape adjusted\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(1),\n",
    "        ],\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_simulation_with_propensity_scores(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "):\n",
    "    true_beta_A_list = []\n",
    "    true_beta_B_list = []\n",
    "    true_beta_C_list = []\n",
    "    estimated_beta_A_list = []\n",
    "    estimated_beta_B_list = []\n",
    "    estimated_beta_C_list = []\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        (\n",
    "            X,\n",
    "            treatment_assignment,\n",
    "            y,\n",
    "            true_beta_A,\n",
    "            true_beta_B,\n",
    "            true_beta_C,\n",
    "        ) = generate_data_with_propensity_scores(N, num_features)\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Train neural network model\n",
    "        model = train_nn_with_propensity_scores(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            num_features + 1,  # Add 1 for propensity score feature\n",
    "            epochs,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "        )\n",
    "\n",
    "        # Compute true and estimated coefficients\n",
    "        true_beta_A_list.append(true_beta_A)\n",
    "        true_beta_B_list.append(true_beta_B)\n",
    "        true_beta_C_list.append(true_beta_C)\n",
    "        estimated_coefficients = estimate_coefficients(model)\n",
    "        estimated_beta_A_list.append(estimated_coefficients[0])\n",
    "        estimated_beta_B_list.append(estimated_coefficients[1])\n",
    "        estimated_beta_C_list.append(estimated_coefficients[2])\n",
    "\n",
    "    return (true_beta_A_list, true_beta_B_list, true_beta_C_list), (\n",
    "        estimated_beta_A_list,\n",
    "        estimated_beta_B_list,\n",
    "        estimated_beta_C_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_simulations = 100  # Number of simulations\n",
    "N = 1000  # Number of samples\n",
    "num_features = 5  # Number of features\n",
    "epochs = 50  # Number of epochs\n",
    "batch_size = 32  # Batch size\n",
    "learning_rate = 0.001  # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_coefficients(model):\n",
    "    return model.layers[-1].get_weights()[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulation with propensity scores\n",
    "(\n",
    "    true_values_propensity,\n",
    "    estimated_values_propensity,\n",
    ") = monte_carlo_simulation_with_propensity_scores(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    ")\n",
    "\n",
    "# Calculate average absolute errors\n",
    "errors_A_propensity = np.abs(\n",
    "    np.array(true_values_propensity[0]) - np.array(estimated_values_propensity[0]),\n",
    ")\n",
    "errors_B_propensity = np.abs(\n",
    "    np.array(true_values_propensity[1]) - np.array(estimated_values_propensity[1]),\n",
    ")\n",
    "errors_C_propensity = np.abs(\n",
    "    np.array(true_values_propensity[2]) - np.array(estimated_values_propensity[2]),\n",
    ")\n",
    "\n",
    "# Calculate average estimated coefficients\n",
    "avg_estimated_beta_A_propensity = np.mean(estimated_values_propensity[0])\n",
    "avg_estimated_beta_B_propensity = np.mean(estimated_values_propensity[1])\n",
    "avg_estimated_beta_C_propensity = np.mean(estimated_values_propensity[2])\n",
    "\n",
    "print(\n",
    "    \"Average Absolute Error for Beta_A with Propensity Scores:\",\n",
    "    np.mean(errors_A_propensity),\n",
    ")\n",
    "print(\n",
    "    \"Average Absolute Error for Beta_B with Propensity Scores:\",\n",
    "    np.mean(errors_B_propensity),\n",
    ")\n",
    "print(\n",
    "    \"Average Absolute Error for Beta_C with Propensity Scores:\",\n",
    "    np.mean(errors_C_propensity),\n",
    ")\n",
    "\n",
    "print(\"\\nAverage Estimated Coefficients with Propensity Scores:\")\n",
    "print(\"Average Beta_A:\", avg_estimated_beta_A_propensity)\n",
    "print(\"Average Beta_B:\", avg_estimated_beta_B_propensity)\n",
    "print(\"Average Beta_C:\", avg_estimated_beta_C_propensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract estimated beta values with propensity scores\n",
    "(\n",
    "    estimated_beta_A_list_propensity,\n",
    "    estimated_beta_B_list_propensity,\n",
    "    estimated_beta_C_list_propensity,\n",
    ") = estimated_values_propensity\n",
    "\n",
    "# Create kernel density plot for estimated beta coefficients with propensity scores\n",
    "sns.kdeplot(\n",
    "    estimated_beta_A_list_propensity,\n",
    "    label=\"Estimated Beta A with Propensity Scores\",\n",
    "    shade=True,\n",
    ")\n",
    "sns.kdeplot(\n",
    "    estimated_beta_B_list_propensity,\n",
    "    label=\"Estimated Beta B with Propensity Scores\",\n",
    "    shade=True,\n",
    ")\n",
    "sns.kdeplot(\n",
    "    estimated_beta_C_list_propensity,\n",
    "    label=\"Estimated Beta C with Propensity Scores\",\n",
    "    shade=True,\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Beta Coefficients\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Kernel Density Plot of Estimated Beta Coefficients with Propensity Scores\")\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last try for today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to generate synthetic data\n",
    "\n",
    "\n",
    "def generate_data(N, num_features):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(N, num_features)\n",
    "    A = np.random.randint(0, 2, size=N)\n",
    "    B = np.random.randint(0, 2, size=N)\n",
    "    C = A * B  # Interaction term of A and B\n",
    "    true_beta_A = 2\n",
    "    true_beta_B = 3\n",
    "    true_beta_C = 6\n",
    "\n",
    "    control_1 = np.random.normal(0, 1, size=N)\n",
    "    control_2 = np.random.normal(0, 1, size=N)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=N)\n",
    "\n",
    "    propensity_scores = 1 / (1 + np.exp(-C))\n",
    "    treatment_assignment = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    y = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    return X, treatment_assignment, y, true_beta_A, true_beta_B, true_beta_C\n",
    "\n",
    "\n",
    "# Function to train the neural network\n",
    "\n",
    "\n",
    "def train_nn(X, y, num_features, epochs, batch_size, learning_rate):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(64, activation=\"relu\", input_shape=(num_features,)),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(1),\n",
    "        ],\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to estimate treatment effects using DID\n",
    "\n",
    "\n",
    "def estimate_did(X, treatment_assignment, y, model):\n",
    "    # Predict outcomes using the trained neural network\n",
    "    predicted_outcomes = model.predict(X).flatten()\n",
    "\n",
    "    # Combine observed outcomes and predicted outcomes\n",
    "    df = pd.DataFrame(\n",
    "        {\"Y\": y, \"Y_predicted\": predicted_outcomes, \"Treatment\": treatment_assignment},\n",
    "    )\n",
    "\n",
    "    # Compute differences in differences\n",
    "    control_group = df[df[\"Treatment\"] == 0]\n",
    "    treatment_group = df[df[\"Treatment\"] == 1]\n",
    "    pre_period = control_group[control_group.index < len(control_group) // 2]\n",
    "    post_period = control_group[control_group.index >= len(control_group) // 2]\n",
    "\n",
    "    control_diff = post_period[\"Y\"].mean() - pre_period[\"Y\"].mean()\n",
    "    return (\n",
    "        treatment_group[\"Y_predicted\"].mean() - treatment_group[\"Y\"].mean()\n",
    "    ) - control_diff\n",
    "\n",
    "\n",
    "# Function for Monte Carlo simulation\n",
    "\n",
    "\n",
    "def monte_carlo_simulation(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "):\n",
    "    treatment_diffs = []\n",
    "    for _ in range(num_simulations):\n",
    "        (\n",
    "            X,\n",
    "            treatment_assignment,\n",
    "            y,\n",
    "            true_beta_A,\n",
    "            true_beta_B,\n",
    "            true_beta_C,\n",
    "        ) = generate_data(N, num_features)\n",
    "        model = train_nn(\n",
    "            X,\n",
    "            y,\n",
    "            num_features,\n",
    "            epochs,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "        )\n",
    "        treatment_diff = estimate_did(X, treatment_assignment, y, model)\n",
    "        treatment_diffs.append(treatment_diff)\n",
    "    return treatment_diffs\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_simulations = 100  # Number of simulations\n",
    "N = 1000  # Number of samples\n",
    "num_features = 5  # Number of features\n",
    "epochs = 50  # Number of epochs\n",
    "batch_size = 32  # Batch size\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Run Monte Carlo simulation\n",
    "treatment_diffs = monte_carlo_simulation(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    ")\n",
    "\n",
    "# Calculate average treatment effect\n",
    "avg_treatment_diff = np.mean(treatment_diffs)\n",
    "print(\"Average treatment effect estimated using DID:\", avg_treatment_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# True treatment effect\n",
    "true_effect = np.mean(treatment_diffs)\n",
    "\n",
    "# Plot histogram of treatment effects\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(\n",
    "    treatment_diffs,\n",
    "    bins=20,\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Estimated Treatment Effect\",\n",
    ")\n",
    "plt.axvline(x=true_effect, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect\")\n",
    "plt.title(\"Distribution of Treatment Effects Estimated using DID\")\n",
    "plt.xlabel(\"Treatment Effect\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for Deep Learning and propensity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_polynomial_X_times_weights(X, nconsumer_characteristics, weights):\n",
    "    \"\"\"Evaluate the non-linear part of a quadratic polynomial in\n",
    "    consumer characteristics, X, with prescribed weights.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        X: ndarray, shape = (N, nconsumer_characteristics)\n",
    "            Feature matrix containing consumer characteristics.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "        weights: ndarray, shape = (num_additional_poly_terms, )\n",
    "            Weights corresponding to quadratic terms.\n",
    "    Outputs:\n",
    "    -------\n",
    "        sum_x: ndarray, shape = (N, 1)\n",
    "            Non-linear part of the quadratic polynomial evaluated\n",
    "            for each consumer.\n",
    "    \"\"\"\n",
    "    from itertools import combinations_with_replacement\n",
    "\n",
    "    my_polynomial_indices = combinations_with_replacement(\n",
    "        list(range(nconsumer_characteristics)),\n",
    "        2,\n",
    "    )\n",
    "    sum_x = 0\n",
    "    for i, p in enumerate(my_polynomial_indices):\n",
    "        sum_x = sum_x + weights[i] * np.multiply(X[:, p[0]], X[:, p[1]])\n",
    "    return sum_x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have X, nconsumer_characteristics, and weights defined somewhere\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "N = 1000  # Number of consumers\n",
    "nconsumer_characteristics = 100  # Number of consumer characteristics\n",
    "X = np.random.rand(N, nconsumer_characteristics)  # Feature matrix\n",
    "weights = np.random.rand(\n",
    "    nconsumer_characteristics * (nconsumer_characteristics + 1) // 2,\n",
    ")  # Random weights\n",
    "# Call the function\n",
    "result = sum_polynomial_X_times_weights(X, nconsumer_characteristics, weights)\n",
    "\n",
    "# Print the result\n",
    "print(\"Result shape:\", result.shape)\n",
    "print(\"Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_TE_coefs(X, nconsumer_characteristics, model=\"quadratic\"):\n",
    "    \"\"\"Create treatment effect coefficients.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        X: ndarray, shape = (N, nconsumer_characteristics)\n",
    "            Feature matrix containing consumer characteristics.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "        model: str, optional\n",
    "            Type of model to use for coefficients. Default is 'simple'.\n",
    "            Options are {'simple', 'quadratic'}.\n",
    "\n",
    "    Outputs:\n",
    "    -------\n",
    "        bias_tau: float\n",
    "            Constant term in equation for tau.\n",
    "        alpha_tau: ndarray, shape = [nconsumer_characteristics]\n",
    "            Linear coefficients in equation for tau.\n",
    "        beta_tau: ndarray or None\n",
    "            Quadratic coefficients in equation for tau.\n",
    "            If model is 'quadratic', returns coefficients.\n",
    "            Otherwise, returns None.\n",
    "    \"\"\"\n",
    "    np.random.seed(63)\n",
    "\n",
    "    # Calculating tau\n",
    "    alpha_tau = np.random.uniform(low=0.1, high=0.22, size=nconsumer_characteristics)\n",
    "    bias_tau = -0.05\n",
    "    tau = np.dot(X, alpha_tau) + bias_tau\n",
    "\n",
    "    if model == \"quadratic\":\n",
    "        count = nconsumer_characteristics * (nconsumer_characteristics + 1) // 2\n",
    "        beta_tau = np.random.uniform(low=-0.05, high=0.06, size=count)\n",
    "        tau = tau + sum_polynomial_X_times_weights(\n",
    "            X,\n",
    "            nconsumer_characteristics,\n",
    "            beta_tau,\n",
    "        )\n",
    "    else:\n",
    "        beta_tau = None\n",
    "\n",
    "    # Calculating mu0\n",
    "    alpha_mu0 = np.random.normal(loc=0.3, scale=0.7, size=nconsumer_characteristics)\n",
    "    bias_mu0 = 0.09\n",
    "    mu0 = np.dot(X, alpha_mu0) + bias_mu0\n",
    "\n",
    "    if model == \"quadratic\":\n",
    "        beta_mu0 = np.random.normal(loc=0.01, scale=0.3, size=count)\n",
    "        mu0 = mu0 + sum_polynomial_X_times_weights(\n",
    "            X,\n",
    "            nconsumer_characteristics,\n",
    "            beta_mu0,\n",
    "        )\n",
    "\n",
    "    return bias_tau, alpha_tau, beta_tau, tau, mu0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have nconsumer_characteristics defined somewhere\n",
    "# Assuming you have X defined somewhere\n",
    "# Call the function with 'simple' model\n",
    "bias_tau, alpha_tau, beta_tau, tau, mu0 = create_TE_coefs(\n",
    "    X,\n",
    "    nconsumer_characteristics,\n",
    "    model=\"quadtratic\",\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Bias_tau:\", bias_tau)\n",
    "print(\"Alpha_tau:\", alpha_tau)\n",
    "print(\"Beta_tau:\", beta_tau)\n",
    "print(\"tau:\", tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have nconsumer_characteristics defined somewhere\n",
    "# Assuming you have X defined somewhere\n",
    "\n",
    "# Call the function with 'quadratic' model\n",
    "bias_tau, alpha_tau, beta_tau, tau, mu0 = create_TE_coefs(\n",
    "    X,\n",
    "    nconsumer_characteristics,\n",
    "    model=\"quadratic\",\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Bias_tau:\", bias_tau)\n",
    "print(\"Alpha_tau:\", alpha_tau)\n",
    "print(\"Beta_tau:\", beta_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_true_tau_mean(\n",
    "    alpha_tau,\n",
    "    bias_tau,\n",
    "    beta_tau,\n",
    "    model,\n",
    "    nconsumer_characteristics,\n",
    "):\n",
    "    \"\"\"Calculate true average treatment effect.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        bias_tau: float\n",
    "            Constant term in equation for tau.\n",
    "        alpha_tau: ndarray, shape = [nconsumer_characteristics, 1]\n",
    "            Linear coefficients in equation for tau.\n",
    "        beta_tau: ndarray, shape = [count]\n",
    "            Quadratic coefficients in equation for tau.\n",
    "            Count is the number of the second degree terms in a\n",
    "            quadratic polynomial where the number of variables is\n",
    "            equal to the number of consumer characteristics.\n",
    "        model: {'simple', 'quadratic'}\n",
    "            If 'simple' coefficients a and b in the artificial\n",
    "            dataset depend linearly on consumer characteristics.\n",
    "            Otherwise, the dependence is quadratic.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "    \"\"\"\n",
    "    X = 0.5\n",
    "\n",
    "    tau_true_mean = np.sum(X * alpha_tau) + bias_tau\n",
    "\n",
    "    if model == \"quadratic\":\n",
    "        count = nconsumer_characteristics * (nconsumer_characteristics + 1) // 2\n",
    "        X_poly = 0.25 * np.ones(count)\n",
    "        s = 0\n",
    "        for i in range(nconsumer_characteristics):\n",
    "            X_poly[s] = 1 / 3.0\n",
    "            s = s + nconsumer_characteristics - i\n",
    "\n",
    "        tau_true_mean = tau_true_mean + np.sum(X_poly * beta_tau)\n",
    "\n",
    "    return tau_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have alpha_tau, bias_tau, beta_tau, model, and nconsumer_characteristics defined somewhere\n",
    "model = \"quadratic\"\n",
    "# Call the function\n",
    "tau_true_mean = calculate_true_tau_mean(\n",
    "    alpha_tau,\n",
    "    bias_tau,\n",
    "    beta_tau,\n",
    "    model,\n",
    "    nconsumer_characteristics,\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(\"True average treatment effect:\", tau_true_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_propensity_scores(X, N, treatment=\"random\"):\n",
    "    \"\"\"Calculate propensity scores and create treatment variable for\n",
    "    fake dataset.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        X: ndarray, shape = (N, nconsumer_characteristics)\n",
    "            Feature matrix containing consumer characteristics.\n",
    "        N: int\n",
    "            Number of consumers.\n",
    "        treatment: {'random', 'not_random'}, optional\n",
    "            If 'random', consumers are being treated at random.\n",
    "            Otherwise, probability of being treated is a function\n",
    "            of consumer characteristics. Default is 'random'.\n",
    "    \"\"\"\n",
    "    if treatment == \"random\":\n",
    "        prob_of_T = 0.5\n",
    "        T = np.random.binomial(size=N, n=1, p=prob_of_T).reshape(N, 1)\n",
    "    else:\n",
    "        bias_p = 0.09\n",
    "        np.random.seed(72)\n",
    "        alpha_p = np.random.uniform(low=-0.55, high=0.55, size=[20, 1])\n",
    "        # Probability of t only depends on the first 20 consumers' features\n",
    "        p_of_t = np.dot(X[:, :20], alpha_p) + bias_p\n",
    "        p_of_t = p_of_t.reshape(-1)\n",
    "        prob_of_T = 1 / (1 + np.exp(-p_of_t))\n",
    "        T = np.random.binomial(size=N, n=1, p=prob_of_T).reshape(N, 1)\n",
    "\n",
    "    return T, prob_of_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have X, N, and treatment defined somewhere\n",
    "# Call the function\n",
    "T, prob_of_T = create_propensity_scores(X, N, treatment=\"\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Treatment variable (T):\")\n",
    "print(T)\n",
    "print(\"Propensity scores:\")\n",
    "print(prob_of_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def create_fake_data(\n",
    "    N,\n",
    "    nconsumer_characteristics,\n",
    "    model=\"quadtratic\",\n",
    "    verbose=False,\n",
    "    treatment=\"random\",\n",
    "):\n",
    "    seed = random.randint(1, 100000)\n",
    "    if verbose:\n",
    "        print(\"Seed number is: \", seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X = np.random.uniform(low=0, high=1, size=[N, nconsumer_characteristics])\n",
    "    normal_errors = np.random.normal(size=[N, 1], loc=0.0, scale=1.0)\n",
    "    alpha_tau, bias_tau, beta_tau = create_TE_coefs(model, nconsumer_characteristics)\n",
    "    tau_true_mean = calculate_true_tau_mean(alpha_tau, bias_tau, beta_tau, model)\n",
    "    T = np.random.choice([0, 1], size=(N, 1)) if treatment == \"random\" else None\n",
    "    prob_of_T = None\n",
    "    if treatment == \"not_random\":\n",
    "        prob_of_T = create_propensity_scores(X)\n",
    "        T = np.random.binomial(1, prob_of_T, size=(N, 1))\n",
    "    Y = mu0 + tau * T + normal_errors\n",
    "    return Y, X, T, seed, prob_of_T, tau_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the required parameters defined\n",
    "N = 1000\n",
    "nconsumer_characteristics = 100\n",
    "model = \"quadratic\"  # Specify the model type, either 'simple' or 'quadratic'\n",
    "verbose = True\n",
    "treatment = \"not_random\"  # Specify the treatment type, either 'random' or 'not_random'\n",
    "\n",
    "# Call the function\n",
    "Y, X, T, seed, prob_of_T, tau_true_mean = create_fake_data(\n",
    "    N,\n",
    "    nconsumer_characteristics,\n",
    "    model,\n",
    "    verbose,\n",
    "    treatment,\n",
    ")\n",
    "\n",
    "# Print or use the generated data as needed\n",
    "print(\"Generated data:\")\n",
    "print(\"Y:\", Y)\n",
    "print(\"X:\", X)\n",
    "print(\"Mu0:\", mu0)\n",
    "print(\"Tau:\", tau)\n",
    "print(\"T:\", T)\n",
    "print(\"Seed:\", seed)\n",
    "print(\"Probability of T:\", prob_of_T)\n",
    "print(\"True average treatment effect:\", tau_true_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for 2xt DifDif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latest system for difference in difference estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # Number of simulations\n",
    "T = 10  # Number of observations per group\n",
    "beta = 2.0  # True treatment effect\n",
    "gamma = 0.5  # True time trend coefficient\n",
    "sigma = 1.0  # Standard deviation of error term\n",
    "\n",
    "# Initialize arrays to store results\n",
    "bias = []\n",
    "rmse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "pre_treatment = np.random.normal(0, sigma, (T, 2))  # Two groups: treatment and control\n",
    "time_trend = np.arange(1, T + 1) * gamma\n",
    "post_treatment = pre_treatment.copy()\n",
    "post_treatment[:, 0] += beta  # Introduce treatment effect for treatment group\n",
    "post_treatment[:, :] += time_trend.reshape(-1, 1)  # Add time trend\n",
    "\n",
    "# Combine pre and post treatment data\n",
    "data = np.vstack((pre_treatment, post_treatment))\n",
    "groups = np.repeat([\"Control\", \"Treatment\"], T)\n",
    "time_periods = np.repeat(np.arange(1, T + 1), 2)\n",
    "\n",
    "df = pd.DataFrame({\"Group\": groups, \"Time\": time_periods, \"Outcome\": data.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define parameters\n",
    "N = 1000  # Number of simulations\n",
    "T = 10  # Number of observations per group\n",
    "beta = 2.0  # True treatment effect\n",
    "gamma = 0.5  # True time trend coefficient\n",
    "sigma = 1.0  # Standard deviation of error term\n",
    "\n",
    "# Initialize arrays to store results\n",
    "bias = []\n",
    "rmse = []\n",
    "\n",
    "# Run simulations\n",
    "for _ in range(N):\n",
    "    # Generate data\n",
    "    pre_treatment = np.random.normal(\n",
    "        0,\n",
    "        sigma,\n",
    "        (T, 2),\n",
    "    )  # Two groups: treatment and control\n",
    "    time_trend = np.arange(1, T + 1) * gamma\n",
    "    post_treatment = pre_treatment.copy()\n",
    "    post_treatment[:, 0] += beta  # Introduce treatment effect for treatment group\n",
    "    post_treatment[:, :] += time_trend.reshape(-1, 1)  # Add time trend\n",
    "\n",
    "    # Combine pre and post treatment data\n",
    "    data = np.vstack((pre_treatment, post_treatment))\n",
    "    groups = np.repeat([\"Control\", \"Treatment\"], T)\n",
    "    time_periods = np.repeat(np.arange(1, T + 1), 2)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\"Group\": groups, \"Time\": time_periods, \"Outcome\": data.flatten()},\n",
    "    )\n",
    "\n",
    "    # Run DiD regression\n",
    "    df[\"Treat\"] = (df[\"Group\"] == \"Treatment\").astype(int)\n",
    "    df[\"Post\"] = (df[\"Time\"] > T).astype(int)\n",
    "    df[\"Treat_Post\"] = df[\"Treat\"] * df[\"Post\"]\n",
    "\n",
    "    X = df[[\"Treat\", \"Post\", \"Treat_Post\"]]\n",
    "    X = sm.add_constant(X)\n",
    "    y = df[\"Outcome\"]\n",
    "\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Extract estimated treatment effect\n",
    "    est_beta = model.params[\"Treat_Post\"]\n",
    "\n",
    "    # Calculate bias and RMSE\n",
    "    bias.append(est_beta - beta)\n",
    "    rmse.append((est_beta - beta) ** 2)\n",
    "\n",
    "# Calculate mean bias and RMSE\n",
    "mean_bias = np.mean(bias)\n",
    "mean_rmse = np.sqrt(np.mean(rmse))\n",
    "\n",
    "print(\"Mean Bias:\", mean_bias)\n",
    "print(\"RMSE:\", mean_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate synthetic data with known true ATE\n",
    "\n",
    "\n",
    "def generate_data_with_true_ate(n_samples, true_ate, seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    control_data = pd.DataFrame(\n",
    "        {\n",
    "            \"individual\": range(1, n_samples + 1),\n",
    "            \"Age\": np.random.randint(20, 65, size=n_samples),\n",
    "            \"WagePartner_income\": np.random.normal(\n",
    "                loc=30000,\n",
    "                scale=5000,\n",
    "                size=n_samples,\n",
    "            ),\n",
    "            \"education_level\": np.random.choice(\n",
    "                [\"No High School\", \"High School\", \"Bachelor\", \"Master\", \"PhD\"],\n",
    "                size=n_samples,\n",
    "            ),\n",
    "            \"time\": np.random.choice([-2, -1, 1, 2, 3, 4, 5, 6, 7, 8], size=n_samples),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fq_levels = [\"Low\", \"High\"]\n",
    "    reform_levels = [\"Before\", \"After\"]\n",
    "    categorical_data = pd.DataFrame(\n",
    "        {\n",
    "            \"individual\": range(1, n_samples + 1),\n",
    "            \"FQ\": np.random.choice(fq_levels, size=n_samples),\n",
    "            \"Reform\": np.random.choice(reform_levels, size=n_samples),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fq_encoding = {\"Low\": 0, \"High\": 1}\n",
    "    reform_encoding = {\"Before\": 0, \"After\": 1}\n",
    "    categorical_data[\"FQ_encoded\"] = categorical_data[\"FQ\"].map(fq_encoding)\n",
    "    categorical_data[\"Reform_encoded\"] = categorical_data[\"Reform\"].map(reform_encoding)\n",
    "\n",
    "    categorical_data[\"interaction_effect\"] = (\n",
    "        categorical_data[\"FQ_encoded\"] * categorical_data[\"Reform_encoded\"]\n",
    "    )\n",
    "\n",
    "    control_data[\"wage_year_male\"] = np.random.normal(\n",
    "        loc=30,\n",
    "        scale=10,\n",
    "        size=n_samples,\n",
    "    ) * (1 + 0.1 * categorical_data[\"interaction_effect\"])\n",
    "    control_data[\"wage_year_female\"] = control_data[\"WagePartner_income\"]\n",
    "    control_data[\"dependent_variable\"] = (\n",
    "        control_data[\"wage_year_male\"] - control_data[\"wage_year_female\"]\n",
    "    )\n",
    "\n",
    "    education_encoding = {\n",
    "        \"No High School\": 0,\n",
    "        \"High School\": 1,\n",
    "        \"Bachelor\": 2,\n",
    "        \"Master\": 3,\n",
    "        \"PhD\": 4,\n",
    "    }\n",
    "    control_data[\"education_level_encoded\"] = control_data[\"education_level\"].map(\n",
    "        education_encoding,\n",
    "    )\n",
    "\n",
    "    data = pd.merge(control_data, categorical_data, on=\"individual\")\n",
    "    data = data.drop([\"Reform\", \"FQ\", \"education_level\"], axis=1)\n",
    "    data = data.drop(\n",
    "        [\"WagePartner_income\", \"wage_year_male\", \"wage_year_female\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return data, true_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Difference-in-Differences (DiD) with known true ATE\n",
    "\n",
    "\n",
    "def difference_in_differences_known_ate(data):\n",
    "    treatment_group = data[data[\"Reform_encoded\"] == 1]\n",
    "    control_group = data[data[\"Reform_encoded\"] == 0]\n",
    "\n",
    "    before_treatment_treatment_group = treatment_group[treatment_group[\"time\"] < 0][\n",
    "        \"dependent_variable\"\n",
    "    ].mean()\n",
    "    after_treatment_treatment_group = treatment_group[treatment_group[\"time\"] > 0][\n",
    "        \"dependent_variable\"\n",
    "    ].mean()\n",
    "    before_treatment_control_group = control_group[control_group[\"time\"] < 0][\n",
    "        \"dependent_variable\"\n",
    "    ].mean()\n",
    "    after_treatment_control_group = control_group[control_group[\"time\"] > 0][\n",
    "        \"dependent_variable\"\n",
    "    ].mean()\n",
    "\n",
    "    pre_treatment_difference = (\n",
    "        before_treatment_treatment_group - before_treatment_control_group\n",
    "    )\n",
    "    post_treatment_difference = (\n",
    "        after_treatment_treatment_group - after_treatment_control_group\n",
    "    )\n",
    "\n",
    "    return post_treatment_difference - pre_treatment_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulation with known true ATE\n",
    "num_simulations = 1000\n",
    "ate_results_with_true_ate = []\n",
    "\n",
    "seed_value = 634\n",
    "\n",
    "true_ate = 10  # Set true ATE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "generate_data_with_true_ate(n_samples, true_ate, seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_simulations):\n",
    "    seed = seed_value + i\n",
    "    synthetic_data, true_ate = generate_data_with_true_ate(\n",
    "        n_samples=1000,  # sample size\n",
    "        true_ate=true_ate,\n",
    "        seed=seed,\n",
    "    )\n",
    "    ate = difference_in_differences_known_ate(synthetic_data)\n",
    "\n",
    "# Calculate mean and standard error of estimated ATE\n",
    "\n",
    "# Calculate mean and standard error of estimated ATE\n",
    "ate_results_with_true_ate = np.array(ate_results_with_true_ate)\n",
    "mean_ate = np.mean(ate)\n",
    "std_err_ate = np.std(ate, ddof=1) / np.sqrt(\n",
    "    num_simulations,\n",
    ")\n",
    "\n",
    "# Calculate t-value\n",
    "t_value_ate = mean_ate / std_err_ate\n",
    "\n",
    "# Calculate p-value\n",
    "degrees_of_freedom = num_simulations - 1\n",
    "p_value_sim_ate = stats.t.cdf(\n",
    "    t_value_ate,\n",
    "    df=degrees_of_freedom,\n",
    ")  # richtige verteilung?\n",
    "critical_value_simulated = stats.t.ppf((1 + 0.95) / 2, df=degrees_of_freedom)\n",
    "\n",
    "# Calculate confidence interval for simulated ATE\n",
    "\n",
    "mean_ate_simulated = np.mean(ate)\n",
    "std_err_ate_simulated = np.std(ate, ddof=1) / np.sqrt(\n",
    "    num_simulations,\n",
    ")\n",
    "margin_of_error_simulated = critical_value_simulated * std_err_ate_simulated\n",
    "lower_bound_simulated = mean_ate_simulated - margin_of_error_simulated\n",
    "upper_bound_simulated = mean_ate_simulated + margin_of_error_simulated\n",
    "\n",
    "\n",
    "# Calculate confidence interval for true ATE\n",
    "critical_value_with_true_ate = stats.t.ppf((1 + 0.95) / 2, df=degrees_of_freedom)\n",
    "margin_of_error_with_true_ate = critical_value_with_true_ate * std_err_ate\n",
    "lower_bound_with_true_ate = true_ate - margin_of_error_with_true_ate\n",
    "upper_bound_with_true_ate = true_ate + margin_of_error_with_true_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Mean ATE with known true ATE:\", 10)\n",
    "print(\n",
    "    \"Confidence Interval for simulated ATE:\",\n",
    "    (lower_bound_simulated, upper_bound_simulated),\n",
    ")\n",
    "print(\"Mean ATE of simulated ATE:\", mean_ate)\n",
    "print(\"Standard Error of simulated ATE:\", std_err_ate)\n",
    "print(\"t-value of simulated ATE:\", t_value_ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(\n",
    "    ate_results_with_true_ate[:, 1],\n",
    "    bins=30,\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    "    label=\"Simulated ATE\",\n",
    ")\n",
    "plt.axvline(\n",
    "    mean_ate_with_true_ate,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=\"Mean Simulated ATE\",\n",
    ")\n",
    "plt.axvline(true_ate, color=\"green\", linestyle=\"--\", linewidth=1.5, label=\"True ATE\")\n",
    "plt.xlabel(\"Average Treatment Effect (ATE)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Simulated Average Treatment Effects (ATE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    [ate_results_with_true_ate[:, 1], [true_ate]],\n",
    "    labels=[\"Simulated ATE\", \"True ATE\"],\n",
    ")\n",
    "plt.ylabel(\"Average Treatment Effect (ATE)\")\n",
    "plt.title(\"Boxplot of Simulated and True Average Treatment Effects (ATE)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTE\n",
    "\n",
    "\\[ Y_{it} = \\beta_0 + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\delta (\\text{Post}_t \\times \\text{Treatment}_i) + \\epsilon_{it} \\]\n",
    "\n",
    "Where:\n",
    "- \\( Y_{it} \\) represents the outcome variable (e.g., wages) for individual \\( i \\) at time \\( t \\).\n",
    "- \\( \\text{Post}_t \\) is a binary variable indicating whether the observation is from the post-treatment period.\n",
    "- \\( \\text{Treatment}_i \\) is a binary variable indicating whether individual \\( i \\) is in the treatment group.\n",
    "- \\( \\delta \\) represents the coefficient for the interaction between the post-treatment period and the treatment group, capturing the average treatment effect (ATE).\n",
    "- \\( \\epsilon_{it} \\) is the error term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATE\n",
    "\n",
    "\\[ Y_{it} = \\beta_0 + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\sum_{k=1}^{K} \\beta_{k+3} (\\text{X}_i = k \\times \\text{Treatment}_i) + \\epsilon_{it} \\]\n",
    "\n",
    "Where:\n",
    "- \\( Y_{it} \\) represents the outcome variable (e.g., wages) for individual \\( i \\) at time \\( t \\).\n",
    "- \\( \\text{Post}_t \\) is a binary variable indicating whether the observation is from the post-treatment period.\n",
    "- \\( \\text{Treatment}_i \\) is a binary variable indicating whether individual \\( i \\) is in the treatment group.\n",
    "- \\( \\text{X}_i \\) is a categorical variable representing a conditioning variable (e.g., education level) for individual \\( i \\).\n",
    "- \\( K \\) is the total number of levels of the conditioning variable.\n",
    "- \\( \\beta_{k+3} \\) represents the coefficient for the interaction between the conditioning variable level \\( k \\) and the treatment indicator.\n",
    "- \\( \\epsilon_{it} \\) is the error term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the modell converges very fast to the true value\n",
    "\n",
    "- proposed changes: more zeroes in the income values \n",
    "- CATE and estimation with a non CATE DiD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def generate_data_with_true_ate(n_samples, true_ate, seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    control_data = pd.DataFrame(\n",
    "        {\n",
    "            \"individual\": range(1, n_samples + 1),\n",
    "            \"Age\": np.random.randint(20, 65, size=n_samples),\n",
    "            \"WagePartner_income\": np.random.normal(\n",
    "                loc=30000,\n",
    "                scale=5000,\n",
    "                size=n_samples,\n",
    "            ),\n",
    "            \"education_level\": np.random.choice(\n",
    "                [\"No High School\", \"High School\", \"Bachelor\", \"Master\", \"PhD\"],\n",
    "                size=n_samples,\n",
    "            ),\n",
    "            \"time\": np.random.choice([-2, -1, 1, 2, 3, 4, 5, 6, 7, 8], size=n_samples),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fq_levels = [\"Low\", \"High\"]\n",
    "    reform_levels = [\"Before\", \"After\"]\n",
    "    categorical_data = pd.DataFrame(\n",
    "        {\n",
    "            \"individual\": range(1, n_samples + 1),\n",
    "            \"FQ\": np.random.choice(fq_levels, size=n_samples),\n",
    "            \"Reform\": np.random.choice(reform_levels, size=n_samples),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fq_encoding = {\"Low\": 0, \"High\": 1}\n",
    "    reform_encoding = {\"Before\": 0, \"After\": 1}\n",
    "    categorical_data[\"FQ_encoded\"] = categorical_data[\"FQ\"].map(fq_encoding)\n",
    "    categorical_data[\"Reform_encoded\"] = categorical_data[\"Reform\"].map(reform_encoding)\n",
    "\n",
    "    categorical_data[\"interaction_effect\"] = (\n",
    "        categorical_data[\"FQ_encoded\"] * categorical_data[\"Reform_encoded\"]\n",
    "    )\n",
    "\n",
    "    control_data[\"wage_year_male\"] = np.random.normal(\n",
    "        loc=30,\n",
    "        scale=10,\n",
    "        size=n_samples,\n",
    "    ) * (1 + 0.1 * categorical_data[\"interaction_effect\"])\n",
    "    control_data[\"wage_year_female\"] = control_data[\"WagePartner_income\"]\n",
    "    control_data[\"dependent_variable\"] = (\n",
    "        control_data[\"wage_year_male\"] - control_data[\"wage_year_female\"]\n",
    "    )\n",
    "\n",
    "    education_encoding = {\n",
    "        \"No High School\": 0,\n",
    "        \"High School\": 1,\n",
    "        \"Bachelor\": 2,\n",
    "        \"Master\": 3,\n",
    "        \"PhD\": 4,\n",
    "    }\n",
    "    control_data[\"education_level_encoded\"] = control_data[\"education_level\"].map(\n",
    "        education_encoding,\n",
    "    )\n",
    "\n",
    "    data = pd.merge(control_data, categorical_data, on=\"individual\")\n",
    "    data = data.drop([\"Reform\", \"FQ\", \"education_level\"], axis=1)\n",
    "    data = data.drop(\n",
    "        [\"WagePartner_income\", \"wage_year_male\", \"wage_year_female\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return data, true_ate\n",
    "\n",
    "\n",
    "def difference_in_differences_cte(data, conditioning_variable):\n",
    "    # Define treatment and control groups\n",
    "    treatment_group = data[data[\"Reform_encoded\"] == 1]\n",
    "    control_group = data[data[\"Reform_encoded\"] == 0]\n",
    "\n",
    "    # Initialize lists to store CTEs for each level of the conditioning variable\n",
    "    ctes = []\n",
    "\n",
    "    # Compute CTE for each level of the conditioning variable\n",
    "    for level in data[conditioning_variable].unique():\n",
    "        treatment_group_level = treatment_group[\n",
    "            treatment_group[conditioning_variable] == level\n",
    "        ]\n",
    "        control_group_level = control_group[\n",
    "            control_group[conditioning_variable] == level\n",
    "        ]\n",
    "\n",
    "        pre_diff_treatment_group = treatment_group_level[\n",
    "            treatment_group_level[\"time\"] < 0\n",
    "        ][\"dependent_variable\"].mean()\n",
    "        post_diff_treatment_group = treatment_group_level[\n",
    "            treatment_group_level[\"time\"] > 0\n",
    "        ][\"dependent_variable\"].mean()\n",
    "        pre_diff_control_group = control_group_level[control_group_level[\"time\"] < 0][\n",
    "            \"dependent_variable\"\n",
    "        ].mean()\n",
    "        post_diff_control_group = control_group_level[control_group_level[\"time\"] > 0][\n",
    "            \"dependent_variable\"\n",
    "        ].mean()\n",
    "\n",
    "        cte_treatment_group = post_diff_treatment_group - pre_diff_treatment_group\n",
    "        cte_control_group = post_diff_control_group - pre_diff_control_group\n",
    "\n",
    "        cte = cte_treatment_group - cte_control_group\n",
    "        ctes.append(cte)\n",
    "\n",
    "    # Compute overall CTE\n",
    "    return sum(ctes)\n",
    "\n",
    "\n",
    "# Generate data\n",
    "num_simulations = 1000\n",
    "seed_value = 42\n",
    "true_ate = 10  # Adjust as needed\n",
    "ate_results_with_true_ate = []\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    seed = seed_value + i\n",
    "    synthetic_data, _ = generate_data_with_true_ate(\n",
    "        n_samples=1000,\n",
    "        true_ate=true_ate,\n",
    "        seed=seed,\n",
    "    )\n",
    "    cte = difference_in_differences_cte(synthetic_data, \"education_level_encoded\")\n",
    "    ate_results_with_true_ate.append(cte)\n",
    "\n",
    "# Calculate mean and standard error of estimated CTE\n",
    "mean_cte_with_true_ate = np.mean(ate_results_with_true_ate)\n",
    "std_err_cte_with_true_ate = np.std(ate_results_with_true_ate, ddof=1) / np.sqrt(\n",
    "    num_simulations,\n",
    ")\n",
    "\n",
    "# Calculate t-value\n",
    "t_value_with_true_ate = mean_cte_with_true_ate / std_err_cte_with_true_ate\n",
    "\n",
    "# Calculate p-value\n",
    "degrees_of_freedom = num_simulations - 1\n",
    "p_value_with_true_ate = stats.t.cdf(t_value_with_true_ate, df=degrees_of_freedom)\n",
    "\n",
    "# Calculate confidence interval for simulated CTE\n",
    "critical_value_with_true_ate = stats.t.ppf((1 + 0.95) / 2, df=degrees_of_freedom)\n",
    "margin_of_error_with_true_ate = critical_value_with_true_ate * std_err_cte_with_true_ate\n",
    "lower_bound_with_true_ate = mean_cte_with_true_ate - margin_of_error_with_true_ate\n",
    "upper_bound_with_true_ate = mean_cte_with_true_ate + margin_of_error_with_true_ate\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"True CTE:\", true_ate)\n",
    "print(\"Mean CTE with known true CTE:\", mean_cte_with_true_ate)\n",
    "print(\"Standard Error of CTE with known true CTE:\", std_err_cte_with_true_ate)\n",
    "print(\"t-value with known true CTE:\", t_value_with_true_ate)\n",
    "print(\"p-value with known true CTE:\", p_value_with_true_ate)\n",
    "print(\n",
    "    \"Confidence Interval for simulated CTE:\",\n",
    "    (lower_bound_with_true_ate, upper_bound_with_true_ate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of simulated CTEs\n",
    "plt.hist(\n",
    "    ate_results_with_true_ate,\n",
    "    bins=20,\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    "    label=\"Simulated CTEs\",\n",
    ")\n",
    "plt.axvline(\n",
    "    x=mean_cte_with_true_ate,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean Simulated CTE\",\n",
    ")\n",
    "\n",
    "# Plot true CTE\n",
    "plt.axvline(x=true_ate, color=\"green\", linestyle=\"-\", label=\"True CTE\")\n",
    "\n",
    "plt.xlabel(\"Conditional Treatment Effect (CTE)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Simulated and True Conditional Treatment Effects\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
