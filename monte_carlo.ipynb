{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo System for simple 2x2 DiFnDif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATE\n",
    "\n",
    "$ Y_{it} = \\beta_0 + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\sum_{k=1}^{K} \\beta_{k+3} (\\text{X}_i = k \\times \\text{Treatment}_i) + \\epsilon_{it} $\n",
    "\n",
    "Where:\n",
    "- $ Y_{it} $ represents the outcome variable (e.g., wages) for individual $ i $ at time $ t $.\n",
    "- $ \\text{Post}_t $ is a binary variable indicating whether the observation is from the post-treatment period.\n",
    "- $ \\text{Treatment}_i $ is a binary variable indicating whether individual $ i $ is in the treatment group.\n",
    "- $ \\text{X}_i $ is a categorical variable representing a conditioning variable (e.g., education level) for individual $ i $.\n",
    "- $ K $ is the total number of levels of the conditioning variable.\n",
    "- $ \\beta_{k+3} $ represents the coefficient for the interaction between the conditioning variable level $ k $ and the treatment indicator.\n",
    "- $ \\epsilon_{it} $ is the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Estimated Treatment Effect ($ \\beta $)**:\n",
    "   The treatment effect ($ \\beta $) is the coefficient associated with the interaction term between the treatment indicator and the post-treatment period indicator. Mathematically, it is given by:\n",
    "\n",
    "   $ \\beta = \\text{Coefficient of } (\\text{Post} \\times \\text{Treatment}) $\n",
    "\n",
    "2. **Standard Error ($ SE $)**:\n",
    "   The standard error ($ SE $) of the treatment effect estimates how much the estimated treatment effect varies across different samples. It can be calculated as the square root of the variance of the coefficient estimate. \n",
    "\n",
    "3. **t-statistic ($ t $)**:\n",
    "   The t-statistic ($ t $) is a measure of the signal-to-noise ratio in the estimated treatment effect. It is calculated by dividing the estimated treatment effect by its standard error. Mathematically, it can be expressed as:\n",
    "\n",
    "   $ t = \\frac{\\beta}{SE} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data generating process fertig machen mit richtiger verteilung\n",
    "- monte carlo machen\n",
    "- simulationsergebnisse und true values vergleichen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for homogenous Treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set a seed value for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the coefficients (betas)\n",
    "true_beta_A = 2\n",
    "true_beta_B = 3\n",
    "true_beta_C = 6\n",
    "\n",
    "# Set the number of simulations\n",
    "num_simulations = 1000\n",
    "\n",
    "# Initialize lists to store estimated coefficients\n",
    "estimated_beta_A_list = []\n",
    "estimated_beta_B_list = []\n",
    "estimated_beta_C_list = []\n",
    "\n",
    "# Perform the simulations\n",
    "for _ in range(num_simulations):\n",
    "    # Generate random binary variables A, B, and C\n",
    "    A = np.random.randint(0, 2, size=num_simulations)\n",
    "    B = np.random.randint(0, 2, size=num_simulations)\n",
    "    C = A * B  # Interaction term of A and B\n",
    "\n",
    "    # Generate random control variables\n",
    "    control_1 = np.random.normal(0, 1, size=num_simulations)\n",
    "    control_2 = np.random.normal(0, 1, size=num_simulations)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=num_simulations)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    std_wage = 10  # Standard deviation of wage\n",
    "    wage = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame for the variables\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"A\": A,\n",
    "            \"B\": B,\n",
    "            \"C\": C,\n",
    "            \"Control_1\": control_1,\n",
    "            \"Control_2\": control_2,\n",
    "            \"Wage\": wage,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Create the model\n",
    "    X = sm.add_constant(data[[\"A\", \"B\", \"C\", \"Control_1\", \"Control_2\"]])\n",
    "    y = data[\"Wage\"]\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    # Extract the estimated coefficients and append to the lists\n",
    "    estimated_beta_A_list.append(results.params[\"A\"])\n",
    "    estimated_beta_B_list.append(results.params[\"B\"])\n",
    "    estimated_beta_C_list.append(results.params[\"C\"])\n",
    "\n",
    "# Calculate the average estimated coefficients\n",
    "average_estimated_beta_A = np.mean(estimated_beta_A_list)\n",
    "average_estimated_beta_B = np.mean(estimated_beta_B_list)\n",
    "average_estimated_beta_C = np.mean(estimated_beta_C_list)\n",
    "\n",
    "# Print the true coefficients and the average estimated coefficients\n",
    "print(\"True Coefficients:\")\n",
    "print(\"Beta_A:\", true_beta_A)\n",
    "print(\"Beta_B:\", true_beta_B)\n",
    "print(\"Beta_C:\", true_beta_C)\n",
    "print(\"\\nAverage Estimated Coefficients:\")\n",
    "print(\"Beta_A (Average Estimated):\", average_estimated_beta_A)\n",
    "print(\"Beta_B (Average Estimated):\", average_estimated_beta_B)\n",
    "print(\"Beta_C (Average Estimated):\", average_estimated_beta_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine the estimated coefficients into a single DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": estimated_beta_A_list,\n",
    "        \"B\": estimated_beta_B_list,\n",
    "        \"C\": estimated_beta_C_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Plot kernel density estimates for each coefficient\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=df, fill=True, palette=\"Set1\")\n",
    "plt.axvline(x=true_beta_A, color=\"red\", linestyle=\"--\", label=\"True Beta A\")\n",
    "plt.axvline(x=true_beta_B, color=\"blue\", linestyle=\"--\", label=\"True Beta B\")\n",
    "plt.axvline(x=true_beta_C, color=\"green\", linestyle=\"--\", label=\"True Beta C\")\n",
    "plt.title(\"Kernel Density Plot of Estimated Coefficients\")\n",
    "plt.xlabel(\"Estimated Coefficient Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for hetergenous Treatment effects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set a seed value for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the true coefficients\n",
    "true_beta_A = 2\n",
    "true_beta_B = 3\n",
    "true_beta_C = 6\n",
    "true_beta_A_X = -20  # Interaction effect of A and covariate X\n",
    "true_beta_B_X = 40  # Interaction effect of B and covariate X\n",
    "\n",
    "# Set the number of simulations\n",
    "num_simulations = 1000\n",
    "\n",
    "# Initialize lists to store estimated coefficients for both models\n",
    "estimated_beta_homogeneous_list = []\n",
    "estimated_beta_heterogeneous_list = []\n",
    "\n",
    "# Perform the simulations\n",
    "for _ in range(num_simulations):\n",
    "    # Generate random binary variables A, B, and C\n",
    "    A = np.random.randint(0, 2, size=num_simulations)\n",
    "    B = np.random.randint(0, 2, size=num_simulations)\n",
    "    C = np.random.randint(0, 2, size=num_simulations)\n",
    "\n",
    "    # Generate random covariate X\n",
    "    X = np.random.normal(0, 1, size=num_simulations)\n",
    "\n",
    "    # Generate random control variables\n",
    "    control_1 = np.random.normal(0, 1, size=num_simulations)\n",
    "    control_2 = np.random.normal(0, 1, size=num_simulations)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=num_simulations)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    std_wage = 10  # Standard deviation of wage\n",
    "    wage = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + true_beta_A_X * A * X\n",
    "        + true_beta_B_X * B * X\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame for the variables\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"A\": A,\n",
    "            \"B\": B,\n",
    "            \"C\": C,\n",
    "            \"X\": X,\n",
    "            \"Control_1\": control_1,\n",
    "            \"Control_2\": control_2,\n",
    "            \"Wage\": wage,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Create the homogenous treatment effect model\n",
    "    X_homogeneous = sm.add_constant(\n",
    "        data[[\"A\", \"B\", \"C\", \"X\", \"Control_1\", \"Control_2\"]],\n",
    "    )\n",
    "    y_homogeneous = data[\"Wage\"]\n",
    "    model_homogeneous = sm.OLS(y_homogeneous, X_homogeneous)\n",
    "    results_homogeneous = model_homogeneous.fit()\n",
    "    estimated_beta_homogeneous_list.append(results_homogeneous.params[\"C\"])\n",
    "\n",
    "    # Create the heterogeneous treatment effect model\n",
    "    data[\"A_X\"] = data[\"A\"] * data[\"X\"]\n",
    "    data[\"B_X\"] = data[\"B\"] * data[\"X\"]\n",
    "    X_heterogeneous = sm.add_constant(\n",
    "        data[[\"A\", \"B\", \"C\", \"X\", \"A_X\", \"B_X\", \"Control_1\", \"Control_2\"]],\n",
    "    )\n",
    "    y_heterogeneous = data[\"Wage\"]\n",
    "    model_heterogeneous = sm.OLS(y_heterogeneous, X_heterogeneous)\n",
    "    results_heterogeneous = model_heterogeneous.fit()\n",
    "    estimated_beta_heterogeneous_list.append(results_heterogeneous.params[\"C\"])\n",
    "\n",
    "# Calculate the average estimated coefficients for both models\n",
    "average_estimated_beta_homogeneous = np.mean(estimated_beta_homogeneous_list)\n",
    "average_estimated_beta_heterogeneous = np.mean(estimated_beta_heterogeneous_list)\n",
    "\n",
    "# Print the true coefficient and the average estimated coefficients for both models\n",
    "print(\"True Coefficient (Heterogeneous Treatment Effect):\", true_beta_C)\n",
    "print(\n",
    "    \"Average Estimated Coefficient (Homogeneous Treatment Effect Model):\",\n",
    "    average_estimated_beta_homogeneous,\n",
    ")\n",
    "print(\n",
    "    \"Average Estimated Coefficient (Heterogeneous Treatment Effect Model):\",\n",
    "    average_estimated_beta_heterogeneous,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define custom colors for coefficients\n",
    "homogeneous_color = \"blue\"\n",
    "heterogeneous_color = \"orange\"\n",
    "\n",
    "# Combine the estimated coefficients into DataFrames\n",
    "homogeneous_df = pd.DataFrame(\n",
    "    {\"Coefficient\": estimated_beta_homogeneous_list, \"Model\": \"Homogeneous\"},\n",
    ")\n",
    "heterogeneous_df = pd.DataFrame(\n",
    "    {\"Coefficient\": estimated_beta_heterogeneous_list, \"Model\": \"Heterogeneous\"},\n",
    ")\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df = pd.concat([homogeneous_df, heterogeneous_df])\n",
    "\n",
    "# Plot kernel density estimates for each coefficient\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(\n",
    "    data=df,\n",
    "    x=\"Coefficient\",\n",
    "    hue=\"Model\",\n",
    "    fill=True,\n",
    "    palette={\"Homogeneous\": homogeneous_color, \"Heterogeneous\": heterogeneous_color},\n",
    ")\n",
    "plt.axvline(x=true_beta_C, color=\"green\", linestyle=\"--\", label=\"True Beta C\")\n",
    "plt.title(\"Kernel Density Plot of Estimated Coefficients\")\n",
    "plt.xlabel(\"Estimated Coefficient Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for Deep Learning and propensity scores - self made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to generate synthetic data\n",
    "\n",
    "\n",
    "def generate_data(N, num_features):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(N, num_features)\n",
    "    A = np.random.randint(0, 2, size=N)\n",
    "    B = np.random.randint(0, 2, size=N)\n",
    "    C = A * B  # Interaction term of A and B\n",
    "    true_beta_A = 2\n",
    "    true_beta_B = 3\n",
    "    true_beta_C = 6\n",
    "\n",
    "    control_1 = np.random.normal(0, 1, size=N)\n",
    "    control_2 = np.random.normal(0, 1, size=N)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=N)\n",
    "\n",
    "    propensity_scores = 1 / (1 + np.exp(-C))\n",
    "    treatment_assignment = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    y = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    return X, treatment_assignment, y, true_beta_A, true_beta_B, true_beta_C\n",
    "\n",
    "\n",
    "# Function to train the neural network\n",
    "\n",
    "\n",
    "def train_nn(X_train, y_train, num_features, epochs, batch_size, learning_rate):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(64, activation=\"relu\", input_shape=(num_features,)),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(1),\n",
    "        ],\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to estimate coefficients\n",
    "\n",
    "\n",
    "def estimate_coefficients(model):\n",
    "    return model.layers[-1].get_weights()[0].flatten()\n",
    "\n",
    "\n",
    "# Function for Monte Carlo simulation\n",
    "\n",
    "\n",
    "def monte_carlo_simulation(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "):\n",
    "    true_beta_A_list = []\n",
    "    true_beta_B_list = []\n",
    "    true_beta_C_list = []\n",
    "    estimated_beta_A_list = []\n",
    "    estimated_beta_B_list = []\n",
    "    estimated_beta_C_list = []\n",
    "    for _ in range(num_simulations):\n",
    "        (\n",
    "            X,\n",
    "            treatment_assignment,\n",
    "            y,\n",
    "            true_beta_A,\n",
    "            true_beta_B,\n",
    "            true_beta_C,\n",
    "        ) = generate_data(N, num_features)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "        )\n",
    "        model = train_nn(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            num_features,\n",
    "            epochs,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "        )\n",
    "        true_beta_A_list.append(true_beta_A)\n",
    "        true_beta_B_list.append(true_beta_B)\n",
    "        true_beta_C_list.append(true_beta_C)\n",
    "        estimated_coefficients = estimate_coefficients(model)\n",
    "        estimated_beta_A_list.append(estimated_coefficients[0])\n",
    "        estimated_beta_B_list.append(estimated_coefficients[1])\n",
    "        estimated_beta_C_list.append(estimated_coefficients[2])\n",
    "    return (true_beta_A_list, true_beta_B_list, true_beta_C_list), (\n",
    "        estimated_beta_A_list,\n",
    "        estimated_beta_B_list,\n",
    "        estimated_beta_C_list,\n",
    "    )\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_simulations = 100  # Number of simulations\n",
    "N = 1000  # Number of samples\n",
    "num_features = 5  # Number of features\n",
    "epochs = 50  # Number of epochs\n",
    "batch_size = 32  # Batch size\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Run Monte Carlo simulation\n",
    "true_values, estimated_values = monte_carlo_simulation(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    ")\n",
    "\n",
    "# Calculate average absolute errors\n",
    "errors_A = np.abs(np.array(true_values[0]) - np.array(estimated_values[0]))\n",
    "errors_B = np.abs(np.array(true_values[1]) - np.array(estimated_values[1]))\n",
    "errors_C = np.abs(np.array(true_values[2]) - np.array(estimated_values[2]))\n",
    "\n",
    "# Calculate average estimated coefficients\n",
    "avg_estimated_beta_A = np.mean(estimated_values[0])\n",
    "avg_estimated_beta_B = np.mean(estimated_values[1])\n",
    "avg_estimated_beta_C = np.mean(estimated_values[2])\n",
    "\n",
    "print(\"Average Absolute Error for Beta_A:\", np.mean(errors_A))\n",
    "print(\"Average Absolute Error for Beta_B:\", np.mean(errors_B))\n",
    "print(\"Average Absolute Error for Beta_C:\", np.mean(errors_C))\n",
    "\n",
    "print(\"\\nAverage Estimated Coefficients:\")\n",
    "print(\"Average Beta_A:\", avg_estimated_beta_A)\n",
    "print(\"Average Beta_B:\", avg_estimated_beta_B)\n",
    "print(\"Average Beta_C:\", avg_estimated_beta_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract true and estimated beta values\n",
    "true_beta_A_list, true_beta_B_list, true_beta_C_list = true_values\n",
    "estimated_beta_A_list, estimated_beta_B_list, estimated_beta_C_list = estimated_values\n",
    "\n",
    "# Create kernel density plot for beta coefficients\n",
    "sns.kdeplot(true_beta_A_list, label=\"True Beta A\", fill=True, color=\"blue\")\n",
    "sns.kdeplot(true_beta_B_list, label=\"True Beta B\", fill=True, color=\"orange\")\n",
    "sns.kdeplot(true_beta_C_list, label=\"True Beta C\", fill=True, color=\"green\")\n",
    "sns.kdeplot(estimated_beta_A_list, label=\"Estimated Beta A\", fill=True)\n",
    "sns.kdeplot(estimated_beta_B_list, label=\"Estimated Beta B\", fill=True)\n",
    "sns.kdeplot(estimated_beta_C_list, label=\"Estimated Beta C\", fill=True)\n",
    "\n",
    "# Add vertical lines for true coefficients\n",
    "plt.axvline(\n",
    "    np.mean(true_beta_A_list),\n",
    "    color=\"blue\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean True Beta A\",\n",
    ")\n",
    "plt.axvline(\n",
    "    np.mean(true_beta_B_list),\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean True Beta B\",\n",
    ")\n",
    "plt.axvline(\n",
    "    np.mean(true_beta_C_list),\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean True Beta C\",\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Beta Coefficients\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Kernel Density Plot of Beta Coefficients\")\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# propensity scores approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_propensity_scores(N, num_features):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(N, num_features)\n",
    "    A = np.random.randint(0, 2, size=N)\n",
    "    B = np.random.randint(0, 2, size=N)\n",
    "    C = A * B  # Interaction term of A and B\n",
    "    true_beta_A = 2\n",
    "    true_beta_B = 3\n",
    "    true_beta_C = 6\n",
    "\n",
    "    control_1 = np.random.normal(0, 1, size=N)\n",
    "    control_2 = np.random.normal(0, 1, size=N)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=N)\n",
    "\n",
    "    propensity_scores = 1 / (1 + np.exp(-C))\n",
    "    treatment_assignment = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    y = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    # Include propensity scores as features\n",
    "    X = np.column_stack((X, propensity_scores))\n",
    "\n",
    "    return X, treatment_assignment, y, true_beta_A, true_beta_B, true_beta_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_with_propensity_scores(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                input_shape=(num_features,),\n",
    "            ),  # Input shape adjusted\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(1),\n",
    "        ],\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_simulation_with_propensity_scores(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "):\n",
    "    true_beta_A_list = []\n",
    "    true_beta_B_list = []\n",
    "    true_beta_C_list = []\n",
    "    estimated_beta_A_list = []\n",
    "    estimated_beta_B_list = []\n",
    "    estimated_beta_C_list = []\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        (\n",
    "            X,\n",
    "            treatment_assignment,\n",
    "            y,\n",
    "            true_beta_A,\n",
    "            true_beta_B,\n",
    "            true_beta_C,\n",
    "        ) = generate_data_with_propensity_scores(N, num_features)\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Train neural network model\n",
    "        model = train_nn_with_propensity_scores(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            num_features + 1,  # Add 1 for propensity score feature\n",
    "            epochs,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "        )\n",
    "\n",
    "        # Compute true and estimated coefficients\n",
    "        true_beta_A_list.append(true_beta_A)\n",
    "        true_beta_B_list.append(true_beta_B)\n",
    "        true_beta_C_list.append(true_beta_C)\n",
    "        estimated_coefficients = estimate_coefficients(model)\n",
    "        estimated_beta_A_list.append(estimated_coefficients[0])\n",
    "        estimated_beta_B_list.append(estimated_coefficients[1])\n",
    "        estimated_beta_C_list.append(estimated_coefficients[2])\n",
    "\n",
    "    return (true_beta_A_list, true_beta_B_list, true_beta_C_list), (\n",
    "        estimated_beta_A_list,\n",
    "        estimated_beta_B_list,\n",
    "        estimated_beta_C_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_simulations = 100  # Number of simulations\n",
    "N = 1000  # Number of samples\n",
    "num_features = 5  # Number of features\n",
    "epochs = 50  # Number of epochs\n",
    "batch_size = 32  # Batch size\n",
    "learning_rate = 0.001  # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_coefficients(model):\n",
    "    return model.layers[-1].get_weights()[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulation with propensity scores\n",
    "(\n",
    "    true_values_propensity,\n",
    "    estimated_values_propensity,\n",
    ") = monte_carlo_simulation_with_propensity_scores(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    ")\n",
    "\n",
    "# Calculate average absolute errors\n",
    "errors_A_propensity = np.abs(\n",
    "    np.array(true_values_propensity[0]) - np.array(estimated_values_propensity[0]),\n",
    ")\n",
    "errors_B_propensity = np.abs(\n",
    "    np.array(true_values_propensity[1]) - np.array(estimated_values_propensity[1]),\n",
    ")\n",
    "errors_C_propensity = np.abs(\n",
    "    np.array(true_values_propensity[2]) - np.array(estimated_values_propensity[2]),\n",
    ")\n",
    "\n",
    "# Calculate average estimated coefficients\n",
    "avg_estimated_beta_A_propensity = np.mean(estimated_values_propensity[0])\n",
    "avg_estimated_beta_B_propensity = np.mean(estimated_values_propensity[1])\n",
    "avg_estimated_beta_C_propensity = np.mean(estimated_values_propensity[2])\n",
    "\n",
    "print(\n",
    "    \"Average Absolute Error for Beta_A with Propensity Scores:\",\n",
    "    np.mean(errors_A_propensity),\n",
    ")\n",
    "print(\n",
    "    \"Average Absolute Error for Beta_B with Propensity Scores:\",\n",
    "    np.mean(errors_B_propensity),\n",
    ")\n",
    "print(\n",
    "    \"Average Absolute Error for Beta_C with Propensity Scores:\",\n",
    "    np.mean(errors_C_propensity),\n",
    ")\n",
    "\n",
    "print(\"\\nAverage Estimated Coefficients with Propensity Scores:\")\n",
    "print(\"Average Beta_A:\", avg_estimated_beta_A_propensity)\n",
    "print(\"Average Beta_B:\", avg_estimated_beta_B_propensity)\n",
    "print(\"Average Beta_C:\", avg_estimated_beta_C_propensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract estimated beta values with propensity scores\n",
    "(\n",
    "    estimated_beta_A_list_propensity,\n",
    "    estimated_beta_B_list_propensity,\n",
    "    estimated_beta_C_list_propensity,\n",
    ") = estimated_values_propensity\n",
    "\n",
    "# Create kernel density plot for estimated beta coefficients with propensity scores\n",
    "sns.kdeplot(\n",
    "    estimated_beta_A_list_propensity,\n",
    "    label=\"Estimated Beta A with Propensity Scores\",\n",
    "    shade=True,\n",
    ")\n",
    "sns.kdeplot(\n",
    "    estimated_beta_B_list_propensity,\n",
    "    label=\"Estimated Beta B with Propensity Scores\",\n",
    "    shade=True,\n",
    ")\n",
    "sns.kdeplot(\n",
    "    estimated_beta_C_list_propensity,\n",
    "    label=\"Estimated Beta C with Propensity Scores\",\n",
    "    shade=True,\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Beta Coefficients\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Kernel Density Plot of Estimated Beta Coefficients with Propensity Scores\")\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last try for today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function to generate synthetic data\n",
    "\n",
    "\n",
    "def generate_data(N, num_features):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(N, num_features)\n",
    "    A = np.random.randint(0, 2, size=N)\n",
    "    B = np.random.randint(0, 2, size=N)\n",
    "    C = A * B  # Interaction term of A and B\n",
    "    true_beta_A = 2\n",
    "    true_beta_B = 3\n",
    "    true_beta_C = 6\n",
    "\n",
    "    control_1 = np.random.normal(0, 1, size=N)\n",
    "    control_2 = np.random.normal(0, 1, size=N)\n",
    "\n",
    "    mean_error = 0  # Mean of the error term\n",
    "    std_error = 10  # Standard deviation of the error term\n",
    "    error = np.random.normal(mean_error, std_error, size=N)\n",
    "\n",
    "    propensity_scores = 1 / (1 + np.exp(-C))\n",
    "    treatment_assignment = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate normally distributed outcome variable (wage)\n",
    "    mean_wage = 50  # Mean wage\n",
    "    y = (\n",
    "        mean_wage\n",
    "        + true_beta_A * A\n",
    "        + true_beta_B * B\n",
    "        + true_beta_C * C\n",
    "        + control_1\n",
    "        + control_2\n",
    "        + error\n",
    "    )\n",
    "\n",
    "    return X, treatment_assignment, y, true_beta_A, true_beta_B, true_beta_C\n",
    "\n",
    "\n",
    "# Function to train the neural network\n",
    "\n",
    "\n",
    "def train_nn(X, y, num_features, epochs, batch_size, learning_rate):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(64, activation=\"relu\", input_shape=(num_features,)),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(1),\n",
    "        ],\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to estimate treatment effects using DID\n",
    "\n",
    "\n",
    "def estimate_did(X, treatment_assignment, y, model):\n",
    "    # Predict outcomes using the trained neural network\n",
    "    predicted_outcomes = model.predict(X).flatten()\n",
    "\n",
    "    # Combine observed outcomes and predicted outcomes\n",
    "    df = pd.DataFrame(\n",
    "        {\"Y\": y, \"Y_predicted\": predicted_outcomes, \"Treatment\": treatment_assignment},\n",
    "    )\n",
    "\n",
    "    # Compute differences in differences\n",
    "    control_group = df[df[\"Treatment\"] == 0]\n",
    "    treatment_group = df[df[\"Treatment\"] == 1]\n",
    "    pre_period = control_group[control_group.index < len(control_group) // 2]\n",
    "    post_period = control_group[control_group.index >= len(control_group) // 2]\n",
    "\n",
    "    control_diff = post_period[\"Y\"].mean() - pre_period[\"Y\"].mean()\n",
    "    return (\n",
    "        treatment_group[\"Y_predicted\"].mean() - treatment_group[\"Y\"].mean()\n",
    "    ) - control_diff\n",
    "\n",
    "\n",
    "# Function for Monte Carlo simulation\n",
    "\n",
    "\n",
    "def monte_carlo_simulation(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "):\n",
    "    treatment_diffs = []\n",
    "    for _ in range(num_simulations):\n",
    "        (\n",
    "            X,\n",
    "            treatment_assignment,\n",
    "            y,\n",
    "            true_beta_A,\n",
    "            true_beta_B,\n",
    "            true_beta_C,\n",
    "        ) = generate_data(N, num_features)\n",
    "        model = train_nn(\n",
    "            X,\n",
    "            y,\n",
    "            num_features,\n",
    "            epochs,\n",
    "            batch_size,\n",
    "            learning_rate,\n",
    "        )\n",
    "        treatment_diff = estimate_did(X, treatment_assignment, y, model)\n",
    "        treatment_diffs.append(treatment_diff)\n",
    "    return treatment_diffs\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_simulations = 100  # Number of simulations\n",
    "N = 1000  # Number of samples\n",
    "num_features = 5  # Number of features\n",
    "epochs = 50  # Number of epochs\n",
    "batch_size = 32  # Batch size\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Run Monte Carlo simulation\n",
    "treatment_diffs = monte_carlo_simulation(\n",
    "    num_simulations,\n",
    "    N,\n",
    "    num_features,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    ")\n",
    "\n",
    "# Calculate average treatment effect\n",
    "avg_treatment_diff = np.mean(treatment_diffs)\n",
    "print(\"Average treatment effect estimated using DID:\", avg_treatment_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# True treatment effect\n",
    "true_effect = np.mean(treatment_diffs)\n",
    "\n",
    "# Plot histogram of treatment effects\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(\n",
    "    treatment_diffs,\n",
    "    bins=20,\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Estimated Treatment Effect\",\n",
    ")\n",
    "plt.axvline(x=true_effect, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect\")\n",
    "plt.title(\"Distribution of Treatment Effects Estimated using DID\")\n",
    "plt.xlabel(\"Treatment Effect\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for Deep Learning and propensity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_polynomial_X_times_weights(X, nconsumer_characteristics, weights):\n",
    "    \"\"\"Evaluate the non-linear part of a quadratic polynomial in\n",
    "    consumer characteristics, X, with prescribed weights.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        X: ndarray, shape = (N, nconsumer_characteristics)\n",
    "            Feature matrix containing consumer characteristics.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "        weights: ndarray, shape = (num_additional_poly_terms, )\n",
    "            Weights corresponding to quadratic terms.\n",
    "    Outputs:\n",
    "    -------\n",
    "        sum_x: ndarray, shape = (N, 1)\n",
    "            Non-linear part of the quadratic polynomial evaluated\n",
    "            for each consumer.\n",
    "    \"\"\"\n",
    "    from itertools import combinations_with_replacement\n",
    "\n",
    "    my_polynomial_indices = combinations_with_replacement(\n",
    "        list(range(nconsumer_characteristics)),\n",
    "        2,\n",
    "    )\n",
    "    sum_x = 0\n",
    "    for i, p in enumerate(my_polynomial_indices):\n",
    "        sum_x = sum_x + weights[i] * np.multiply(X[:, p[0]], X[:, p[1]])\n",
    "    return sum_x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have X, nconsumer_characteristics, and weights defined somewhere\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "N = 1000  # Number of consumers\n",
    "nconsumer_characteristics = 100  # Number of consumer characteristics\n",
    "X = np.random.rand(N, nconsumer_characteristics)  # Feature matrix\n",
    "weights = np.random.rand(\n",
    "    nconsumer_characteristics * (nconsumer_characteristics + 1) // 2,\n",
    ")  # Random weights\n",
    "# Call the function\n",
    "result = sum_polynomial_X_times_weights(X, nconsumer_characteristics, weights)\n",
    "\n",
    "# Print the result\n",
    "print(\"Result shape:\", result.shape)\n",
    "print(\"Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_TE_coefs(X, nconsumer_characteristics, model=\"quadratic\"):\n",
    "    \"\"\"Create treatment effect coefficients.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        X: ndarray, shape = (N, nconsumer_characteristics)\n",
    "            Feature matrix containing consumer characteristics.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "        model: str, optional\n",
    "            Type of model to use for coefficients. Default is 'simple'.\n",
    "            Options are {'simple', 'quadratic'}.\n",
    "\n",
    "    Outputs:\n",
    "    -------\n",
    "        bias_tau: float\n",
    "            Constant term in equation for tau.\n",
    "        alpha_tau: ndarray, shape = [nconsumer_characteristics]\n",
    "            Linear coefficients in equation for tau.\n",
    "        beta_tau: ndarray or None\n",
    "            Quadratic coefficients in equation for tau.\n",
    "            If model is 'quadratic', returns coefficients.\n",
    "            Otherwise, returns None.\n",
    "    \"\"\"\n",
    "    np.random.seed(63)\n",
    "\n",
    "    # Calculating tau\n",
    "    alpha_tau = np.random.uniform(low=0.1, high=0.22, size=nconsumer_characteristics)\n",
    "    bias_tau = -0.05\n",
    "    tau = np.dot(X, alpha_tau) + bias_tau\n",
    "\n",
    "    if model == \"quadratic\":\n",
    "        count = nconsumer_characteristics * (nconsumer_characteristics + 1) // 2\n",
    "        beta_tau = np.random.uniform(low=-0.05, high=0.06, size=count)\n",
    "        tau = tau + sum_polynomial_X_times_weights(\n",
    "            X,\n",
    "            nconsumer_characteristics,\n",
    "            beta_tau,\n",
    "        )\n",
    "    else:\n",
    "        beta_tau = None\n",
    "\n",
    "    # Calculating mu0\n",
    "    alpha_mu0 = np.random.normal(loc=0.3, scale=0.7, size=nconsumer_characteristics)\n",
    "    bias_mu0 = 0.09\n",
    "    mu0 = np.dot(X, alpha_mu0) + bias_mu0\n",
    "\n",
    "    if model == \"quadratic\":\n",
    "        beta_mu0 = np.random.normal(loc=0.01, scale=0.3, size=count)\n",
    "        mu0 = mu0 + sum_polynomial_X_times_weights(\n",
    "            X,\n",
    "            nconsumer_characteristics,\n",
    "            beta_mu0,\n",
    "        )\n",
    "\n",
    "    return bias_tau, alpha_tau, beta_tau, tau, mu0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have nconsumer_characteristics defined somewhere\n",
    "# Assuming you have X defined somewhere\n",
    "# Call the function with 'simple' model\n",
    "bias_tau, alpha_tau, beta_tau, tau, mu0 = create_TE_coefs(\n",
    "    X,\n",
    "    nconsumer_characteristics,\n",
    "    model=\"quadtratic\",\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Bias_tau:\", bias_tau)\n",
    "print(\"Alpha_tau:\", alpha_tau)\n",
    "print(\"Beta_tau:\", beta_tau)\n",
    "print(\"tau:\", tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have nconsumer_characteristics defined somewhere\n",
    "# Assuming you have X defined somewhere\n",
    "\n",
    "# Call the function with 'quadratic' model\n",
    "bias_tau, alpha_tau, beta_tau, tau, mu0 = create_TE_coefs(\n",
    "    X,\n",
    "    nconsumer_characteristics,\n",
    "    model=\"quadratic\",\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Bias_tau:\", bias_tau)\n",
    "print(\"Alpha_tau:\", alpha_tau)\n",
    "print(\"Beta_tau:\", beta_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_true_tau_mean(\n",
    "    alpha_tau,\n",
    "    bias_tau,\n",
    "    beta_tau,\n",
    "    model,\n",
    "    nconsumer_characteristics,\n",
    "):\n",
    "    \"\"\"Calculate true average treatment effect.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        bias_tau: float\n",
    "            Constant term in equation for tau.\n",
    "        alpha_tau: ndarray, shape = [nconsumer_characteristics, 1]\n",
    "            Linear coefficients in equation for tau.\n",
    "        beta_tau: ndarray, shape = [count]\n",
    "            Quadratic coefficients in equation for tau.\n",
    "            Count is the number of the second degree terms in a\n",
    "            quadratic polynomial where the number of variables is\n",
    "            equal to the number of consumer characteristics.\n",
    "        model: {'simple', 'quadratic'}\n",
    "            If 'simple' coefficients a and b in the artificial\n",
    "            dataset depend linearly on consumer characteristics.\n",
    "            Otherwise, the dependence is quadratic.\n",
    "        nconsumer_characteristics: int\n",
    "            Number of consumer characteristics.\n",
    "    \"\"\"\n",
    "    X = 0.5\n",
    "\n",
    "    tau_true_mean = np.sum(X * alpha_tau) + bias_tau\n",
    "\n",
    "    if model == \"quadratic\":\n",
    "        count = nconsumer_characteristics * (nconsumer_characteristics + 1) // 2\n",
    "        X_poly = 0.25 * np.ones(count)\n",
    "        s = 0\n",
    "        for i in range(nconsumer_characteristics):\n",
    "            X_poly[s] = 1 / 3.0\n",
    "            s = s + nconsumer_characteristics - i\n",
    "\n",
    "        tau_true_mean = tau_true_mean + np.sum(X_poly * beta_tau)\n",
    "\n",
    "    return tau_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have alpha_tau, bias_tau, beta_tau, model, and nconsumer_characteristics defined somewhere\n",
    "model = \"quadratic\"\n",
    "# Call the function\n",
    "tau_true_mean = calculate_true_tau_mean(\n",
    "    alpha_tau,\n",
    "    bias_tau,\n",
    "    beta_tau,\n",
    "    model,\n",
    "    nconsumer_characteristics,\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(\"True average treatment effect:\", tau_true_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_propensity_scores(X, N, treatment=\"random\"):\n",
    "    \"\"\"Calculate propensity scores and create treatment variable for\n",
    "    fake dataset.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        X: ndarray, shape = (N, nconsumer_characteristics)\n",
    "            Feature matrix containing consumer characteristics.\n",
    "        N: int\n",
    "            Number of consumers.\n",
    "        treatment: {'random', 'not_random'}, optional\n",
    "            If 'random', consumers are being treated at random.\n",
    "            Otherwise, probability of being treated is a function\n",
    "            of consumer characteristics. Default is 'random'.\n",
    "    \"\"\"\n",
    "    if treatment == \"random\":\n",
    "        prob_of_T = 0.5\n",
    "        T = np.random.binomial(size=N, n=1, p=prob_of_T).reshape(N, 1)\n",
    "    else:\n",
    "        bias_p = 0.09\n",
    "        np.random.seed(72)\n",
    "        alpha_p = np.random.uniform(low=-0.55, high=0.55, size=[20, 1])\n",
    "        # Probability of t only depends on the first 20 consumers' features\n",
    "        p_of_t = np.dot(X[:, :20], alpha_p) + bias_p\n",
    "        p_of_t = p_of_t.reshape(-1)\n",
    "        prob_of_T = 1 / (1 + np.exp(-p_of_t))\n",
    "        T = np.random.binomial(size=N, n=1, p=prob_of_T).reshape(N, 1)\n",
    "\n",
    "    return T, prob_of_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have X, N, and treatment defined somewhere\n",
    "# Call the function\n",
    "T, prob_of_T = create_propensity_scores(X, N, treatment=\"\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Treatment variable (T):\")\n",
    "print(T)\n",
    "print(\"Propensity scores:\")\n",
    "print(prob_of_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def create_fake_data(\n",
    "    N,\n",
    "    nconsumer_characteristics,\n",
    "    model=\"quadtratic\",\n",
    "    verbose=False,\n",
    "    treatment=\"random\",\n",
    "):\n",
    "    seed = random.randint(1, 100000)\n",
    "    if verbose:\n",
    "        print(\"Seed number is: \", seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X = np.random.uniform(low=0, high=1, size=[N, nconsumer_characteristics])\n",
    "    normal_errors = np.random.normal(size=[N, 1], loc=0.0, scale=1.0)\n",
    "    alpha_tau, bias_tau, beta_tau = create_TE_coefs(model, nconsumer_characteristics)\n",
    "    tau_true_mean = calculate_true_tau_mean(alpha_tau, bias_tau, beta_tau, model)\n",
    "    T = np.random.choice([0, 1], size=(N, 1)) if treatment == \"random\" else None\n",
    "    prob_of_T = None\n",
    "    if treatment == \"not_random\":\n",
    "        prob_of_T = create_propensity_scores(X)\n",
    "        T = np.random.binomial(1, prob_of_T, size=(N, 1))\n",
    "    Y = mu0 + tau * T + normal_errors\n",
    "    return Y, X, T, seed, prob_of_T, tau_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the required parameters defined\n",
    "N = 1000\n",
    "nconsumer_characteristics = 100\n",
    "model = \"quadratic\"  # Specify the model type, either 'simple' or 'quadratic'\n",
    "verbose = True\n",
    "treatment = \"not_random\"  # Specify the treatment type, either 'random' or 'not_random'\n",
    "\n",
    "# Call the function\n",
    "Y, X, T, seed, prob_of_T, tau_true_mean = create_fake_data(\n",
    "    N,\n",
    "    nconsumer_characteristics,\n",
    "    model,\n",
    "    verbose,\n",
    "    treatment,\n",
    ")\n",
    "\n",
    "# Print or use the generated data as needed\n",
    "print(\"Generated data:\")\n",
    "print(\"Y:\", Y)\n",
    "print(\"X:\", X)\n",
    "print(\"Mu0:\", mu0)\n",
    "print(\"Tau:\", tau)\n",
    "print(\"T:\", T)\n",
    "print(\"Seed:\", seed)\n",
    "print(\"Probability of T:\", prob_of_T)\n",
    "print(\"True average treatment effect:\", tau_true_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for 2xt DifDif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latest system for difference in difference estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # Number of simulations\n",
    "T = 10  # Number of observations per group\n",
    "beta = 2.0  # True treatment effect\n",
    "gamma = 0.5  # True time trend coefficient\n",
    "sigma = 1.0  # Standard deviation of error term\n",
    "\n",
    "# Initialize arrays to store results\n",
    "bias = []\n",
    "rmse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "pre_treatment = np.random.normal(0, sigma, (T, 2))  # Two groups: treatment and control\n",
    "time_trend = np.arange(1, T + 1) * gamma\n",
    "post_treatment = pre_treatment.copy()\n",
    "post_treatment[:, 0] += beta  # Introduce treatment effect for treatment group\n",
    "post_treatment[:, :] += time_trend.reshape(-1, 1)  # Add time trend\n",
    "\n",
    "# Combine pre and post treatment data\n",
    "data = np.vstack((pre_treatment, post_treatment))\n",
    "groups = np.repeat([\"Control\", \"Treatment\"], T)\n",
    "time_periods = np.repeat(np.arange(1, T + 1), 2)\n",
    "\n",
    "df = pd.DataFrame({\"Group\": groups, \"Time\": time_periods, \"Outcome\": data.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define parameters\n",
    "N = 1000  # Number of simulations\n",
    "T = 10  # Number of observations per group\n",
    "beta = 2.0  # True treatment effect\n",
    "gamma = 0.5  # True time trend coefficient\n",
    "sigma = 1.0  # Standard deviation of error term\n",
    "\n",
    "# Initialize arrays to store results\n",
    "bias = []\n",
    "rmse = []\n",
    "\n",
    "# Run simulations\n",
    "for _ in range(N):\n",
    "    # Generate data\n",
    "    pre_treatment = np.random.normal(\n",
    "        0,\n",
    "        sigma,\n",
    "        (T, 2),\n",
    "    )  # Two groups: treatment and control\n",
    "    time_trend = np.arange(1, T + 1) * gamma\n",
    "    post_treatment = pre_treatment.copy()\n",
    "    post_treatment[:, 0] += beta  # Introduce treatment effect for treatment group\n",
    "    post_treatment[:, :] += time_trend.reshape(-1, 1)  # Add time trend\n",
    "\n",
    "    # Combine pre and post treatment data\n",
    "    data = np.vstack((pre_treatment, post_treatment))\n",
    "    groups = np.repeat([\"Control\", \"Treatment\"], T)\n",
    "    time_periods = np.repeat(np.arange(1, T + 1), 2)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\"Group\": groups, \"Time\": time_periods, \"Outcome\": data.flatten()},\n",
    "    )\n",
    "\n",
    "    # Run DiD regression\n",
    "    df[\"Treat\"] = (df[\"Group\"] == \"Treatment\").astype(int)\n",
    "    df[\"Post\"] = (df[\"Time\"] > T).astype(int)\n",
    "    df[\"Treat_Post\"] = df[\"Treat\"] * df[\"Post\"]\n",
    "\n",
    "    X = df[[\"Treat\", \"Post\", \"Treat_Post\"]]\n",
    "    X = sm.add_constant(X)\n",
    "    y = df[\"Outcome\"]\n",
    "\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Extract estimated treatment effect\n",
    "    est_beta = model.params[\"Treat_Post\"]\n",
    "\n",
    "    # Calculate bias and RMSE\n",
    "    bias.append(est_beta - beta)\n",
    "    rmse.append((est_beta - beta) ** 2)\n",
    "\n",
    "# Calculate mean bias and RMSE\n",
    "mean_bias = np.mean(bias)\n",
    "mean_rmse = np.sqrt(np.mean(rmse))\n",
    "\n",
    "print(\"Mean Bias:\", mean_bias)\n",
    "print(\"RMSE:\", mean_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate synthetic data with known true ATE\n",
    "\n",
    "\n",
    "def generate_data_with_true_ate(n_samples, true_ate, seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    control_data = pd.DataFrame(\n",
    "        {\n",
    "            \"individual\": range(1, n_samples + 1),\n",
    "            \"Age\": np.random.randint(20, 65, size=n_samples),\n",
    "            \"WagePartner_income\": np.random.normal(\n",
    "                loc=30000,\n",
    "                scale=5000,\n",
    "                size=n_samples,\n",
    "            ),\n",
    "            \"education_level\": np.random.choice(\n",
    "                [\"No High School\", \"High School\", \"Bachelor\", \"Master\", \"PhD\"],\n",
    "                size=n_samples,\n",
    "            ),\n",
    "            \"time\": np.random.choice([-2, -1, 1, 2, 3, 4, 5, 6, 7, 8], size=n_samples),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fq_levels = [\"Low\", \"High\"]\n",
    "    reform_levels = [\"Before\", \"After\"]\n",
    "    categorical_data = pd.DataFrame(\n",
    "        {\n",
    "            \"individual\": range(1, n_samples + 1),\n",
    "            \"FQ\": np.random.choice(fq_levels, size=n_samples),\n",
    "            \"Reform\": np.random.choice(reform_levels, size=n_samples),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fq_encoding = {\"Low\": 0, \"High\": 1}\n",
    "    reform_encoding = {\"Before\": 0, \"After\": 1}\n",
    "    categorical_data[\"FQ_encoded\"] = categorical_data[\"FQ\"].map(fq_encoding)\n",
    "    categorical_data[\"Reform_encoded\"] = categorical_data[\"Reform\"].map(reform_encoding)\n",
    "\n",
    "    categorical_data[\"interaction_effect\"] = (\n",
    "        categorical_data[\"FQ_encoded\"] * categorical_data[\"Reform_encoded\"]\n",
    "    )\n",
    "\n",
    "    control_data[\"wage_year_male\"] = np.random.normal(\n",
    "        loc=30,\n",
    "        scale=10,\n",
    "        size=n_samples,\n",
    "    ) * (1 + 0.1 * categorical_data[\"interaction_effect\"])\n",
    "    control_data[\"wage_year_female\"] = control_data[\"WagePartner_income\"]\n",
    "    control_data[\"dependent_variable\"] = (\n",
    "        control_data[\"wage_year_male\"] - control_data[\"wage_year_female\"]\n",
    "    )\n",
    "\n",
    "    education_encoding = {\n",
    "        \"No High School\": 0,\n",
    "        \"High School\": 1,\n",
    "        \"Bachelor\": 2,\n",
    "        \"Master\": 3,\n",
    "        \"PhD\": 4,\n",
    "    }\n",
    "    control_data[\"education_level_encoded\"] = control_data[\"education_level\"].map(\n",
    "        education_encoding,\n",
    "    )\n",
    "\n",
    "    data = pd.merge(control_data, categorical_data, on=\"individual\")\n",
    "    data = data.drop([\"Reform\", \"FQ\", \"education_level\"], axis=1)\n",
    "    data = data.drop(\n",
    "        [\"WagePartner_income\", \"wage_year_male\", \"wage_year_female\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return data, true_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Difference-in-Differences (DiD) with known true ATE\n",
    "\n",
    "\n",
    "def difference_in_differences_known_ate(data):\n",
    "    treatment_group = data[data[\"Reform_encoded\"] == 1]\n",
    "    control_group = data[data[\"Reform_encoded\"] == 0]\n",
    "\n",
    "    before_treatment_treatment_group = treatment_group[treatment_group[\"time\"] < 0][\n",
    "        \"dependent_variable\"\n",
    "    ].mean()\n",
    "    after_treatment_treatment_group = treatment_group[treatment_group[\"time\"] > 0][\n",
    "        \"dependent_variable\"\n",
    "    ].mean()\n",
    "    before_treatment_control_group = control_group[control_group[\"time\"] < 0][\n",
    "        \"dependent_variable\"\n",
    "    ].mean()\n",
    "    after_treatment_control_group = control_group[control_group[\"time\"] > 0][\n",
    "        \"dependent_variable\"\n",
    "    ].mean()\n",
    "\n",
    "    pre_treatment_difference = (\n",
    "        before_treatment_treatment_group - before_treatment_control_group\n",
    "    )\n",
    "    post_treatment_difference = (\n",
    "        after_treatment_treatment_group - after_treatment_control_group\n",
    "    )\n",
    "\n",
    "    return post_treatment_difference - pre_treatment_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulation with known true ATE\n",
    "num_simulations = 1000\n",
    "ate_results_with_true_ate = []\n",
    "\n",
    "seed_value = 634\n",
    "\n",
    "true_ate = 10  # Set true ATE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "generate_data_with_true_ate(n_samples, true_ate, seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_simulations):\n",
    "    seed = seed_value + i\n",
    "    synthetic_data, true_ate = generate_data_with_true_ate(\n",
    "        n_samples=1000,  # sample size\n",
    "        true_ate=true_ate,\n",
    "        seed=seed,\n",
    "    )\n",
    "    ate = difference_in_differences_known_ate(synthetic_data)\n",
    "\n",
    "# Calculate mean and standard error of estimated ATE\n",
    "\n",
    "# Calculate mean and standard error of estimated ATE\n",
    "ate_results_with_true_ate = np.array(ate_results_with_true_ate)\n",
    "mean_ate = np.mean(ate)\n",
    "std_err_ate = np.std(ate, ddof=1) / np.sqrt(\n",
    "    num_simulations,\n",
    ")\n",
    "\n",
    "# Calculate t-value\n",
    "t_value_ate = mean_ate / std_err_ate\n",
    "\n",
    "# Calculate p-value\n",
    "degrees_of_freedom = num_simulations - 1\n",
    "p_value_sim_ate = stats.t.cdf(\n",
    "    t_value_ate,\n",
    "    df=degrees_of_freedom,\n",
    ")  # richtige verteilung?\n",
    "critical_value_simulated = stats.t.ppf((1 + 0.95) / 2, df=degrees_of_freedom)\n",
    "\n",
    "# Calculate confidence interval for simulated ATE\n",
    "\n",
    "mean_ate_simulated = np.mean(ate)\n",
    "std_err_ate_simulated = np.std(ate, ddof=1) / np.sqrt(\n",
    "    num_simulations,\n",
    ")\n",
    "margin_of_error_simulated = critical_value_simulated * std_err_ate_simulated\n",
    "lower_bound_simulated = mean_ate_simulated - margin_of_error_simulated\n",
    "upper_bound_simulated = mean_ate_simulated + margin_of_error_simulated\n",
    "\n",
    "\n",
    "# Calculate confidence interval for true ATE\n",
    "critical_value_with_true_ate = stats.t.ppf((1 + 0.95) / 2, df=degrees_of_freedom)\n",
    "margin_of_error_with_true_ate = critical_value_with_true_ate * std_err_ate\n",
    "lower_bound_with_true_ate = true_ate - margin_of_error_with_true_ate\n",
    "upper_bound_with_true_ate = true_ate + margin_of_error_with_true_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Mean ATE with known true ATE:\", 10)\n",
    "print(\n",
    "    \"Confidence Interval for simulated ATE:\",\n",
    "    (lower_bound_simulated, upper_bound_simulated),\n",
    ")\n",
    "print(\"Mean ATE of simulated ATE:\", mean_ate)\n",
    "print(\"Standard Error of simulated ATE:\", std_err_ate)\n",
    "print(\"t-value of simulated ATE:\", t_value_ate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(\n",
    "    ate_results_with_true_ate[:, 1],\n",
    "    bins=30,\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    "    label=\"Simulated ATE\",\n",
    ")\n",
    "plt.axvline(\n",
    "    mean_ate_with_true_ate,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.5,\n",
    "    label=\"Mean Simulated ATE\",\n",
    ")\n",
    "plt.axvline(true_ate, color=\"green\", linestyle=\"--\", linewidth=1.5, label=\"True ATE\")\n",
    "plt.xlabel(\"Average Treatment Effect (ATE)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Simulated Average Treatment Effects (ATE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    [ate_results_with_true_ate[:, 1], [true_ate]],\n",
    "    labels=[\"Simulated ATE\", \"True ATE\"],\n",
    ")\n",
    "plt.ylabel(\"Average Treatment Effect (ATE)\")\n",
    "plt.title(\"Boxplot of Simulated and True Average Treatment Effects (ATE)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTE\n",
    "\n",
    "\\[ Y_{it} = \\beta_0 + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\delta (\\text{Post}_t \\times \\text{Treatment}_i) + \\epsilon_{it} \\]\n",
    "\n",
    "Where:\n",
    "- \\( Y_{it} \\) represents the outcome variable (e.g., wages) for individual \\( i \\) at time \\( t \\).\n",
    "- \\( \\text{Post}_t \\) is a binary variable indicating whether the observation is from the post-treatment period.\n",
    "- \\( \\text{Treatment}_i \\) is a binary variable indicating whether individual \\( i \\) is in the treatment group.\n",
    "- \\( \\delta \\) represents the coefficient for the interaction between the post-treatment period and the treatment group, capturing the average treatment effect (ATE).\n",
    "- \\( \\epsilon_{it} \\) is the error term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATE\n",
    "\n",
    "\\[ Y_{it} = \\beta_0 + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treatment}_i + \\beta_3 (\\text{Post}_t \\times \\text{Treatment}_i) + \\sum_{k=1}^{K} \\beta_{k+3} (\\text{X}_i = k \\times \\text{Treatment}_i) + \\epsilon_{it} \\]\n",
    "\n",
    "Where:\n",
    "- \\( Y_{it} \\) represents the outcome variable (e.g., wages) for individual \\( i \\) at time \\( t \\).\n",
    "- \\( \\text{Post}_t \\) is a binary variable indicating whether the observation is from the post-treatment period.\n",
    "- \\( \\text{Treatment}_i \\) is a binary variable indicating whether individual \\( i \\) is in the treatment group.\n",
    "- \\( \\text{X}_i \\) is a categorical variable representing a conditioning variable (e.g., education level) for individual \\( i \\).\n",
    "- \\( K \\) is the total number of levels of the conditioning variable.\n",
    "- \\( \\beta_{k+3} \\) represents the coefficient for the interaction between the conditioning variable level \\( k \\) and the treatment indicator.\n",
    "- \\( \\epsilon_{it} \\) is the error term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the modell converges very fast to the true value\n",
    "\n",
    "- proposed changes: more zeroes in the income values \n",
    "- CATE and estimation with a non CATE DiD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def generate_data_with_true_ate(n_samples, true_ate, seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    control_data = pd.DataFrame(\n",
    "        {\n",
    "            \"individual\": range(1, n_samples + 1),\n",
    "            \"Age\": np.random.randint(20, 65, size=n_samples),\n",
    "            \"WagePartner_income\": np.random.normal(\n",
    "                loc=30000,\n",
    "                scale=5000,\n",
    "                size=n_samples,\n",
    "            ),\n",
    "            \"education_level\": np.random.choice(\n",
    "                [\"No High School\", \"High School\", \"Bachelor\", \"Master\", \"PhD\"],\n",
    "                size=n_samples,\n",
    "            ),\n",
    "            \"time\": np.random.choice([-2, -1, 1, 2, 3, 4, 5, 6, 7, 8], size=n_samples),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fq_levels = [\"Low\", \"High\"]\n",
    "    reform_levels = [\"Before\", \"After\"]\n",
    "    categorical_data = pd.DataFrame(\n",
    "        {\n",
    "            \"individual\": range(1, n_samples + 1),\n",
    "            \"FQ\": np.random.choice(fq_levels, size=n_samples),\n",
    "            \"Reform\": np.random.choice(reform_levels, size=n_samples),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fq_encoding = {\"Low\": 0, \"High\": 1}\n",
    "    reform_encoding = {\"Before\": 0, \"After\": 1}\n",
    "    categorical_data[\"FQ_encoded\"] = categorical_data[\"FQ\"].map(fq_encoding)\n",
    "    categorical_data[\"Reform_encoded\"] = categorical_data[\"Reform\"].map(reform_encoding)\n",
    "\n",
    "    categorical_data[\"interaction_effect\"] = (\n",
    "        categorical_data[\"FQ_encoded\"] * categorical_data[\"Reform_encoded\"]\n",
    "    )\n",
    "\n",
    "    control_data[\"wage_year_male\"] = np.random.normal(\n",
    "        loc=30,\n",
    "        scale=10,\n",
    "        size=n_samples,\n",
    "    ) * (1 + 0.1 * categorical_data[\"interaction_effect\"])\n",
    "    control_data[\"wage_year_female\"] = control_data[\"WagePartner_income\"]\n",
    "    control_data[\"dependent_variable\"] = (\n",
    "        control_data[\"wage_year_male\"] - control_data[\"wage_year_female\"]\n",
    "    )\n",
    "\n",
    "    education_encoding = {\n",
    "        \"No High School\": 0,\n",
    "        \"High School\": 1,\n",
    "        \"Bachelor\": 2,\n",
    "        \"Master\": 3,\n",
    "        \"PhD\": 4,\n",
    "    }\n",
    "    control_data[\"education_level_encoded\"] = control_data[\"education_level\"].map(\n",
    "        education_encoding,\n",
    "    )\n",
    "\n",
    "    data = pd.merge(control_data, categorical_data, on=\"individual\")\n",
    "    data = data.drop([\"Reform\", \"FQ\", \"education_level\"], axis=1)\n",
    "    data = data.drop(\n",
    "        [\"WagePartner_income\", \"wage_year_male\", \"wage_year_female\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return data, true_ate\n",
    "\n",
    "\n",
    "def difference_in_differences_cte(data, conditioning_variable):\n",
    "    # Define treatment and control groups\n",
    "    treatment_group = data[data[\"Reform_encoded\"] == 1]\n",
    "    control_group = data[data[\"Reform_encoded\"] == 0]\n",
    "\n",
    "    # Initialize lists to store CTEs for each level of the conditioning variable\n",
    "    ctes = []\n",
    "\n",
    "    # Compute CTE for each level of the conditioning variable\n",
    "    for level in data[conditioning_variable].unique():\n",
    "        treatment_group_level = treatment_group[\n",
    "            treatment_group[conditioning_variable] == level\n",
    "        ]\n",
    "        control_group_level = control_group[\n",
    "            control_group[conditioning_variable] == level\n",
    "        ]\n",
    "\n",
    "        pre_diff_treatment_group = treatment_group_level[\n",
    "            treatment_group_level[\"time\"] < 0\n",
    "        ][\"dependent_variable\"].mean()\n",
    "        post_diff_treatment_group = treatment_group_level[\n",
    "            treatment_group_level[\"time\"] > 0\n",
    "        ][\"dependent_variable\"].mean()\n",
    "        pre_diff_control_group = control_group_level[control_group_level[\"time\"] < 0][\n",
    "            \"dependent_variable\"\n",
    "        ].mean()\n",
    "        post_diff_control_group = control_group_level[control_group_level[\"time\"] > 0][\n",
    "            \"dependent_variable\"\n",
    "        ].mean()\n",
    "\n",
    "        cte_treatment_group = post_diff_treatment_group - pre_diff_treatment_group\n",
    "        cte_control_group = post_diff_control_group - pre_diff_control_group\n",
    "\n",
    "        cte = cte_treatment_group - cte_control_group\n",
    "        ctes.append(cte)\n",
    "\n",
    "    # Compute overall CTE\n",
    "    return sum(ctes)\n",
    "\n",
    "\n",
    "# Generate data\n",
    "num_simulations = 1000\n",
    "seed_value = 42\n",
    "true_ate = 10  # Adjust as needed\n",
    "ate_results_with_true_ate = []\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    seed = seed_value + i\n",
    "    synthetic_data, _ = generate_data_with_true_ate(\n",
    "        n_samples=1000,\n",
    "        true_ate=true_ate,\n",
    "        seed=seed,\n",
    "    )\n",
    "    cte = difference_in_differences_cte(synthetic_data, \"education_level_encoded\")\n",
    "    ate_results_with_true_ate.append(cte)\n",
    "\n",
    "# Calculate mean and standard error of estimated CTE\n",
    "mean_cte_with_true_ate = np.mean(ate_results_with_true_ate)\n",
    "std_err_cte_with_true_ate = np.std(ate_results_with_true_ate, ddof=1) / np.sqrt(\n",
    "    num_simulations,\n",
    ")\n",
    "\n",
    "# Calculate t-value\n",
    "t_value_with_true_ate = mean_cte_with_true_ate / std_err_cte_with_true_ate\n",
    "\n",
    "# Calculate p-value\n",
    "degrees_of_freedom = num_simulations - 1\n",
    "p_value_with_true_ate = stats.t.cdf(t_value_with_true_ate, df=degrees_of_freedom)\n",
    "\n",
    "# Calculate confidence interval for simulated CTE\n",
    "critical_value_with_true_ate = stats.t.ppf((1 + 0.95) / 2, df=degrees_of_freedom)\n",
    "margin_of_error_with_true_ate = critical_value_with_true_ate * std_err_cte_with_true_ate\n",
    "lower_bound_with_true_ate = mean_cte_with_true_ate - margin_of_error_with_true_ate\n",
    "upper_bound_with_true_ate = mean_cte_with_true_ate + margin_of_error_with_true_ate\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"True CTE:\", true_ate)\n",
    "print(\"Mean CTE with known true CTE:\", mean_cte_with_true_ate)\n",
    "print(\"Standard Error of CTE with known true CTE:\", std_err_cte_with_true_ate)\n",
    "print(\"t-value with known true CTE:\", t_value_with_true_ate)\n",
    "print(\"p-value with known true CTE:\", p_value_with_true_ate)\n",
    "print(\n",
    "    \"Confidence Interval for simulated CTE:\",\n",
    "    (lower_bound_with_true_ate, upper_bound_with_true_ate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of simulated CTEs\n",
    "plt.hist(\n",
    "    ate_results_with_true_ate,\n",
    "    bins=20,\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    "    label=\"Simulated CTEs\",\n",
    ")\n",
    "plt.axvline(\n",
    "    x=mean_cte_with_true_ate,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean Simulated CTE\",\n",
    ")\n",
    "\n",
    "# Plot true CTE\n",
    "plt.axvline(x=true_ate, color=\"green\", linestyle=\"-\", label=\"True CTE\")\n",
    "\n",
    "plt.xlabel(\"Conditional Treatment Effect (CTE)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Simulated and True Conditional Treatment Effects\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
