{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture 1 taken from paper\n",
    "# maybe later add more architectures\n",
    "hidden_layer_sizes = [20, 10, 5]\n",
    "dropout_rates_train = [0, 0, 0, 0]\n",
    "activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propensity scores taken from paper\n",
    "\n",
    "hidden_layer_sizes_treatment = [50, 30]\n",
    "activation_functions_treatment = [\"relu\", \"relu\", \"none\"]\n",
    "dropout_rates_train_treatment = [0, 0, 0]\n",
    "dropout_rates_test_treatment = [0 for i in dropout_rates_train_treatment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "train_proportion = 0.9\n",
    "max_nepochs = 5000\n",
    "max_epochs_without_change = 30\n",
    "optimizer = \"Adam\"\n",
    "learning_rate = 0.009\n",
    "batch_size = 128\n",
    "batch_size_t = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization parameters\n",
    "alpha = 0.0\n",
    "r = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the real data\n",
    "data = pd.read_csv(\n",
    "    \"bld/difference_in_difference_with_deep_learning/data/cleaned_data.csv\",\n",
    ")  # Change the filename to your actual data file\n",
    "\n",
    "# Assuming 'wage_year' is the variable of interest\n",
    "X = data[[\"Individual\", \"Age\", \"WagePartner\"]]  # Features\n",
    "T_real = data[\"interaction\"]  # Treatment assignment\n",
    "Y = data[\"wage_year\"]  # Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time variable to numerical values\n",
    "time_mapping = {\"t-2\": -2, \"t-1\": -1, \"t+1\": 1, \"t+2\": 2, \"t+3\": 3}\n",
    "df[\"time\"] = df[\"time\"].map(time_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fakedata get some values\n",
    "- mu0_real\n",
    "- tau_real\n",
    "- T_real\n",
    "- seed\n",
    "- prob_of_T\n",
    "- tau_true_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which functions do I need:\n",
    "- _sum_polynomial_X_times_weights\n",
    "- _create_TE_coefs\n",
    "- _calculate_true_tau_mean\n",
    "- _create_propensity_scores\n",
    "- create_fake_data, just to create the datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = df.drop(\"wage_year\", axis=1).values\n",
    "Target = df[\"wage_year\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quadratic_polynomial(features, weights):\n",
    "    \"\"\"Evaluate the non-linear part of a quadratic polynomial in\n",
    "    consumer characteristics, features, with prescribed weights.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        features: ndarray, shape = (N, n_features)\n",
    "            Input features.\n",
    "        weights: ndarray, shape = (num_additional_poly_terms, )\n",
    "            Weights corresponding to quadratic terms.\n",
    "    Outputs:\n",
    "    -------\n",
    "        sum_x: ndarray, shape = (N, 1)\n",
    "            Non-linear part of the quadratic polynomial evaluated\n",
    "            for each sample.\n",
    "    \"\"\"\n",
    "    n_features = features.shape[1]\n",
    "    my_polynomial_indices = combinations_with_replacement(list(range(n_features)), 2)\n",
    "    i = 0\n",
    "    sum_x = 0\n",
    "    for p in my_polynomial_indices:\n",
    "        sum_x = sum_x + weights[i % len(weights)] * np.multiply(\n",
    "            features[:, p[0]],\n",
    "            features[:, p[1]],\n",
    "        )\n",
    "        i += 1\n",
    "    return sum_x.reshape(-1, 1)\n",
    "\n",
    "\n",
    "num_additional_poly_terms = 10\n",
    "\n",
    "# Generate random weights\n",
    "weights = np.random.uniform(low=-1.0, high=1.0, size=num_additional_poly_terms)\n",
    "\n",
    "# Call the function for X_train\n",
    "sum_x = evaluate_quadratic_polynomial(feature, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_TE_coefs(features, treatment_interaction, model):\n",
    "    \"\"\"Create treatment effect coefficients.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        features: ndarray, shape = (N, n_features)\n",
    "            Input features.\n",
    "        treatment_interaction: ndarray, shape = (N, 1)\n",
    "            Interaction term from the data.\n",
    "        model: {'simple', 'quadratic'}\n",
    "            If 'simple' coefficients a and b in the artificial\n",
    "            dataset depend linearly on consumer characteristics.\n",
    "            Otherwise, the dependence is quadratic.\n",
    "    Outputs:\n",
    "    -------\n",
    "        bias_tau: float\n",
    "            Constant term in equation for tau.\n",
    "        alpha_tau: ndarray, shape = [n_features, 1]\n",
    "            Linear coefficients in equation for tau.\n",
    "        beta_tau: ndarray, shape = [count]\n",
    "            Quadratic coefficients in equation for tau.\n",
    "            Count is the number of the second degree terms in a\n",
    "            quadratic polynomial where the number of variables is\n",
    "            equal to the number of consumer characteristics.\n",
    "    \"\"\"\n",
    "    # Concatenate features and treatment_interaction\n",
    "    features_with_interaction = np.concatenate(\n",
    "        (features, treatment_interaction),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Calculating tau\n",
    "    alpha_tau = np.random.uniform(\n",
    "        low=0.1,\n",
    "        high=0.22,\n",
    "        size=[features_with_interaction.shape[1], 1],\n",
    "    )\n",
    "    bias_tau = -0.05\n",
    "    tau = np.dot(features_with_interaction, alpha_tau) + bias_tau\n",
    "\n",
    "    if model == \"quadratic\":\n",
    "        num_additional_poly_terms = comb(\n",
    "            features_with_interaction.shape[1],\n",
    "            2,\n",
    "            True,\n",
    "            True,\n",
    "        )\n",
    "        beta_tau = np.random.uniform(\n",
    "            low=-0.05,\n",
    "            high=0.06,\n",
    "            size=num_additional_poly_terms,\n",
    "        )\n",
    "        sum_x = evaluate_quadratic_polynomial(features_with_interaction, beta_tau)\n",
    "        tau += sum_x\n",
    "    else:\n",
    "        beta_tau = None\n",
    "\n",
    "    return alpha_tau, bias_tau, beta_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"time\", \"wage_year\", \"FQ\", \"Reform\", \"Age\", \"WagePartner\"]].values\n",
    "treatment_interaction = df[\"interaction\"].values.reshape(\n",
    "    -1,\n",
    "    1,\n",
    ")  # Assuming 'interaction' is the treatment interaction term\n",
    "\n",
    "# Call the function for X\n",
    "alpha_tau, bias_tau, beta_tau = _create_TE_coefs(X, treatment_interaction, \"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_true_tau_mean(\n",
    "    alpha_tau,\n",
    "    bias_tau,\n",
    "    beta_tau,\n",
    "    treatment_interaction,\n",
    "    model,\n",
    "):\n",
    "    \"\"\"Calculate true average treatment effect.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "        bias_tau: float\n",
    "            Constant term in equation for tau.\n",
    "        alpha_tau: ndarray, shape = [nconsumer_characteristics, 1]\n",
    "            Linear coefficients in equation for tau.\n",
    "        beta_tau: ndarray, shape = [count]\n",
    "            Quadratic coefficients in equation for tau.\n",
    "            Count is the number of the second degree terms in a\n",
    "            quadratic polynomial where the number of variables is\n",
    "            equal to the number of consumer characteristics.\n",
    "        treatment_interaction: ndarray, shape = (N, 1)\n",
    "            Interaction term representing the treatment.\n",
    "        model: {'simple', 'quadratic'}\n",
    "            If 'simple' coefficients a and b in the artificial\n",
    "            dataset depend linearly on consumer characteristics.\n",
    "            Otherwise, the dependence is quadratic.\n",
    "    \"\"\"\n",
    "    X = treatment_interaction\n",
    "\n",
    "    tau_true_mean = np.sum(X * alpha_tau) + bias_tau\n",
    "\n",
    "    if model == \"quadratic\":\n",
    "        X_poly = 0.25 * np.ones(len(beta_tau))\n",
    "        s = 0\n",
    "        for i in range(treatment_interaction.shape[1]):\n",
    "            X_poly[s] = 1 / 3.0\n",
    "            s = s + treatment_interaction.shape[1] - i\n",
    "\n",
    "        tau_true_mean = tau_true_mean + np.sum(X_poly * beta_tau)\n",
    "\n",
    "    return tau_true_mean\n",
    "\n",
    "\n",
    "# Assuming you have already computed the alpha_tau, bias_tau, beta_tau\n",
    "tau_true_mean = _calculate_true_tau_mean(\n",
    "    alpha_tau,\n",
    "    bias_tau,\n",
    "    beta_tau,\n",
    "    treatment_interaction,\n",
    "    \"quadratic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed?\n",
    "hello = 1\n",
    "# the question is whether train and test split are in the part beforehand relevant\n",
    "\n",
    "df = df.sort_values(by=\"time\").reset_index(drop=True)\n",
    "\n",
    "# Determine the index to split the data into training and testing sets\n",
    "split_index = int(0.8 * len(df))  # 80% training, 20% testing\n",
    "\n",
    "# Split the DataFrame into training and testing sets\n",
    "train_df = df.iloc[:split_index]\n",
    "test_df = df.iloc[split_index:]\n",
    "\n",
    "# Separate features and target for training and testing sets\n",
    "X_train = train_df.drop(\"wage_year\", axis=1).values\n",
    "Y_train = train_df[\"wage_year\"].values.reshape(-1, 1)\n",
    "X_test = test_df.drop(\"wage_year\", axis=1).values\n",
    "Y_test = test_df[\"wage_year\"].values.reshape(-1, 1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
