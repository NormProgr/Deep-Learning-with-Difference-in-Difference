{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture 1 taken from paper\n",
    "# maybe later add more architectures\n",
    "hidden_layer_sizes = [20, 10, 5]\n",
    "dropout_rates_train = [0, 0, 0, 0]\n",
    "activation_functions = [\"relu\", \"relu\", \"relu\", \"none\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propensity scores taken from paper\n",
    "\n",
    "hidden_layer_sizes_treatment = [50, 30]\n",
    "activation_functions_treatment = [\"relu\", \"relu\", \"none\"]\n",
    "dropout_rates_train_treatment = [0, 0, 0]\n",
    "dropout_rates_test_treatment = [0 for i in dropout_rates_train_treatment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "train_proportion = 0.9\n",
    "max_nepochs = 5000\n",
    "max_epochs_without_change = 30\n",
    "optimizer = \"Adam\"\n",
    "learning_rate = 0.009\n",
    "batch_size = 128\n",
    "batch_size_t = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization parameters\n",
    "alpha = 0.0\n",
    "r = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the real data\n",
    "data = pd.read_csv(\n",
    "    \"bld/difference_in_difference_with_deep_learning/data/cleaned_data.csv\",\n",
    ")  # Change the filename to your actual data file\n",
    "\n",
    "# Assuming 'wage_year' is the variable of interest\n",
    "X = data[[\"Individual\", \"Age\", \"WagePartner\"]]  # Features\n",
    "T_real = data[\"interaction\"]  # Treatment assignment\n",
    "Y = data[\"wage_year\"]  # Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have your data stored in variables X, T_real, and Y\n",
    "# and train_proportion is set to 0.9\n",
    "\n",
    "# Calculate test size based on train_proportion\n",
    "test_size = 1.0 - train_proportion\n",
    "\n",
    "# Set up the test and training split\n",
    "X_train, X_valid, T_train, T_valid, Y_train, Y_valid = train_test_split(\n",
    "    X,\n",
    "    T_real,\n",
    "    Y,\n",
    "    test_size=test_size,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Print the shapes of the training and validation sets to verify\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"T_train shape:\", T_train.shape)\n",
    "print(\"T_valid shape:\", T_valid.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_valid shape:\", Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fakedata get some values\n",
    "- mu0_real\n",
    "- tau_real\n",
    "- T_real\n",
    "- seed\n",
    "- prob_of_T\n",
    "- tau_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
